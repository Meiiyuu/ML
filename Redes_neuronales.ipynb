{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a1b1880a",
      "metadata": {
        "id": "a1b1880a"
      },
      "source": [
        "## Proyecto 2: Redes neuronales\n",
        "Daniela Gil y Sofia Ochoa\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "El objetivo del proyecto es usar una red neuronal para clasificar datos sobre señales de voz, de acuerdo al sentimiento de dicha voz: enojado, triste, feliz.\n",
        "\n",
        " Primero importamos las librerias necesarias"
      ],
      "metadata": {
        "id": "G7Ak21BBnjt6"
      },
      "id": "G7Ak21BBnjt6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1db57d0d",
      "metadata": {
        "id": "1db57d0d"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import TensorDataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importamos como dataframe de pandas al excel que contiene los datos sobre las señales de voz"
      ],
      "metadata": {
        "id": "BMYgPiJhoBBc"
      },
      "id": "BMYgPiJhoBBc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d52c69d1",
      "metadata": {
        "id": "d52c69d1"
      },
      "outputs": [],
      "source": [
        "df=pd.read_csv('emotions_by_voice registers.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a6b8918",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "id": "3a6b8918",
        "outputId": "6538709d-2f00-44e3-d580-4e952af84916"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     Unnamed: 0.1  Unnamed: 0     X  meanfreq        sd    median       Q25  \\\n",
              "0               0           1     1  0.181338  0.060495  0.187476  0.126197   \n",
              "1               1           2     2  0.186897  0.062260  0.195070  0.130847   \n",
              "2               2           3     3  0.189102  0.062901  0.204945  0.131422   \n",
              "3               4           5     5  0.183036  0.060051  0.174115  0.129949   \n",
              "4               5           6     6  0.168793  0.057910  0.156266  0.116783   \n",
              "..            ...         ...   ...       ...       ...       ...       ...   \n",
              "904          1243        1244  1436  0.244013  0.035477  0.254385  0.229653   \n",
              "905          1244        1245  1437  0.235383  0.045303  0.248974  0.220745   \n",
              "906          1245        1246  1438  0.231211  0.044793  0.234847  0.221477   \n",
              "907          1246        1247  1439  0.213587  0.082267  0.249435  0.207680   \n",
              "908          1247        1248  1440  0.212537  0.078746  0.245034  0.209794   \n",
              "\n",
              "          Q75       IQR      skew  ...  centroid   meanfun    minfun  \\\n",
              "0    0.233586  0.107389  0.869088  ...  0.181338  0.137742  0.023022   \n",
              "1    0.243987  0.113140  1.191767  ...  0.186897  0.121811  0.018412   \n",
              "2    0.249978  0.118556  1.312690  ...  0.189102  0.123758  0.083333   \n",
              "3    0.236967  0.107017  1.096409  ...  0.183036  0.128469  0.044693   \n",
              "4    0.216326  0.099543  1.386837  ...  0.168793  0.109720  0.022472   \n",
              "..        ...       ...       ...  ...       ...       ...       ...   \n",
              "904  0.265573  0.035920  2.214752  ...  0.244013  0.202433  0.028829   \n",
              "905  0.264233  0.043488  2.474743  ...  0.235383  0.189293  0.031250   \n",
              "906  0.262090  0.040613  2.607668  ...  0.231211  0.171805  0.022346   \n",
              "907  0.268538  0.060858  3.460579  ...  0.213587  0.155277  0.020592   \n",
              "908  0.264031  0.054238  2.563983  ...  0.212537  0.162938  0.024845   \n",
              "\n",
              "       maxfun   meandom    mindom    maxdom   dfrange   modindx  label  \n",
              "0    0.271186  0.777344  0.085938  6.226562  6.140625  0.116586    sad  \n",
              "1    0.271186  0.930339  0.085938  4.000000  3.914062  0.144983    sad  \n",
              "2    0.262295  0.332386  0.085938  0.625000  0.539062  0.334783    sad  \n",
              "3    0.258065  1.012019  0.085938  5.468750  5.382812  0.304910    sad  \n",
              "4    0.235294  0.228795  0.093750  0.750000  0.656250  0.306777    sad  \n",
              "..        ...       ...       ...       ...       ...       ...    ...  \n",
              "904  0.271186  0.616536  0.210938  1.609375  1.398438  0.281869  happy  \n",
              "905  0.275862  1.115723  0.265625  5.500000  5.234375  0.167861  happy  \n",
              "906  0.275862  1.070801  0.265625  4.554688  4.289062  0.214936  happy  \n",
              "907  0.275862  1.724888  0.273438  6.812500  6.539062  0.238857  happy  \n",
              "908  0.258065  0.915625  0.000000  6.281250  6.281250  0.193141  happy  \n",
              "\n",
              "[909 rows x 24 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e30d2891-7490-4c82-a917-98d1ddabe911\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0.1</th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>X</th>\n",
              "      <th>meanfreq</th>\n",
              "      <th>sd</th>\n",
              "      <th>median</th>\n",
              "      <th>Q25</th>\n",
              "      <th>Q75</th>\n",
              "      <th>IQR</th>\n",
              "      <th>skew</th>\n",
              "      <th>...</th>\n",
              "      <th>centroid</th>\n",
              "      <th>meanfun</th>\n",
              "      <th>minfun</th>\n",
              "      <th>maxfun</th>\n",
              "      <th>meandom</th>\n",
              "      <th>mindom</th>\n",
              "      <th>maxdom</th>\n",
              "      <th>dfrange</th>\n",
              "      <th>modindx</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.181338</td>\n",
              "      <td>0.060495</td>\n",
              "      <td>0.187476</td>\n",
              "      <td>0.126197</td>\n",
              "      <td>0.233586</td>\n",
              "      <td>0.107389</td>\n",
              "      <td>0.869088</td>\n",
              "      <td>...</td>\n",
              "      <td>0.181338</td>\n",
              "      <td>0.137742</td>\n",
              "      <td>0.023022</td>\n",
              "      <td>0.271186</td>\n",
              "      <td>0.777344</td>\n",
              "      <td>0.085938</td>\n",
              "      <td>6.226562</td>\n",
              "      <td>6.140625</td>\n",
              "      <td>0.116586</td>\n",
              "      <td>sad</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0.186897</td>\n",
              "      <td>0.062260</td>\n",
              "      <td>0.195070</td>\n",
              "      <td>0.130847</td>\n",
              "      <td>0.243987</td>\n",
              "      <td>0.113140</td>\n",
              "      <td>1.191767</td>\n",
              "      <td>...</td>\n",
              "      <td>0.186897</td>\n",
              "      <td>0.121811</td>\n",
              "      <td>0.018412</td>\n",
              "      <td>0.271186</td>\n",
              "      <td>0.930339</td>\n",
              "      <td>0.085938</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>3.914062</td>\n",
              "      <td>0.144983</td>\n",
              "      <td>sad</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>0.189102</td>\n",
              "      <td>0.062901</td>\n",
              "      <td>0.204945</td>\n",
              "      <td>0.131422</td>\n",
              "      <td>0.249978</td>\n",
              "      <td>0.118556</td>\n",
              "      <td>1.312690</td>\n",
              "      <td>...</td>\n",
              "      <td>0.189102</td>\n",
              "      <td>0.123758</td>\n",
              "      <td>0.083333</td>\n",
              "      <td>0.262295</td>\n",
              "      <td>0.332386</td>\n",
              "      <td>0.085938</td>\n",
              "      <td>0.625000</td>\n",
              "      <td>0.539062</td>\n",
              "      <td>0.334783</td>\n",
              "      <td>sad</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>0.183036</td>\n",
              "      <td>0.060051</td>\n",
              "      <td>0.174115</td>\n",
              "      <td>0.129949</td>\n",
              "      <td>0.236967</td>\n",
              "      <td>0.107017</td>\n",
              "      <td>1.096409</td>\n",
              "      <td>...</td>\n",
              "      <td>0.183036</td>\n",
              "      <td>0.128469</td>\n",
              "      <td>0.044693</td>\n",
              "      <td>0.258065</td>\n",
              "      <td>1.012019</td>\n",
              "      <td>0.085938</td>\n",
              "      <td>5.468750</td>\n",
              "      <td>5.382812</td>\n",
              "      <td>0.304910</td>\n",
              "      <td>sad</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>0.168793</td>\n",
              "      <td>0.057910</td>\n",
              "      <td>0.156266</td>\n",
              "      <td>0.116783</td>\n",
              "      <td>0.216326</td>\n",
              "      <td>0.099543</td>\n",
              "      <td>1.386837</td>\n",
              "      <td>...</td>\n",
              "      <td>0.168793</td>\n",
              "      <td>0.109720</td>\n",
              "      <td>0.022472</td>\n",
              "      <td>0.235294</td>\n",
              "      <td>0.228795</td>\n",
              "      <td>0.093750</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.656250</td>\n",
              "      <td>0.306777</td>\n",
              "      <td>sad</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>904</th>\n",
              "      <td>1243</td>\n",
              "      <td>1244</td>\n",
              "      <td>1436</td>\n",
              "      <td>0.244013</td>\n",
              "      <td>0.035477</td>\n",
              "      <td>0.254385</td>\n",
              "      <td>0.229653</td>\n",
              "      <td>0.265573</td>\n",
              "      <td>0.035920</td>\n",
              "      <td>2.214752</td>\n",
              "      <td>...</td>\n",
              "      <td>0.244013</td>\n",
              "      <td>0.202433</td>\n",
              "      <td>0.028829</td>\n",
              "      <td>0.271186</td>\n",
              "      <td>0.616536</td>\n",
              "      <td>0.210938</td>\n",
              "      <td>1.609375</td>\n",
              "      <td>1.398438</td>\n",
              "      <td>0.281869</td>\n",
              "      <td>happy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>905</th>\n",
              "      <td>1244</td>\n",
              "      <td>1245</td>\n",
              "      <td>1437</td>\n",
              "      <td>0.235383</td>\n",
              "      <td>0.045303</td>\n",
              "      <td>0.248974</td>\n",
              "      <td>0.220745</td>\n",
              "      <td>0.264233</td>\n",
              "      <td>0.043488</td>\n",
              "      <td>2.474743</td>\n",
              "      <td>...</td>\n",
              "      <td>0.235383</td>\n",
              "      <td>0.189293</td>\n",
              "      <td>0.031250</td>\n",
              "      <td>0.275862</td>\n",
              "      <td>1.115723</td>\n",
              "      <td>0.265625</td>\n",
              "      <td>5.500000</td>\n",
              "      <td>5.234375</td>\n",
              "      <td>0.167861</td>\n",
              "      <td>happy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>906</th>\n",
              "      <td>1245</td>\n",
              "      <td>1246</td>\n",
              "      <td>1438</td>\n",
              "      <td>0.231211</td>\n",
              "      <td>0.044793</td>\n",
              "      <td>0.234847</td>\n",
              "      <td>0.221477</td>\n",
              "      <td>0.262090</td>\n",
              "      <td>0.040613</td>\n",
              "      <td>2.607668</td>\n",
              "      <td>...</td>\n",
              "      <td>0.231211</td>\n",
              "      <td>0.171805</td>\n",
              "      <td>0.022346</td>\n",
              "      <td>0.275862</td>\n",
              "      <td>1.070801</td>\n",
              "      <td>0.265625</td>\n",
              "      <td>4.554688</td>\n",
              "      <td>4.289062</td>\n",
              "      <td>0.214936</td>\n",
              "      <td>happy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>907</th>\n",
              "      <td>1246</td>\n",
              "      <td>1247</td>\n",
              "      <td>1439</td>\n",
              "      <td>0.213587</td>\n",
              "      <td>0.082267</td>\n",
              "      <td>0.249435</td>\n",
              "      <td>0.207680</td>\n",
              "      <td>0.268538</td>\n",
              "      <td>0.060858</td>\n",
              "      <td>3.460579</td>\n",
              "      <td>...</td>\n",
              "      <td>0.213587</td>\n",
              "      <td>0.155277</td>\n",
              "      <td>0.020592</td>\n",
              "      <td>0.275862</td>\n",
              "      <td>1.724888</td>\n",
              "      <td>0.273438</td>\n",
              "      <td>6.812500</td>\n",
              "      <td>6.539062</td>\n",
              "      <td>0.238857</td>\n",
              "      <td>happy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>908</th>\n",
              "      <td>1247</td>\n",
              "      <td>1248</td>\n",
              "      <td>1440</td>\n",
              "      <td>0.212537</td>\n",
              "      <td>0.078746</td>\n",
              "      <td>0.245034</td>\n",
              "      <td>0.209794</td>\n",
              "      <td>0.264031</td>\n",
              "      <td>0.054238</td>\n",
              "      <td>2.563983</td>\n",
              "      <td>...</td>\n",
              "      <td>0.212537</td>\n",
              "      <td>0.162938</td>\n",
              "      <td>0.024845</td>\n",
              "      <td>0.258065</td>\n",
              "      <td>0.915625</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>6.281250</td>\n",
              "      <td>6.281250</td>\n",
              "      <td>0.193141</td>\n",
              "      <td>happy</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>909 rows × 24 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e30d2891-7490-4c82-a917-98d1ddabe911')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e30d2891-7490-4c82-a917-98d1ddabe911 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e30d2891-7490-4c82-a917-98d1ddabe911');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "El dataset cotiene unicamente datos de tipo númerico que representan parámetros estadísticos que describen la frecuencia de las señales de voz como lo son la media, la desviación estandar, el rango intercuartilico, etc.\n",
        "\n",
        "Las posibles emociones que describen la señal de voz son 3 (enojado,triste y feliz), las cuales correponden a las 3 categorías de nuestra variable objetivo llamada **label**.\n",
        "\n",
        "En este caso, como varios de los atributos son parámetros estadísticos entonces no hace falta normalizar las columnas."
      ],
      "metadata": {
        "id": "AhRUla-KnSy0"
      },
      "id": "AhRUla-KnSy0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se eliminan las columnas de tipo índice que no aportan información"
      ],
      "metadata": {
        "id": "s9EMXXcioOKy"
      },
      "id": "s9EMXXcioOKy"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7d415f5",
      "metadata": {
        "id": "c7d415f5"
      },
      "outputs": [],
      "source": [
        "df = df.drop(columns= ['Unnamed: 0', 'X', 'Unnamed: 0.1'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creamos los dummies de la variable objetivo, **label**, ya que es una variable categorica, y se convierte cada valor de la celda en una columna diferente de tipo númerica"
      ],
      "metadata": {
        "id": "hPb6zJ7cauRi"
      },
      "id": "hPb6zJ7cauRi"
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.get_dummies(df, 'label')"
      ],
      "metadata": {
        "id": "CxdIGQ_EFuak"
      },
      "id": "CxdIGQ_EFuak",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "OKP3Vx5j_PlE",
        "outputId": "f5e67a2d-a651-473c-ffbd-cdecc1632e1e"
      },
      "id": "OKP3Vx5j_PlE",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     meanfreq        sd    median       Q25       Q75       IQR      skew  \\\n",
              "0    0.181338  0.060495  0.187476  0.126197  0.233586  0.107389  0.869088   \n",
              "1    0.186897  0.062260  0.195070  0.130847  0.243987  0.113140  1.191767   \n",
              "2    0.189102  0.062901  0.204945  0.131422  0.249978  0.118556  1.312690   \n",
              "3    0.183036  0.060051  0.174115  0.129949  0.236967  0.107017  1.096409   \n",
              "4    0.168793  0.057910  0.156266  0.116783  0.216326  0.099543  1.386837   \n",
              "..        ...       ...       ...       ...       ...       ...       ...   \n",
              "904  0.244013  0.035477  0.254385  0.229653  0.265573  0.035920  2.214752   \n",
              "905  0.235383  0.045303  0.248974  0.220745  0.264233  0.043488  2.474743   \n",
              "906  0.231211  0.044793  0.234847  0.221477  0.262090  0.040613  2.607668   \n",
              "907  0.213587  0.082267  0.249435  0.207680  0.268538  0.060858  3.460579   \n",
              "908  0.212537  0.078746  0.245034  0.209794  0.264031  0.054238  2.563983   \n",
              "\n",
              "          kurt    sp.ent       sfm  ...    minfun    maxfun   meandom  \\\n",
              "0     2.863717  0.923566  0.307220  ...  0.023022  0.271186  0.777344   \n",
              "1     3.878650  0.918848  0.298859  ...  0.018412  0.271186  0.930339   \n",
              "2     4.589995  0.919519  0.313069  ...  0.083333  0.262295  0.332386   \n",
              "3     3.680995  0.921361  0.329295  ...  0.044693  0.258065  1.012019   \n",
              "4     5.031744  0.926238  0.337047  ...  0.022472  0.235294  0.228795   \n",
              "..         ...       ...       ...  ...       ...       ...       ...   \n",
              "904   7.565052  0.821874  0.136933  ...  0.028829  0.271186  0.616536   \n",
              "905   9.959019  0.848109  0.236957  ...  0.031250  0.275862  1.115723   \n",
              "906  10.698821  0.848702  0.241998  ...  0.022346  0.275862  1.070801   \n",
              "907  18.034614  0.882544  0.425394  ...  0.020592  0.275862  1.724888   \n",
              "908  10.392885  0.887389  0.404993  ...  0.024845  0.258065  0.915625   \n",
              "\n",
              "       mindom    maxdom   dfrange   modindx  label_angry  label_happy  \\\n",
              "0    0.085938  6.226562  6.140625  0.116586            0            0   \n",
              "1    0.085938  4.000000  3.914062  0.144983            0            0   \n",
              "2    0.085938  0.625000  0.539062  0.334783            0            0   \n",
              "3    0.085938  5.468750  5.382812  0.304910            0            0   \n",
              "4    0.093750  0.750000  0.656250  0.306777            0            0   \n",
              "..        ...       ...       ...       ...          ...          ...   \n",
              "904  0.210938  1.609375  1.398438  0.281869            0            1   \n",
              "905  0.265625  5.500000  5.234375  0.167861            0            1   \n",
              "906  0.265625  4.554688  4.289062  0.214936            0            1   \n",
              "907  0.273438  6.812500  6.539062  0.238857            0            1   \n",
              "908  0.000000  6.281250  6.281250  0.193141            0            1   \n",
              "\n",
              "     label_sad  \n",
              "0            1  \n",
              "1            1  \n",
              "2            1  \n",
              "3            1  \n",
              "4            1  \n",
              "..         ...  \n",
              "904          0  \n",
              "905          0  \n",
              "906          0  \n",
              "907          0  \n",
              "908          0  \n",
              "\n",
              "[909 rows x 23 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-86d904a0-ff0f-4ac1-8366-0a80fa840912\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>meanfreq</th>\n",
              "      <th>sd</th>\n",
              "      <th>median</th>\n",
              "      <th>Q25</th>\n",
              "      <th>Q75</th>\n",
              "      <th>IQR</th>\n",
              "      <th>skew</th>\n",
              "      <th>kurt</th>\n",
              "      <th>sp.ent</th>\n",
              "      <th>sfm</th>\n",
              "      <th>...</th>\n",
              "      <th>minfun</th>\n",
              "      <th>maxfun</th>\n",
              "      <th>meandom</th>\n",
              "      <th>mindom</th>\n",
              "      <th>maxdom</th>\n",
              "      <th>dfrange</th>\n",
              "      <th>modindx</th>\n",
              "      <th>label_angry</th>\n",
              "      <th>label_happy</th>\n",
              "      <th>label_sad</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.181338</td>\n",
              "      <td>0.060495</td>\n",
              "      <td>0.187476</td>\n",
              "      <td>0.126197</td>\n",
              "      <td>0.233586</td>\n",
              "      <td>0.107389</td>\n",
              "      <td>0.869088</td>\n",
              "      <td>2.863717</td>\n",
              "      <td>0.923566</td>\n",
              "      <td>0.307220</td>\n",
              "      <td>...</td>\n",
              "      <td>0.023022</td>\n",
              "      <td>0.271186</td>\n",
              "      <td>0.777344</td>\n",
              "      <td>0.085938</td>\n",
              "      <td>6.226562</td>\n",
              "      <td>6.140625</td>\n",
              "      <td>0.116586</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.186897</td>\n",
              "      <td>0.062260</td>\n",
              "      <td>0.195070</td>\n",
              "      <td>0.130847</td>\n",
              "      <td>0.243987</td>\n",
              "      <td>0.113140</td>\n",
              "      <td>1.191767</td>\n",
              "      <td>3.878650</td>\n",
              "      <td>0.918848</td>\n",
              "      <td>0.298859</td>\n",
              "      <td>...</td>\n",
              "      <td>0.018412</td>\n",
              "      <td>0.271186</td>\n",
              "      <td>0.930339</td>\n",
              "      <td>0.085938</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>3.914062</td>\n",
              "      <td>0.144983</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.189102</td>\n",
              "      <td>0.062901</td>\n",
              "      <td>0.204945</td>\n",
              "      <td>0.131422</td>\n",
              "      <td>0.249978</td>\n",
              "      <td>0.118556</td>\n",
              "      <td>1.312690</td>\n",
              "      <td>4.589995</td>\n",
              "      <td>0.919519</td>\n",
              "      <td>0.313069</td>\n",
              "      <td>...</td>\n",
              "      <td>0.083333</td>\n",
              "      <td>0.262295</td>\n",
              "      <td>0.332386</td>\n",
              "      <td>0.085938</td>\n",
              "      <td>0.625000</td>\n",
              "      <td>0.539062</td>\n",
              "      <td>0.334783</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.183036</td>\n",
              "      <td>0.060051</td>\n",
              "      <td>0.174115</td>\n",
              "      <td>0.129949</td>\n",
              "      <td>0.236967</td>\n",
              "      <td>0.107017</td>\n",
              "      <td>1.096409</td>\n",
              "      <td>3.680995</td>\n",
              "      <td>0.921361</td>\n",
              "      <td>0.329295</td>\n",
              "      <td>...</td>\n",
              "      <td>0.044693</td>\n",
              "      <td>0.258065</td>\n",
              "      <td>1.012019</td>\n",
              "      <td>0.085938</td>\n",
              "      <td>5.468750</td>\n",
              "      <td>5.382812</td>\n",
              "      <td>0.304910</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.168793</td>\n",
              "      <td>0.057910</td>\n",
              "      <td>0.156266</td>\n",
              "      <td>0.116783</td>\n",
              "      <td>0.216326</td>\n",
              "      <td>0.099543</td>\n",
              "      <td>1.386837</td>\n",
              "      <td>5.031744</td>\n",
              "      <td>0.926238</td>\n",
              "      <td>0.337047</td>\n",
              "      <td>...</td>\n",
              "      <td>0.022472</td>\n",
              "      <td>0.235294</td>\n",
              "      <td>0.228795</td>\n",
              "      <td>0.093750</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.656250</td>\n",
              "      <td>0.306777</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>904</th>\n",
              "      <td>0.244013</td>\n",
              "      <td>0.035477</td>\n",
              "      <td>0.254385</td>\n",
              "      <td>0.229653</td>\n",
              "      <td>0.265573</td>\n",
              "      <td>0.035920</td>\n",
              "      <td>2.214752</td>\n",
              "      <td>7.565052</td>\n",
              "      <td>0.821874</td>\n",
              "      <td>0.136933</td>\n",
              "      <td>...</td>\n",
              "      <td>0.028829</td>\n",
              "      <td>0.271186</td>\n",
              "      <td>0.616536</td>\n",
              "      <td>0.210938</td>\n",
              "      <td>1.609375</td>\n",
              "      <td>1.398438</td>\n",
              "      <td>0.281869</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>905</th>\n",
              "      <td>0.235383</td>\n",
              "      <td>0.045303</td>\n",
              "      <td>0.248974</td>\n",
              "      <td>0.220745</td>\n",
              "      <td>0.264233</td>\n",
              "      <td>0.043488</td>\n",
              "      <td>2.474743</td>\n",
              "      <td>9.959019</td>\n",
              "      <td>0.848109</td>\n",
              "      <td>0.236957</td>\n",
              "      <td>...</td>\n",
              "      <td>0.031250</td>\n",
              "      <td>0.275862</td>\n",
              "      <td>1.115723</td>\n",
              "      <td>0.265625</td>\n",
              "      <td>5.500000</td>\n",
              "      <td>5.234375</td>\n",
              "      <td>0.167861</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>906</th>\n",
              "      <td>0.231211</td>\n",
              "      <td>0.044793</td>\n",
              "      <td>0.234847</td>\n",
              "      <td>0.221477</td>\n",
              "      <td>0.262090</td>\n",
              "      <td>0.040613</td>\n",
              "      <td>2.607668</td>\n",
              "      <td>10.698821</td>\n",
              "      <td>0.848702</td>\n",
              "      <td>0.241998</td>\n",
              "      <td>...</td>\n",
              "      <td>0.022346</td>\n",
              "      <td>0.275862</td>\n",
              "      <td>1.070801</td>\n",
              "      <td>0.265625</td>\n",
              "      <td>4.554688</td>\n",
              "      <td>4.289062</td>\n",
              "      <td>0.214936</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>907</th>\n",
              "      <td>0.213587</td>\n",
              "      <td>0.082267</td>\n",
              "      <td>0.249435</td>\n",
              "      <td>0.207680</td>\n",
              "      <td>0.268538</td>\n",
              "      <td>0.060858</td>\n",
              "      <td>3.460579</td>\n",
              "      <td>18.034614</td>\n",
              "      <td>0.882544</td>\n",
              "      <td>0.425394</td>\n",
              "      <td>...</td>\n",
              "      <td>0.020592</td>\n",
              "      <td>0.275862</td>\n",
              "      <td>1.724888</td>\n",
              "      <td>0.273438</td>\n",
              "      <td>6.812500</td>\n",
              "      <td>6.539062</td>\n",
              "      <td>0.238857</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>908</th>\n",
              "      <td>0.212537</td>\n",
              "      <td>0.078746</td>\n",
              "      <td>0.245034</td>\n",
              "      <td>0.209794</td>\n",
              "      <td>0.264031</td>\n",
              "      <td>0.054238</td>\n",
              "      <td>2.563983</td>\n",
              "      <td>10.392885</td>\n",
              "      <td>0.887389</td>\n",
              "      <td>0.404993</td>\n",
              "      <td>...</td>\n",
              "      <td>0.024845</td>\n",
              "      <td>0.258065</td>\n",
              "      <td>0.915625</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>6.281250</td>\n",
              "      <td>6.281250</td>\n",
              "      <td>0.193141</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>909 rows × 23 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-86d904a0-ff0f-4ac1-8366-0a80fa840912')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-86d904a0-ff0f-4ac1-8366-0a80fa840912 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-86d904a0-ff0f-4ac1-8366-0a80fa840912');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creamos los conjuntos de entrenamiento (60%), validación (20%) y test (20%)"
      ],
      "metadata": {
        "id": "qpTcggUYbihN"
      },
      "id": "qpTcggUYbihN"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36cd08f8",
      "metadata": {
        "id": "36cd08f8"
      },
      "outputs": [],
      "source": [
        "test= df[546:726]\n",
        "val= df[726:]\n",
        "train= df[:546]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora definimos la clase 'MyDataset' para convertir los datos de los atributos y la variable objetivo a parejas ordenadas de tensores y así alimentar la red neuronal"
      ],
      "metadata": {
        "id": "GYXO2HHLb9ai"
      },
      "id": "GYXO2HHLb9ai"
    },
    {
      "cell_type": "code",
      "source": [
        "class MyDataset():\n",
        "\n",
        "  def __init__(self, df, target_columns):\n",
        "    y = df.iloc[:, target_columns].values\n",
        "    X = df.drop(columns=df.columns[target_columns]).values\n",
        "    self.X = torch.tensor(X, dtype=torch.float32)\n",
        "    self.y = torch.tensor(y, dtype=torch.float32)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.y)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.X[idx], self.y[idx]\n"
      ],
      "metadata": {
        "id": "v4YSCGhZBDC2"
      },
      "id": "v4YSCGhZBDC2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A continuación hacemos uso de la clase MyDataset para convertir cada conjunto definido anteriormente a tensores. En este caso, le mandamos como atributos el respectivo conjunto y la variable objetivo que se subdivide en tres columnas, una por cada categoría"
      ],
      "metadata": {
        "id": "qMD4szO4ppzS"
      },
      "id": "qMD4szO4ppzS"
    },
    {
      "cell_type": "code",
      "source": [
        "train_sec=MyDataset(train,[20, 21, 22])\n",
        "test_sec=MyDataset(test,[20, 21, 22])\n",
        "val_sec=MyDataset(val,[20, 21, 22])"
      ],
      "metadata": {
        "id": "sHN_ogmWEmEb"
      },
      "id": "sHN_ogmWEmEb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora definimos los dataloaders para procesar los datos y alimentar a la red neuronal por lotes. Lo hicimos con batch_size = 3"
      ],
      "metadata": {
        "id": "Px1ri885qDF1"
      },
      "id": "Px1ri885qDF1"
    },
    {
      "cell_type": "code",
      "source": [
        "train_data=DataLoader(\n",
        "    train_sec,\n",
        "    batch_size=3,\n",
        "    shuffle=False,\n",
        " )\n",
        "\n",
        "test_data=DataLoader(\n",
        "    test_sec,\n",
        "    batch_size=3,\n",
        "    shuffle=False,\n",
        " )\n",
        "\n",
        "val_data=DataLoader(\n",
        "    val_sec,\n",
        "    batch_size=3,\n",
        "    shuffle=False,\n",
        " )"
      ],
      "metadata": {
        "id": "JVPwUpTEEqiC"
      },
      "id": "JVPwUpTEEqiC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se crea la clase Net que define la arquitectura de la red neuronal. Tiene una sola capa oculta con 30 nodos y como función de activación se usó la función sigmoide."
      ],
      "metadata": {
        "id": "kN-SufqCqZIz"
      },
      "id": "kN-SufqCqZIz"
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(20, 30)\n",
        "        self.fc2 = nn.Linear(30, 3)\n",
        "\n",
        "    def forward(self, x):\n",
        "    \t# Do the forward pass\n",
        "        x = F.sigmoid(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "_Qhu3OQiKaBH"
      },
      "id": "_Qhu3OQiKaBH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se define el modelo, el optimizador y la función de costo. Se usó un learning rate de 0.1. Como función de costo se usó CrossEntropyLoss ya que es un problema de clasificación y se adapta mejor a este tipo de problemas."
      ],
      "metadata": {
        "id": "MSv71BMXrQtI"
      },
      "id": "MSv71BMXrQtI"
    },
    {
      "cell_type": "code",
      "source": [
        "model=Net()\n",
        "optimizer=torch.optim.SGD(model.parameters(), lr=0.1)\n",
        "criterion=nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "Jvn33X2-Ppbd"
      },
      "id": "Jvn33X2-Ppbd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train con 100 epochs"
      ],
      "metadata": {
        "id": "bi2BOIgklICb"
      },
      "id": "bi2BOIgklICb"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se define la función para entrenar la red:"
      ],
      "metadata": {
        "id": "RnmKN20EsMob"
      },
      "id": "RnmKN20EsMob"
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model,optimizer,loss_module,train_loader,valid_loader,num_epochs):\n",
        "\n",
        "  valid_loss_min =np.inf  #vamos a encontrar el menor valor de error de validación. Por eso la inicializmaos como 'infinito'\n",
        "\n",
        "  for i in range(num_epochs):\n",
        "    model.train()  #ponemos el modelo en modo entrenamiento\n",
        "    train_loss = 0.0\n",
        "    v_loss = 0.0\n",
        "\n",
        "    for data, target in train_loader:\n",
        "        # se reinician los gradientes\n",
        "        optimizer.zero_grad()\n",
        "        # forward pass: calcular la salida para los datos de entrada..\n",
        "        preds = model(data)\n",
        "        #print(preds)\n",
        "        preds = preds.squeeze(dim=1)\n",
        "        # calculate the batch loss\n",
        "        loss = loss_module(preds, target)\n",
        "        # backpropagation: cálculo de gradientes\n",
        "        loss.backward()\n",
        "        # actualizar los parámetros\n",
        "        optimizer.step()\n",
        "        # actualizar la cuenta de costos a lo largo de los lotes\n",
        "        train_loss += loss.item()*data.size(0)\n",
        "    # for data,labels in testloader:\n",
        "    train_loss = train_loss/len(train_loader.dataset)\n",
        "    print('train_loss:')\n",
        "    print(train_loss)\n",
        "\n",
        "    model.eval() #Ponemos el modelo en modo evaluación.\n",
        "    #for param in model.parameters():\n",
        "    #  print(param.data)\n",
        "    # vamos a evaluar el modelo entrenado, calculando predicciones con el conjunto de validación\n",
        "    for data,target in valid_loader:\n",
        "        output=model(data)\n",
        "        valid_loss= loss_module(output, target)\n",
        "        valid_loss += loss.item()*data.size(0)\n",
        "    valid_loss = valid_loss/len(valid_loader.dataset)\n",
        "    #imprimir estadísticas de entrenamiento y validación\n",
        "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
        "        i, train_loss, valid_loss))\n",
        "\n",
        "\n",
        "    #Guardamos el modelo con el menor error de validación.\n",
        "    if valid_loss <= valid_loss_min:\n",
        "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
        "        valid_loss_min,\n",
        "        valid_loss))\n",
        "        torch.save(model.state_dict(), 'proyecto100epoch.pt')\n",
        "        valid_loss_min = valid_loss"
      ],
      "metadata": {
        "id": "aFr2sX4lLgDT"
      },
      "id": "aFr2sX4lLgDT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora se entrena la red con 100 epochs y se imprime el Training Loss y el Validation Loss."
      ],
      "metadata": {
        "id": "FkrYtinxstXk"
      },
      "id": "FkrYtinxstXk"
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(model,optimizer,criterion,train_data,val_data, 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJqXOyDQPIAt",
        "outputId": "ec64901b-1e83-49c3-c324-bc3787bc91f9"
      },
      "id": "sJqXOyDQPIAt",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_loss:\n",
            "1.1496632744322766\n",
            "Epoch: 0 \tTraining Loss: 1.149663 \tValidation Loss: 0.024657\n",
            "Validation loss decreased (inf --> 0.024657).  Saving model ...\n",
            "train_loss:\n",
            "1.1110562285879155\n",
            "Epoch: 1 \tTraining Loss: 1.111056 \tValidation Loss: 0.024998\n",
            "train_loss:\n",
            "1.103484386598671\n",
            "Epoch: 2 \tTraining Loss: 1.103484 \tValidation Loss: 0.025310\n",
            "train_loss:\n",
            "1.1001260395233448\n",
            "Epoch: 3 \tTraining Loss: 1.100126 \tValidation Loss: 0.025466\n",
            "train_loss:\n",
            "1.0983448251263126\n",
            "Epoch: 4 \tTraining Loss: 1.098345 \tValidation Loss: 0.025522\n",
            "train_loss:\n",
            "1.0972594625347263\n",
            "Epoch: 5 \tTraining Loss: 1.097259 \tValidation Loss: 0.025537\n",
            "train_loss:\n",
            "1.0965345187501594\n",
            "Epoch: 6 \tTraining Loss: 1.096535 \tValidation Loss: 0.025537\n",
            "train_loss:\n",
            "1.0960215814821013\n",
            "Epoch: 7 \tTraining Loss: 1.096022 \tValidation Loss: 0.025534\n",
            "train_loss:\n",
            "1.0956444917144357\n",
            "Epoch: 8 \tTraining Loss: 1.095644 \tValidation Loss: 0.025531\n",
            "train_loss:\n",
            "1.0953544466705112\n",
            "Epoch: 9 \tTraining Loss: 1.095354 \tValidation Loss: 0.025527\n",
            "train_loss:\n",
            "1.095118882891896\n",
            "Epoch: 10 \tTraining Loss: 1.095119 \tValidation Loss: 0.025521\n",
            "train_loss:\n",
            "1.0949193904033074\n",
            "Epoch: 11 \tTraining Loss: 1.094919 \tValidation Loss: 0.025514\n",
            "train_loss:\n",
            "1.0947459805142747\n",
            "Epoch: 12 \tTraining Loss: 1.094746 \tValidation Loss: 0.025507\n",
            "train_loss:\n",
            "1.0945923033651415\n",
            "Epoch: 13 \tTraining Loss: 1.094592 \tValidation Loss: 0.025500\n",
            "train_loss:\n",
            "1.0944537392684393\n",
            "Epoch: 14 \tTraining Loss: 1.094454 \tValidation Loss: 0.025494\n",
            "train_loss:\n",
            "1.0943267741701106\n",
            "Epoch: 15 \tTraining Loss: 1.094327 \tValidation Loss: 0.025487\n",
            "train_loss:\n",
            "1.09420864038415\n",
            "Epoch: 16 \tTraining Loss: 1.094209 \tValidation Loss: 0.025481\n",
            "train_loss:\n",
            "1.0940971983658088\n",
            "Epoch: 17 \tTraining Loss: 1.094097 \tValidation Loss: 0.025474\n",
            "train_loss:\n",
            "1.0939907755825546\n",
            "Epoch: 18 \tTraining Loss: 1.093991 \tValidation Loss: 0.025468\n",
            "train_loss:\n",
            "1.0938879965425847\n",
            "Epoch: 19 \tTraining Loss: 1.093888 \tValidation Loss: 0.025461\n",
            "train_loss:\n",
            "1.0937877909822777\n",
            "Epoch: 20 \tTraining Loss: 1.093788 \tValidation Loss: 0.025453\n",
            "train_loss:\n",
            "1.093689237322126\n",
            "Epoch: 21 \tTraining Loss: 1.093689 \tValidation Loss: 0.025445\n",
            "train_loss:\n",
            "1.0935914634348272\n",
            "Epoch: 22 \tTraining Loss: 1.093591 \tValidation Loss: 0.025436\n",
            "train_loss:\n",
            "1.0934936489377702\n",
            "Epoch: 23 \tTraining Loss: 1.093494 \tValidation Loss: 0.025425\n",
            "train_loss:\n",
            "1.0933948837138794\n",
            "Epoch: 24 \tTraining Loss: 1.093395 \tValidation Loss: 0.025413\n",
            "train_loss:\n",
            "1.0932942772959615\n",
            "Epoch: 25 \tTraining Loss: 1.093294 \tValidation Loss: 0.025399\n",
            "train_loss:\n",
            "1.0931909349593487\n",
            "Epoch: 26 \tTraining Loss: 1.093191 \tValidation Loss: 0.025384\n",
            "train_loss:\n",
            "1.0930839547744164\n",
            "Epoch: 27 \tTraining Loss: 1.093084 \tValidation Loss: 0.025367\n",
            "train_loss:\n",
            "1.0929724249865982\n",
            "Epoch: 28 \tTraining Loss: 1.092972 \tValidation Loss: 0.025348\n",
            "train_loss:\n",
            "1.0928554587311796\n",
            "Epoch: 29 \tTraining Loss: 1.092855 \tValidation Loss: 0.025328\n",
            "train_loss:\n",
            "1.0927322140106788\n",
            "Epoch: 30 \tTraining Loss: 1.092732 \tValidation Loss: 0.025305\n",
            "train_loss:\n",
            "1.0926019346321023\n",
            "Epoch: 31 \tTraining Loss: 1.092602 \tValidation Loss: 0.025282\n",
            "train_loss:\n",
            "1.0924640412514026\n",
            "Epoch: 32 \tTraining Loss: 1.092464 \tValidation Loss: 0.025257\n",
            "train_loss:\n",
            "1.0923180809387794\n",
            "Epoch: 33 \tTraining Loss: 1.092318 \tValidation Loss: 0.025231\n",
            "train_loss:\n",
            "1.0921638513004386\n",
            "Epoch: 34 \tTraining Loss: 1.092164 \tValidation Loss: 0.025204\n",
            "train_loss:\n",
            "1.0920013362889762\n",
            "Epoch: 35 \tTraining Loss: 1.092001 \tValidation Loss: 0.025177\n",
            "train_loss:\n",
            "1.091830748123127\n",
            "Epoch: 36 \tTraining Loss: 1.091831 \tValidation Loss: 0.025150\n",
            "train_loss:\n",
            "1.0916524899529887\n",
            "Epoch: 37 \tTraining Loss: 1.091652 \tValidation Loss: 0.025123\n",
            "train_loss:\n",
            "1.091467152257542\n",
            "Epoch: 38 \tTraining Loss: 1.091467 \tValidation Loss: 0.025097\n",
            "train_loss:\n",
            "1.0912755062946906\n",
            "Epoch: 39 \tTraining Loss: 1.091276 \tValidation Loss: 0.025071\n",
            "train_loss:\n",
            "1.0910784405666394\n",
            "Epoch: 40 \tTraining Loss: 1.091078 \tValidation Loss: 0.025046\n",
            "train_loss:\n",
            "1.0908769703173375\n",
            "Epoch: 41 \tTraining Loss: 1.090877 \tValidation Loss: 0.025022\n",
            "train_loss:\n",
            "1.0906721514004927\n",
            "Epoch: 42 \tTraining Loss: 1.090672 \tValidation Loss: 0.024999\n",
            "train_loss:\n",
            "1.0904650950169825\n",
            "Epoch: 43 \tTraining Loss: 1.090465 \tValidation Loss: 0.024977\n",
            "train_loss:\n",
            "1.0902568976302722\n",
            "Epoch: 44 \tTraining Loss: 1.090257 \tValidation Loss: 0.024957\n",
            "train_loss:\n",
            "1.0900486209890345\n",
            "Epoch: 45 \tTraining Loss: 1.090049 \tValidation Loss: 0.024937\n",
            "train_loss:\n",
            "1.0898412266275386\n",
            "Epoch: 46 \tTraining Loss: 1.089841 \tValidation Loss: 0.024918\n",
            "train_loss:\n",
            "1.089635628592837\n",
            "Epoch: 47 \tTraining Loss: 1.089636 \tValidation Loss: 0.024901\n",
            "train_loss:\n",
            "1.0894325784929506\n",
            "Epoch: 48 \tTraining Loss: 1.089433 \tValidation Loss: 0.024885\n",
            "train_loss:\n",
            "1.0892327496638665\n",
            "Epoch: 49 \tTraining Loss: 1.089233 \tValidation Loss: 0.024870\n",
            "train_loss:\n",
            "1.089036619597739\n",
            "Epoch: 50 \tTraining Loss: 1.089037 \tValidation Loss: 0.024856\n",
            "train_loss:\n",
            "1.0888446088020618\n",
            "Epoch: 51 \tTraining Loss: 1.088845 \tValidation Loss: 0.024842\n",
            "train_loss:\n",
            "1.0886569999076507\n",
            "Epoch: 52 \tTraining Loss: 1.088657 \tValidation Loss: 0.024830\n",
            "train_loss:\n",
            "1.0884739517510593\n",
            "Epoch: 53 \tTraining Loss: 1.088474 \tValidation Loss: 0.024819\n",
            "train_loss:\n",
            "1.0882955593067212\n",
            "Epoch: 54 \tTraining Loss: 1.088296 \tValidation Loss: 0.024808\n",
            "train_loss:\n",
            "1.088121806527232\n",
            "Epoch: 55 \tTraining Loss: 1.088122 \tValidation Loss: 0.024799\n",
            "train_loss:\n",
            "1.087952651492842\n",
            "Epoch: 56 \tTraining Loss: 1.087953 \tValidation Loss: 0.024790\n",
            "train_loss:\n",
            "1.0877879815442222\n",
            "Epoch: 57 \tTraining Loss: 1.087788 \tValidation Loss: 0.024781\n",
            "train_loss:\n",
            "1.087627648979753\n",
            "Epoch: 58 \tTraining Loss: 1.087628 \tValidation Loss: 0.024773\n",
            "train_loss:\n",
            "1.0874714792429745\n",
            "Epoch: 59 \tTraining Loss: 1.087471 \tValidation Loss: 0.024766\n",
            "train_loss:\n",
            "1.0873192728875758\n",
            "Epoch: 60 \tTraining Loss: 1.087319 \tValidation Loss: 0.024760\n",
            "train_loss:\n",
            "1.0871708704220069\n",
            "Epoch: 61 \tTraining Loss: 1.087171 \tValidation Loss: 0.024753\n",
            "train_loss:\n",
            "1.0870260216377594\n",
            "Epoch: 62 \tTraining Loss: 1.087026 \tValidation Loss: 0.024748\n",
            "train_loss:\n",
            "1.086884543463424\n",
            "Epoch: 63 \tTraining Loss: 1.086885 \tValidation Loss: 0.024742\n",
            "train_loss:\n",
            "1.0867462302302267\n",
            "Epoch: 64 \tTraining Loss: 1.086746 \tValidation Loss: 0.024737\n",
            "train_loss:\n",
            "1.0866109086916997\n",
            "Epoch: 65 \tTraining Loss: 1.086611 \tValidation Loss: 0.024733\n",
            "train_loss:\n",
            "1.0864783813665202\n",
            "Epoch: 66 \tTraining Loss: 1.086478 \tValidation Loss: 0.024728\n",
            "train_loss:\n",
            "1.0863484743532243\n",
            "Epoch: 67 \tTraining Loss: 1.086348 \tValidation Loss: 0.024724\n",
            "train_loss:\n",
            "1.0862210540326087\n",
            "Epoch: 68 \tTraining Loss: 1.086221 \tValidation Loss: 0.024720\n",
            "train_loss:\n",
            "1.0860959435557271\n",
            "Epoch: 69 \tTraining Loss: 1.086096 \tValidation Loss: 0.024716\n",
            "train_loss:\n",
            "1.0859730391056983\n",
            "Epoch: 70 \tTraining Loss: 1.085973 \tValidation Loss: 0.024713\n",
            "train_loss:\n",
            "1.085852168418549\n",
            "Epoch: 71 \tTraining Loss: 1.085852 \tValidation Loss: 0.024709\n",
            "train_loss:\n",
            "1.0857332525672494\n",
            "Epoch: 72 \tTraining Loss: 1.085733 \tValidation Loss: 0.024706\n",
            "train_loss:\n",
            "1.0856161546576155\n",
            "Epoch: 73 \tTraining Loss: 1.085616 \tValidation Loss: 0.024703\n",
            "train_loss:\n",
            "1.0855007630128126\n",
            "Epoch: 74 \tTraining Loss: 1.085501 \tValidation Loss: 0.024700\n",
            "train_loss:\n",
            "1.0853869839683994\n",
            "Epoch: 75 \tTraining Loss: 1.085387 \tValidation Loss: 0.024697\n",
            "train_loss:\n",
            "1.0852747281174084\n",
            "Epoch: 76 \tTraining Loss: 1.085275 \tValidation Loss: 0.024694\n",
            "train_loss:\n",
            "1.0851638749405579\n",
            "Epoch: 77 \tTraining Loss: 1.085164 \tValidation Loss: 0.024691\n",
            "train_loss:\n",
            "1.0850543330658924\n",
            "Epoch: 78 \tTraining Loss: 1.085054 \tValidation Loss: 0.024688\n",
            "train_loss:\n",
            "1.0849459852491106\n",
            "Epoch: 79 \tTraining Loss: 1.084946 \tValidation Loss: 0.024685\n",
            "train_loss:\n",
            "1.0848387813830114\n",
            "Epoch: 80 \tTraining Loss: 1.084839 \tValidation Loss: 0.024682\n",
            "train_loss:\n",
            "1.084732602585803\n",
            "Epoch: 81 \tTraining Loss: 1.084733 \tValidation Loss: 0.024679\n",
            "train_loss:\n",
            "1.0846273741879306\n",
            "Epoch: 82 \tTraining Loss: 1.084627 \tValidation Loss: 0.024676\n",
            "train_loss:\n",
            "1.0845229982674778\n",
            "Epoch: 83 \tTraining Loss: 1.084523 \tValidation Loss: 0.024673\n",
            "train_loss:\n",
            "1.0844194175122859\n",
            "Epoch: 84 \tTraining Loss: 1.084419 \tValidation Loss: 0.024669\n",
            "train_loss:\n",
            "1.0843165746101966\n",
            "Epoch: 85 \tTraining Loss: 1.084317 \tValidation Loss: 0.024666\n",
            "train_loss:\n",
            "1.084214413559044\n",
            "Epoch: 86 \tTraining Loss: 1.084214 \tValidation Loss: 0.024663\n",
            "train_loss:\n",
            "1.084112869841712\n",
            "Epoch: 87 \tTraining Loss: 1.084113 \tValidation Loss: 0.024659\n",
            "train_loss:\n",
            "1.0840118890935249\n",
            "Epoch: 88 \tTraining Loss: 1.084012 \tValidation Loss: 0.024656\n",
            "Validation loss decreased (0.024657 --> 0.024656).  Saving model ...\n",
            "train_loss:\n",
            "1.083911466074514\n",
            "Epoch: 89 \tTraining Loss: 1.083911 \tValidation Loss: 0.024652\n",
            "Validation loss decreased (0.024656 --> 0.024652).  Saving model ...\n",
            "train_loss:\n",
            "1.0838115352850695\n",
            "Epoch: 90 \tTraining Loss: 1.083812 \tValidation Loss: 0.024648\n",
            "Validation loss decreased (0.024652 --> 0.024648).  Saving model ...\n",
            "train_loss:\n",
            "1.0837121036026505\n",
            "Epoch: 91 \tTraining Loss: 1.083712 \tValidation Loss: 0.024644\n",
            "Validation loss decreased (0.024648 --> 0.024644).  Saving model ...\n",
            "train_loss:\n",
            "1.0836131356574676\n",
            "Epoch: 92 \tTraining Loss: 1.083613 \tValidation Loss: 0.024640\n",
            "Validation loss decreased (0.024644 --> 0.024640).  Saving model ...\n",
            "train_loss:\n",
            "1.0835146265370506\n",
            "Epoch: 93 \tTraining Loss: 1.083515 \tValidation Loss: 0.024636\n",
            "Validation loss decreased (0.024640 --> 0.024636).  Saving model ...\n",
            "train_loss:\n",
            "1.0834165657614614\n",
            "Epoch: 94 \tTraining Loss: 1.083417 \tValidation Loss: 0.024631\n",
            "Validation loss decreased (0.024636 --> 0.024631).  Saving model ...\n",
            "train_loss:\n",
            "1.0833189500557197\n",
            "Epoch: 95 \tTraining Loss: 1.083319 \tValidation Loss: 0.024627\n",
            "Validation loss decreased (0.024631 --> 0.024627).  Saving model ...\n",
            "train_loss:\n",
            "1.0832217797473236\n",
            "Epoch: 96 \tTraining Loss: 1.083222 \tValidation Loss: 0.024622\n",
            "Validation loss decreased (0.024627 --> 0.024622).  Saving model ...\n",
            "train_loss:\n",
            "1.0831250892235682\n",
            "Epoch: 97 \tTraining Loss: 1.083125 \tValidation Loss: 0.024617\n",
            "Validation loss decreased (0.024622 --> 0.024617).  Saving model ...\n",
            "train_loss:\n",
            "1.0830288663670258\n",
            "Epoch: 98 \tTraining Loss: 1.083029 \tValidation Loss: 0.024612\n",
            "Validation loss decreased (0.024617 --> 0.024612).  Saving model ...\n",
            "train_loss:\n",
            "1.0829331671798623\n",
            "Epoch: 99 \tTraining Loss: 1.082933 \tValidation Loss: 0.024607\n",
            "Validation loss decreased (0.024612 --> 0.024607).  Saving model ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora, se imprimen los parámetros del mejor modelo"
      ],
      "metadata": {
        "id": "NtyBCq0MtZ9r"
      },
      "id": "NtyBCq0MtZ9r"
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in model.named_parameters():\n",
        "    if param.requires_grad:\n",
        "        print (name, param.data)"
      ],
      "metadata": {
        "id": "KgI33XqRIdUZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16bc52c1-b731-4d55-f5db-9792781e6219"
      },
      "id": "KgI33XqRIdUZ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fc1.weight tensor([[ 6.1187e-02,  1.6123e-01, -1.6564e-01, -1.2114e-01, -1.6905e-01,\n",
            "         -2.1478e-01,  9.1257e-02, -3.4172e-01,  7.6409e-02, -4.0329e-01,\n",
            "          2.4411e-02,  1.7701e-02, -8.7576e-02, -2.3700e-01, -8.9269e-02,\n",
            "         -4.1935e-02,  1.1632e-01, -3.1349e-01, -5.3395e-02, -9.9308e-02],\n",
            "        [-1.7141e-01, -1.9082e-01,  2.2348e-01, -6.8932e-02, -6.8689e-02,\n",
            "          8.5449e-02, -1.4671e-01, -3.5028e-01,  1.3734e-01, -1.1140e-01,\n",
            "          1.9466e-01,  2.6242e-02,  2.1890e-01, -1.3478e-01,  1.2893e-01,\n",
            "         -2.4178e-01, -1.6706e-01, -2.5666e-01, -2.4467e-02, -7.5713e-02],\n",
            "        [-5.7343e-03, -1.0757e-01, -2.0510e-01, -2.2446e-01, -5.2248e-02,\n",
            "          1.5052e-01, -2.7757e-01, -1.1237e-02,  6.6789e-04, -4.7797e-02,\n",
            "          6.3233e-04,  7.5951e-02, -2.1941e-01, -2.1115e-01,  9.0904e-02,\n",
            "         -4.5474e-01,  9.0440e-02, -7.8209e-01, -9.6474e-01, -1.2026e-01],\n",
            "        [ 4.8604e-02, -7.8192e-02, -1.3993e-01,  1.6485e-04, -6.9792e-02,\n",
            "          1.5429e-02,  4.0634e-02, -3.8791e-01,  2.1516e-03, -2.4784e-01,\n",
            "          1.6798e-02, -9.2507e-02, -7.7537e-02, -1.4031e-01,  2.2673e-01,\n",
            "         -2.1456e-01, -1.8163e-01, -2.9114e-01,  3.5332e-02,  4.8969e-02],\n",
            "        [ 9.5462e-02, -2.2936e-01,  1.1262e-01, -3.2361e-02,  3.0310e-02,\n",
            "         -1.4456e-01,  5.7183e-02, -2.9336e-01, -1.9463e-01,  5.8055e-02,\n",
            "         -9.5473e-02,  7.4777e-02, -1.2825e-01, -9.4298e-03,  7.7997e-02,\n",
            "         -8.2031e-01, -1.1419e-01, -6.0480e-02, -2.4523e-01, -2.8208e-01],\n",
            "        [-1.3978e-01, -1.5549e-01, -1.0499e-01,  2.4126e-01, -1.7346e-01,\n",
            "          6.5660e-02, -3.2714e-02, -2.7919e-01, -7.0806e-02, -2.4462e-01,\n",
            "         -8.5508e-02,  2.0630e-02,  2.4261e-01,  9.7188e-02, -8.3068e-02,\n",
            "         -1.7476e-02,  2.0591e-01, -2.5386e-01, -1.0672e-01, -1.5944e-01],\n",
            "        [ 5.8850e-02,  1.2801e-01, -1.0987e-01, -6.9628e-03,  1.6951e-01,\n",
            "         -1.8833e-01,  8.4111e-02, -3.8650e-01,  3.1092e-02, -4.4627e-01,\n",
            "         -6.1460e-02, -1.9516e-01,  9.5638e-02,  1.8451e-01,  1.0070e-01,\n",
            "         -3.8374e-01, -2.0517e-01,  1.5940e-02, -2.8805e-01, -2.1561e-02],\n",
            "        [-7.9950e-02, -1.5865e-01,  6.2705e-03,  1.9081e-01,  8.2418e-02,\n",
            "         -1.3492e-02, -1.4450e-01, -6.5480e-02, -3.2891e-01, -3.4728e-01,\n",
            "          5.3467e-02,  8.2704e-02, -2.2697e-01,  1.6230e-01, -7.0403e-02,\n",
            "         -6.3910e-01,  1.4168e-01, -2.0006e-01, -3.2755e-01, -3.1906e-01],\n",
            "        [ 1.0040e-01, -1.4250e-01, -8.6860e-02,  1.0703e-01, -1.5124e-02,\n",
            "          1.1863e-01,  1.2856e-01, -5.5440e-01,  1.6299e-01, -1.2291e-01,\n",
            "          1.4422e-01, -1.0476e-01,  1.9839e-02,  6.6742e-02, -2.2304e-01,\n",
            "         -2.9689e-01,  8.9420e-02, -6.3938e-02,  7.0700e-03, -6.4560e-02],\n",
            "        [-1.6755e-02, -5.7727e-02, -1.7548e-01, -1.6736e-01, -1.0492e-01,\n",
            "          1.2144e-01,  1.0714e-01, -2.8824e-01,  2.2244e-01,  1.5143e-01,\n",
            "         -1.2815e-01, -2.0734e-01, -5.8648e-02,  2.2419e-02, -7.8670e-02,\n",
            "         -2.7157e-01, -2.0229e-01, -6.5829e-01, -2.9440e-01, -1.8404e-02],\n",
            "        [-2.4971e-01,  5.2622e-02,  6.3492e-02, -1.9479e-01, -4.8402e-02,\n",
            "          1.8080e-01, -1.2818e+00, -2.4754e+00, -2.4699e-01,  6.1718e-02,\n",
            "         -1.2913e-01, -1.2236e-01,  1.0375e-01,  1.0320e-01, -5.3628e-02,\n",
            "          4.5309e-01, -5.4577e-02,  1.0242e+00,  1.0952e+00, -1.3083e-01],\n",
            "        [ 1.3413e-01,  1.1624e-01, -2.0778e-02,  3.0760e-02,  3.7469e-02,\n",
            "          2.1620e-01, -4.7488e-02, -3.7992e-01,  1.3815e-01, -2.8422e-01,\n",
            "          1.4813e-01,  1.9648e-01, -2.0437e-01, -1.4940e-01, -1.4014e-01,\n",
            "         -5.0724e-01, -2.3535e-01, -1.7387e-01, -1.7829e-02, -2.6197e-01],\n",
            "        [ 1.9419e-01,  1.3807e-01,  8.6870e-02,  2.2284e-01, -9.7803e-02,\n",
            "         -9.7846e-02, -2.4824e-01,  7.1212e-02, -1.5739e-01, -3.7339e-01,\n",
            "         -5.5207e-02,  5.4358e-02,  1.1100e-01, -2.1464e-01, -1.6408e-02,\n",
            "         -3.8147e-01,  1.5482e-01, -3.0004e-01, -5.3694e-01, -2.0038e-01],\n",
            "        [ 7.2687e-02, -9.9861e-02,  2.2123e-01,  1.9579e-01, -2.0808e-01,\n",
            "          1.6858e-01, -4.8875e-02, -3.2193e-01, -1.9959e-01, -1.0636e-02,\n",
            "         -9.4242e-02,  1.5403e-01,  1.4522e-01,  6.7871e-02, -1.0478e-01,\n",
            "         -2.7345e-01, -2.7854e-02, -1.4864e-01, -1.8415e-01,  1.3455e-01],\n",
            "        [ 2.3489e-02,  4.9464e-02, -1.8886e-01, -1.6288e-01, -8.1882e-02,\n",
            "         -1.7967e-03, -1.6014e-01, -3.8429e-02,  4.7619e-02,  4.9006e-02,\n",
            "         -6.5927e-02,  1.6600e-01,  4.4436e-02, -5.8499e-02, -2.1452e-01,\n",
            "         -7.2896e-01, -5.0740e-02, -9.2451e-01, -5.0206e-01, -3.7789e-01],\n",
            "        [-1.8745e-01, -2.2700e-01, -1.6649e-01, -1.9172e-02, -8.6210e-02,\n",
            "          6.3843e-02, -9.6375e-02, -6.0810e-02, -3.2038e-01, -1.8405e-01,\n",
            "          1.1021e-01, -1.1096e-02, -2.1063e-01,  1.5594e-01,  3.4004e-02,\n",
            "         -7.6692e-02,  1.3373e-01, -3.7010e-01, -2.0259e-01, -2.2158e-01],\n",
            "        [-8.9929e-02, -1.5834e-01, -2.3959e-01,  3.7399e-02,  9.5497e-02,\n",
            "         -1.7280e-01,  1.3794e-01, -8.7080e-02, -1.5784e-01, -4.8559e-01,\n",
            "         -2.9587e-02, -6.9464e-02, -2.5126e-01, -7.7295e-02, -2.4403e-01,\n",
            "         -6.8986e-01, -1.0057e-01, -1.2694e-01, -3.0331e-01, -1.3837e-01],\n",
            "        [-9.5995e-02, -2.2902e-01,  7.4816e-02,  9.1653e-02, -1.5592e-01,\n",
            "         -8.4800e-02,  4.6891e-03,  1.1991e-01, -1.3758e-01, -4.6572e-01,\n",
            "         -1.7716e-01,  2.2118e-01, -1.0648e-01, -4.1133e-02,  8.1603e-02,\n",
            "         -7.2353e-02,  1.8125e-01, -4.2273e-01, -6.2614e-01, -1.4631e-01],\n",
            "        [ 1.9268e-01, -1.8618e-01, -1.2660e-01,  4.9220e-02,  1.0582e-01,\n",
            "          1.1457e-02,  1.3035e-03, -1.5663e-01,  1.1276e-02, -5.0956e-01,\n",
            "          1.7645e-01,  2.3614e-01, -1.9117e-01, -2.1372e-01, -1.5467e-02,\n",
            "         -4.4572e-01,  3.5678e-02, -1.9730e-01, -2.6906e-01, -4.6262e-02],\n",
            "        [-1.4516e-02, -7.2377e-02,  1.8173e-01, -9.9793e-02, -1.8045e-01,\n",
            "          7.7329e-02,  6.7116e-02, -2.9739e-01, -2.1510e-01,  4.6498e-03,\n",
            "          1.2750e-01,  2.1074e-01, -1.6290e-01, -2.5875e-02,  7.3672e-03,\n",
            "         -3.6757e-01,  1.1298e-01, -3.1774e-01, -3.7259e-02, -1.4652e-01],\n",
            "        [-2.6683e-02,  1.1557e-01,  1.9121e-02, -9.1052e-02, -6.5061e-02,\n",
            "         -1.8671e-02, -6.2864e-02, -2.6968e-01, -5.0498e-02, -3.8027e-01,\n",
            "         -1.9042e-02, -1.9452e-01,  1.5640e-01, -1.1510e-01, -1.7256e-01,\n",
            "         -2.8296e-01, -9.4222e-02, -1.4702e-01, -2.2046e-01,  1.1480e-01],\n",
            "        [-2.3370e-02, -8.4526e-02, -1.9991e-01, -2.9977e-02,  2.8453e-02,\n",
            "          4.3316e-02, -8.0471e-02, -2.0079e-01,  8.3967e-02, -2.5623e-02,\n",
            "          3.0057e-02, -1.0901e-01,  1.4510e-01,  1.7777e-01,  1.1528e-01,\n",
            "         -5.3060e-01,  1.6920e-01, -4.2903e-01, -6.3695e-01, -1.0207e-02],\n",
            "        [-1.2978e-01,  1.6422e-01, -1.2117e-01, -1.0487e-01, -2.5047e-01,\n",
            "         -5.4733e-02,  5.1836e-02, -6.1075e-02, -3.1458e-01, -4.1808e-01,\n",
            "         -1.7468e-01,  3.2626e-02, -5.3298e-02,  8.5562e-02,  2.1555e-02,\n",
            "         -9.2923e-01, -1.8750e-02, -1.6386e-01, -8.8938e-02, -2.7263e-01],\n",
            "        [-1.4997e-01, -3.8598e-02, -1.5109e-01, -4.7198e-03,  2.1893e-04,\n",
            "         -1.6107e-02, -1.9562e-01, -3.6340e-01,  2.9558e-02,  1.0507e-01,\n",
            "          1.0280e-01,  5.6045e-02, -4.3367e-02, -4.9873e-02,  5.3987e-02,\n",
            "         -3.4480e-01, -1.7985e-01, -1.2134e-02, -2.1299e-01, -2.0464e-01],\n",
            "        [-1.8088e-01, -4.8796e-03, -1.0161e-02, -1.2798e-01, -2.0465e-01,\n",
            "         -3.1514e-02, -1.0509e-02, -2.0359e-02, -3.5068e-01, -4.5723e-01,\n",
            "         -1.6978e-01,  2.2281e-01,  5.6459e-02,  4.3210e-02,  9.1888e-02,\n",
            "         -2.1541e-01,  1.0035e-01, -3.1715e-01, -2.9746e-01, -1.7199e-01],\n",
            "        [-1.8327e-01,  3.8273e-02, -1.7635e-01, -5.4975e-02,  1.7041e-01,\n",
            "          3.0022e-03, -1.2567e-01, -3.1970e-01, -2.3713e-01,  7.5304e-02,\n",
            "         -2.1551e-01,  3.0842e-02, -8.6796e-02,  6.9288e-02,  1.7405e-01,\n",
            "         -1.6101e-01, -2.2409e-01, -1.4590e-01, -2.1768e-01, -2.3706e-01],\n",
            "        [-1.7215e-01, -8.3173e-02, -1.2350e-01,  2.2162e-01, -1.5326e-01,\n",
            "         -2.5715e-01,  5.3463e-02,  5.0229e-03, -2.4081e-01, -5.8284e-01,\n",
            "          1.3728e-01, -4.6928e-02,  7.1253e-02,  1.4374e-01, -1.1054e-01,\n",
            "         -6.2339e-01,  2.3589e-01, -2.4604e-01, -3.1885e-01, -2.7694e-01],\n",
            "        [ 1.7758e-02, -7.6715e-02,  1.0993e-01,  2.6183e-01,  1.0248e-01,\n",
            "         -1.2505e-01,  1.3031e-01,  1.4619e-01, -3.9240e-01, -5.1026e-01,\n",
            "         -1.2591e-01,  1.2073e-01, -2.6439e-02, -1.7739e-01, -1.5734e-02,\n",
            "          1.2090e-01,  4.1335e-01, -6.3172e-01, -5.8141e-01,  1.6178e-01],\n",
            "        [-2.7220e-02, -2.7929e-01,  2.0187e-01,  7.0261e-02,  1.7723e-01,\n",
            "          1.2896e-02, -2.0273e-01,  2.1014e-01, -4.9559e-01, -3.5498e-01,\n",
            "         -1.9827e-01, -1.6206e-02,  5.1182e-02, -5.7022e-02,  4.0254e-02,\n",
            "         -1.0235e-01,  4.7085e-01, -5.8928e-01, -7.5542e-01,  1.5642e-01],\n",
            "        [ 4.3761e-02,  1.5113e-01, -2.0194e-01,  2.8143e-02, -2.0941e-01,\n",
            "         -1.8010e-01,  2.4984e-02, -2.6827e-01, -1.7398e-01, -3.6087e-01,\n",
            "         -2.5535e-01,  9.4256e-02, -6.2714e-02,  1.2107e-01,  1.8129e-01,\n",
            "         -4.9713e-01, -1.4882e-01, -7.9513e-02, -1.9615e-01, -1.2857e-01]])\n",
            "fc1.bias tensor([ 0.1098, -0.1451,  0.0342, -0.0342,  0.1576, -0.0882,  0.1862,  0.0795,\n",
            "        -0.0391, -0.1003, -0.1993,  0.0042, -0.2686,  0.0371, -0.0609, -0.1913,\n",
            "        -0.2600, -0.4628, -0.1156, -0.0823,  0.1464,  0.0207, -0.3840, -0.0454,\n",
            "        -0.1639,  0.0565, -0.4258, -0.2996, -0.5237,  0.0528])\n",
            "fc2.weight tensor([[-2.8990e-01, -4.4416e-01, -3.8102e-01, -4.0310e-01, -5.4169e-01,\n",
            "         -3.6860e-01, -6.2820e-01, -3.6499e-01, -3.4201e-01, -3.5101e-01,\n",
            "          8.8090e-01, -3.9023e-01, -2.4591e-01, -3.9365e-01, -4.6895e-01,\n",
            "         -1.9440e-01, -2.7211e-01, -2.0134e-01, -3.2306e-01, -5.0595e-01,\n",
            "         -2.6562e-01, -4.0995e-01, -1.3457e-01, -4.2575e-01, -2.1007e-01,\n",
            "         -3.8070e-01, -2.7112e-01, -1.5290e-01, -2.3010e-01, -3.7669e-01],\n",
            "        [ 4.0589e-01,  2.5385e-01, -1.5956e-01,  3.1075e-01,  5.0469e-04,\n",
            "          2.8260e-01,  9.3809e-02,  2.5366e-02,  1.9069e-01,  5.1553e-02,\n",
            "         -8.6265e-01,  2.2305e-01,  1.3738e-01,  2.7473e-01, -2.6010e-01,\n",
            "          1.9198e-01,  2.8311e-02,  2.4000e-01,  1.6003e-01,  8.1816e-02,\n",
            "          3.5706e-01, -2.0853e-02, -3.6718e-02,  1.4399e-01,  1.4294e-01,\n",
            "          1.9610e-01,  7.4600e-02,  4.1577e-01,  2.9430e-01,  1.5367e-01],\n",
            "        [ 1.9603e-01,  8.7009e-02,  5.2194e-01,  1.6483e-01,  3.3095e-01,\n",
            "          3.6030e-02,  3.8756e-02,  1.8592e-01,  9.2900e-02,  4.2828e-01,\n",
            "         -1.3579e-01,  3.1771e-01,  1.1288e-01,  1.1271e-01,  4.8211e-01,\n",
            "          1.3303e-01,  2.3275e-01,  4.8110e-02,  1.9874e-01,  6.4664e-02,\n",
            "          2.4664e-01,  3.2533e-01,  3.4961e-01,  9.1283e-02,  1.4378e-01,\n",
            "          1.4022e-01,  1.7558e-01, -2.7974e-02,  1.0525e-02,  2.6628e-01]])\n",
            "fc2.bias tensor([ 0.2242, -0.2916, -0.0491])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se imprime el test_loss del mejor modelo"
      ],
      "metadata": {
        "id": "vJ6ffdLSue_Z"
      },
      "id": "vJ6ffdLSue_Z"
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss=0.0\n",
        "\n",
        "criterion= nn.CrossEntropyLoss()\n",
        "for data, target in test_data:\n",
        "  output=model(data)\n",
        "  loss= criterion(output,target)\n",
        "  test_loss += loss.item()*data.size(0)\n",
        "test_loss = test_loss/len(test_data.dataset)\n",
        "print('Test Loss: {:.6f}\\n'.format(test_loss))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TDAjpBYLIiYF",
        "outputId": "eb9cb367-4f11-4aae-9f01-2bd9e95558f3"
      },
      "id": "TDAjpBYLIiYF",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 1.071757\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Por último, se evalua el desempeño del modelo mediante la métrica accuracy. Se van a tomar 400 valores aleatorios de todo el dataset  y se calcula la métrica dividiendo el número de predicciones correctas sobre 400."
      ],
      "metadata": {
        "id": "zpZlt9snymvD"
      },
      "id": "zpZlt9snymvD"
    },
    {
      "cell_type": "code",
      "source": [
        "data_sec=MyDataset(df,[20, 21, 22])"
      ],
      "metadata": {
        "id": "JOhMgb8PB6IA"
      },
      "id": "JOhMgb8PB6IA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_total_predictions=400\n",
        "num_predicciones_correctas=0\n",
        "for i in range(num_total_predictions):\n",
        "  i = random.randrange(1, 905)\n",
        "  tensor_prediccion = data_sec[i][1]\n",
        "  tensor_columnas= data_sec[i][0]\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    tensor_modelo = model(tensor_columnas)\n",
        "  if np.argmax(tensor_prediccion)== np.argmax(tensor_modelo):\n",
        "    num_predicciones_correctas+=1\n",
        "\n",
        "print(\"Las predicciones que estan bien son:\", num_predicciones_correctas)\n",
        "print(\"Total predicciones:\", num_total_predictions)\n",
        "print(\"Accuracy:\", num_predicciones_correctas/num_total_predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0tehMj2ZB0yl",
        "outputId": "bf581e1a-0aa6-4f41-8e7c-d9d5fac74c29"
      },
      "id": "0tehMj2ZB0yl",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Las predicciones que estan bien son: 153\n",
            "Total predicciones: 400\n",
            "Accuracy: 0.3825\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se elige un registro del dataset de manera aleatoria y se usa el modelo para predecir el sentimiento."
      ],
      "metadata": {
        "id": "vuPi-NRcCdkr"
      },
      "id": "vuPi-NRcCdkr"
    },
    {
      "cell_type": "code",
      "source": [
        "registro = random.randrange(1,905)\n",
        "tensor_prediccion = data_sec[registro][1]\n",
        "tensor_columnas= data_sec[registro][0]\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    tensor_modelo = model(tensor_columnas)\n",
        "\n",
        "print(tensor_prediccion)\n",
        "print(tensor_modelo)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MoFgmNsgCuWl",
        "outputId": "c43f405b-e855-457e-da9f-eff93c586952"
      },
      "id": "MoFgmNsgCuWl",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0., 1., 0.])\n",
            "tensor([ 0.1475, -0.2506, -0.0014])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como se ve, la predicción no es correcta, ya que el modelo predice que el sentimiento es angry pero en realidad es happy."
      ],
      "metadata": {
        "id": "CZ9cT8dzkjbF"
      },
      "id": "CZ9cT8dzkjbF"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Variación de hiperparámetros para encontrar el mejor modelo\n",
        "En esta sección, vamos a variar los hiperparámetros y comparar como cambia el modelo al modificar estos atributos."
      ],
      "metadata": {
        "id": "toi2zYpJy2wo"
      },
      "id": "toi2zYpJy2wo"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Primero, el número de epoch para entrenar el modelo"
      ],
      "metadata": {
        "id": "4spdtfM6zWaD"
      },
      "id": "4spdtfM6zWaD"
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = [100, 500, 1000, 1500, 2000, 4000, 6000]\n",
        "loss_test = []\n",
        "for e in epochs:\n",
        "  print('Numero de epochs:', e)\n",
        "  model=Net()\n",
        "  train_model(model,optimizer,criterion,train_data,val_data, e)\n",
        "  print('---------------------------------------------')\n",
        "  test_loss=0.0\n",
        "  for data, target in test_data:\n",
        "    output=model(data)\n",
        "    loss= criterion(output,target)\n",
        "    test_loss += loss.item()*data.size(0)\n",
        "  test_loss = test_loss/len(test_data.dataset)\n",
        "  print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
        "  loss_test.append(format(test_loss))\n",
        "  print('---------------------------------------------')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OmMix1wM0WAc",
        "outputId": "8d18bd86-68fa-49ce-9b28-3d9a73b2b712"
      },
      "id": "OmMix1wM0WAc",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mSe han truncado las últimas 5000 líneas del flujo de salida.\u001b[0m\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4751 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4752 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4753 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4754 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4755 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4756 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4757 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4758 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4759 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4760 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4761 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4762 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4763 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4764 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4765 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4766 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4767 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4768 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4769 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4770 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4771 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4772 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4773 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4774 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4775 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4776 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4777 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4778 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4779 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4780 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4781 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4782 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4783 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4784 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4785 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4786 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4787 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4788 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4789 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4790 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4791 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4792 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4793 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4794 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4795 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4796 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4797 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4798 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4799 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4800 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4801 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4802 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4803 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4804 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4805 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4806 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4807 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4808 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4809 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4810 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4811 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4812 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4813 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4814 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4815 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4816 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4817 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4818 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4819 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4820 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4821 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4822 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4823 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4824 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4825 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4826 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4827 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4828 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4829 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4830 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4831 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4832 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4833 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4834 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4835 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4836 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4837 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4838 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4839 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4840 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4841 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4842 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4843 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4844 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4845 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4846 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4847 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4848 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4849 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4850 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4851 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4852 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4853 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4854 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4855 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4856 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4857 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4858 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4859 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4860 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4861 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4862 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4863 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4864 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4865 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4866 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4867 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4868 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4869 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4870 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4871 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4872 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4873 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4874 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4875 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4876 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4877 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4878 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4879 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4880 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4881 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4882 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4883 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4884 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4885 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4886 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4887 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4888 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4889 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4890 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4891 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4892 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4893 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4894 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4895 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4896 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4897 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4898 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4899 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4900 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4901 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4902 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4903 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4904 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4905 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4906 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4907 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4908 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4909 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4910 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4911 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4912 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4913 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4914 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4915 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4916 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4917 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4918 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4919 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4920 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4921 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4922 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4923 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4924 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4925 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4926 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4927 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4928 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4929 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4930 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4931 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4932 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4933 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4934 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4935 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4936 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4937 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4938 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4939 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4940 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4941 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4942 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4943 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4944 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4945 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4946 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4947 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4948 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4949 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4950 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4951 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4952 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4953 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4954 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4955 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4956 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4957 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4958 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4959 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4960 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4961 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4962 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4963 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4964 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4965 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4966 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4967 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4968 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4969 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4970 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4971 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4972 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4973 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4974 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4975 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4976 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4977 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4978 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4979 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4980 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4981 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4982 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4983 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4984 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4985 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4986 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4987 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4988 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4989 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4990 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4991 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4992 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4993 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4994 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4995 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4996 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4997 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4998 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 4999 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5000 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5001 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5002 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5003 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5004 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5005 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5006 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5007 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5008 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5009 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5010 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5011 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5012 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5013 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5014 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5015 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5016 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5017 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5018 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5019 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5020 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5021 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5022 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5023 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5024 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5025 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5026 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5027 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5028 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5029 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5030 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5031 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5032 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5033 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5034 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5035 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5036 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5037 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5038 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5039 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5040 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5041 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5042 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5043 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5044 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5045 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5046 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5047 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5048 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5049 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5050 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5051 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5052 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5053 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5054 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5055 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5056 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5057 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5058 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5059 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5060 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5061 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5062 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5063 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5064 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5065 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5066 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5067 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5068 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5069 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5070 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5071 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5072 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5073 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5074 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5075 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5076 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5077 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5078 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5079 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5080 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5081 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5082 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5083 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5084 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5085 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5086 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5087 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5088 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5089 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5090 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5091 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5092 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5093 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5094 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5095 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5096 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5097 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5098 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5099 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5100 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5101 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5102 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5103 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5104 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5105 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5106 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5107 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5108 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5109 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5110 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5111 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5112 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5113 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5114 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5115 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5116 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5117 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5118 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5119 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5120 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5121 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5122 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5123 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5124 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5125 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5126 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5127 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5128 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5129 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5130 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5131 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5132 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5133 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5134 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5135 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5136 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5137 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5138 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5139 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5140 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5141 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5142 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5143 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5144 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5145 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5146 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5147 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5148 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5149 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5150 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5151 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5152 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5153 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5154 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5155 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5156 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5157 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5158 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5159 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5160 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5161 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5162 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5163 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5164 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5165 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5166 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5167 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5168 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5169 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5170 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5171 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5172 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5173 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5174 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5175 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5176 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5177 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5178 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5179 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5180 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5181 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5182 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5183 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5184 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5185 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5186 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5187 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5188 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5189 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5190 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5191 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5192 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5193 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5194 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5195 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5196 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5197 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5198 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5199 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5200 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5201 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5202 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5203 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5204 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5205 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5206 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5207 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5208 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5209 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5210 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5211 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5212 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5213 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5214 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5215 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5216 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5217 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5218 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5219 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5220 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5221 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5222 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5223 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5224 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5225 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5226 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5227 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5228 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5229 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5230 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5231 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5232 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5233 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5234 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5235 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5236 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5237 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5238 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5239 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5240 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5241 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5242 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5243 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5244 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5245 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5246 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5247 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5248 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5249 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5250 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5251 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5252 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5253 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5254 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5255 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5256 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5257 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5258 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5259 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5260 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5261 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5262 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5263 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5264 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5265 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5266 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5267 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5268 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5269 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5270 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5271 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5272 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5273 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5274 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5275 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5276 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5277 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5278 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5279 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5280 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5281 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5282 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5283 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5284 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5285 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5286 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5287 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5288 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5289 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5290 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5291 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5292 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5293 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5294 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5295 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5296 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5297 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5298 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5299 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5300 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5301 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5302 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5303 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5304 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5305 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5306 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5307 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5308 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5309 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5310 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5311 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5312 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5313 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5314 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5315 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5316 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5317 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5318 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5319 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5320 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5321 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5322 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5323 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5324 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5325 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5326 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5327 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5328 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5329 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5330 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5331 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5332 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5333 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5334 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5335 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5336 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5337 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5338 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5339 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5340 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5341 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5342 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5343 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5344 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5345 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5346 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5347 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5348 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5349 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5350 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5351 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5352 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5353 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5354 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5355 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5356 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5357 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5358 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5359 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5360 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5361 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5362 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5363 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5364 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5365 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5366 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5367 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5368 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5369 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5370 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5371 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5372 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5373 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5374 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5375 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5376 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5377 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5378 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5379 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5380 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5381 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5382 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5383 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5384 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5385 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5386 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5387 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5388 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5389 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5390 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5391 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5392 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5393 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5394 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5395 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5396 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5397 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5398 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5399 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5400 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5401 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5402 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5403 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5404 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5405 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5406 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5407 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5408 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5409 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5410 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5411 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5412 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5413 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5414 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5415 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5416 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5417 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5418 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5419 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5420 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5421 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5422 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5423 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5424 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5425 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5426 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5427 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5428 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5429 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5430 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5431 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5432 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5433 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5434 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5435 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5436 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5437 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5438 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5439 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5440 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5441 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5442 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5443 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5444 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5445 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5446 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5447 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5448 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5449 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5450 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5451 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5452 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5453 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5454 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5455 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5456 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5457 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5458 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5459 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5460 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5461 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5462 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5463 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5464 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5465 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5466 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5467 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5468 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5469 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5470 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5471 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5472 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5473 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5474 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5475 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5476 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5477 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5478 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5479 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5480 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5481 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5482 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5483 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5484 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5485 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5486 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5487 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5488 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5489 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5490 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5491 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5492 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5493 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5494 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5495 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5496 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5497 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5498 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5499 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5500 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5501 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5502 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5503 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5504 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5505 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5506 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5507 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5508 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5509 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5510 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5511 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5512 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5513 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5514 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5515 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5516 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5517 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5518 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5519 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5520 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5521 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5522 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5523 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5524 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5525 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5526 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5527 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5528 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5529 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5530 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5531 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5532 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5533 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5534 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5535 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5536 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5537 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5538 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5539 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5540 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5541 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5542 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5543 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5544 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5545 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5546 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5547 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5548 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5549 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5550 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5551 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5552 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5553 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5554 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5555 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5556 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5557 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5558 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5559 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5560 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5561 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5562 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5563 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5564 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5565 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5566 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5567 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5568 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5569 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5570 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5571 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5572 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5573 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5574 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5575 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5576 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5577 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5578 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5579 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5580 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5581 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5582 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5583 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5584 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5585 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5586 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5587 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5588 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5589 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5590 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5591 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5592 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5593 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5594 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5595 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5596 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5597 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5598 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5599 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5600 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5601 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5602 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5603 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5604 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5605 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5606 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5607 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5608 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5609 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5610 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5611 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5612 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5613 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5614 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5615 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5616 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5617 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5618 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5619 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5620 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5621 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5622 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5623 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5624 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5625 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5626 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5627 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5628 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5629 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5630 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5631 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5632 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5633 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5634 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5635 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5636 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5637 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5638 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5639 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5640 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5641 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5642 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5643 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5644 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5645 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5646 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5647 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5648 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5649 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5650 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5651 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5652 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5653 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5654 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5655 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5656 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5657 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5658 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5659 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5660 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5661 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5662 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5663 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5664 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5665 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5666 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5667 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5668 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5669 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5670 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5671 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5672 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5673 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5674 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5675 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5676 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5677 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5678 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5679 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5680 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5681 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5682 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5683 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5684 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5685 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5686 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5687 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5688 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5689 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5690 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5691 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5692 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5693 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5694 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5695 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5696 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5697 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5698 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5699 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5700 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5701 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5702 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5703 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5704 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5705 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5706 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5707 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5708 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5709 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5710 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5711 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5712 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5713 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5714 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5715 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5716 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5717 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5718 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5719 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5720 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5721 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5722 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5723 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5724 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5725 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5726 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5727 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5728 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5729 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5730 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5731 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5732 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5733 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5734 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5735 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5736 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5737 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5738 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5739 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5740 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5741 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5742 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5743 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5744 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5745 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5746 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5747 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5748 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5749 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5750 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5751 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5752 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5753 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5754 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5755 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5756 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5757 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5758 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5759 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5760 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5761 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5762 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5763 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5764 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5765 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5766 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5767 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5768 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5769 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5770 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5771 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5772 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5773 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5774 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5775 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5776 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5777 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5778 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5779 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5780 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5781 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5782 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5783 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5784 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5785 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5786 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5787 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5788 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5789 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5790 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5791 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5792 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5793 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5794 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5795 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5796 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5797 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5798 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5799 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5800 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5801 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5802 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5803 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5804 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5805 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5806 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5807 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5808 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5809 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5810 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5811 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5812 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5813 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5814 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5815 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5816 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5817 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5818 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5819 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5820 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5821 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5822 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5823 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5824 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5825 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5826 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5827 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5828 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5829 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5830 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5831 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5832 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5833 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5834 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5835 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5836 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5837 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5838 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5839 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5840 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5841 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5842 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5843 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5844 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5845 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5846 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5847 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5848 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5849 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5850 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5851 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5852 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5853 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5854 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5855 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5856 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5857 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5858 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5859 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5860 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5861 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5862 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5863 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5864 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5865 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5866 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5867 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5868 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5869 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5870 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5871 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5872 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5873 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5874 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5875 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5876 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5877 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5878 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5879 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5880 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5881 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5882 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5883 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5884 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5885 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5886 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5887 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5888 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5889 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5890 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5891 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5892 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5893 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5894 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5895 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5896 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5897 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5898 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5899 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5900 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5901 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5902 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5903 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5904 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5905 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5906 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5907 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5908 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5909 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5910 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5911 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5912 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5913 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5914 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5915 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5916 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5917 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5918 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5919 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5920 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5921 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5922 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5923 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5924 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5925 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5926 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5927 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5928 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5929 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5930 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5931 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5932 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5933 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5934 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5935 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5936 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5937 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5938 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5939 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5940 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5941 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5942 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5943 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5944 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5945 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5946 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5947 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5948 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5949 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5950 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5951 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5952 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5953 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5954 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5955 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5956 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5957 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5958 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5959 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5960 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5961 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5962 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5963 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5964 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5965 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5966 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5967 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5968 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5969 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5970 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5971 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5972 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5973 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5974 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5975 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5976 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5977 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5978 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5979 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5980 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5981 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5982 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5983 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5984 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5985 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5986 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5987 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5988 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5989 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5990 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5991 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5992 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5993 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5994 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5995 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5996 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5997 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5998 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "train_loss:\n",
            "1.105289571232848\n",
            "Epoch: 5999 \tTraining Loss: 1.105290 \tValidation Loss: 0.020594\n",
            "Validation loss decreased (0.020594 --> 0.020594).  Saving model ...\n",
            "---------------------------------------------\n",
            "Test Loss: 1.113980\n",
            "\n",
            "---------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se redondea el loss_test para graficar"
      ],
      "metadata": {
        "id": "96waQXc61kBx"
      },
      "id": "96waQXc61kBx"
    },
    {
      "cell_type": "code",
      "source": [
        "lt = []\n",
        "for i in loss_test:\n",
        "  redon= round(float(i),3)\n",
        "  lt.append(redon)\n",
        "plt.plot(epochs,lt)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "sZvf_KgYN_Dv",
        "outputId": "fb3796e3-20ee-4c7b-f7c1-0d216cd82ae5"
      },
      "id": "sZvf_KgYN_Dv",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora, se varia el learning rate del optimizador para ver como cambia respecto al test loss"
      ],
      "metadata": {
        "id": "ILNrDu02BaqR"
      },
      "id": "ILNrDu02BaqR"
    },
    {
      "cell_type": "code",
      "source": [
        "lr = [0.001, 0.01, 0.1, 1, 5]\n",
        "loss_test = []\n",
        "for e in lr:\n",
        "  print('Learning rate:', e)\n",
        "  model=Net()\n",
        "  optimizer=torch.optim.SGD(model.parameters(), lr=e)\n",
        "  train_model(model,optimizer,criterion,train_data,val_data, 1000)\n",
        "  print('---------------------------------------------')\n",
        "  test_loss=0.0\n",
        "  for data, target in test_data:\n",
        "    output=model(data)\n",
        "    loss= criterion(output,target)\n",
        "    test_loss += loss.item()*data.size(0)\n",
        "  test_loss = test_loss/len(test_data.dataset)\n",
        "  print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
        "  loss_test.append(format(test_loss))\n",
        "  print('---------------------------------------------')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YN0ernhY137r",
        "outputId": "00c71dbf-8df2-424f-e134-b1853ebe99a4"
      },
      "id": "YN0ernhY137r",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mSe han truncado las últimas 5000 líneas del flujo de salida.\u001b[0m\n",
            "1.1570899176510263\n",
            "Epoch: 394 \tTraining Loss: 1.157090 \tValidation Loss: 0.027550\n",
            "Validation loss decreased (0.027553 --> 0.027550).  Saving model ...\n",
            "train_loss:\n",
            "1.1570602166347015\n",
            "Epoch: 395 \tTraining Loss: 1.157060 \tValidation Loss: 0.027547\n",
            "Validation loss decreased (0.027550 --> 0.027547).  Saving model ...\n",
            "train_loss:\n",
            "1.1570305867946191\n",
            "Epoch: 396 \tTraining Loss: 1.157031 \tValidation Loss: 0.027544\n",
            "Validation loss decreased (0.027547 --> 0.027544).  Saving model ...\n",
            "train_loss:\n",
            "1.1570009600111852\n",
            "Epoch: 397 \tTraining Loss: 1.157001 \tValidation Loss: 0.027541\n",
            "Validation loss decreased (0.027544 --> 0.027541).  Saving model ...\n",
            "train_loss:\n",
            "1.1569714528737052\n",
            "Epoch: 398 \tTraining Loss: 1.156971 \tValidation Loss: 0.027539\n",
            "Validation loss decreased (0.027541 --> 0.027539).  Saving model ...\n",
            "train_loss:\n",
            "1.1569420304490534\n",
            "Epoch: 399 \tTraining Loss: 1.156942 \tValidation Loss: 0.027536\n",
            "Validation loss decreased (0.027539 --> 0.027536).  Saving model ...\n",
            "train_loss:\n",
            "1.1569125866278624\n",
            "Epoch: 400 \tTraining Loss: 1.156913 \tValidation Loss: 0.027533\n",
            "Validation loss decreased (0.027536 --> 0.027533).  Saving model ...\n",
            "train_loss:\n",
            "1.1568832781725313\n",
            "Epoch: 401 \tTraining Loss: 1.156883 \tValidation Loss: 0.027530\n",
            "Validation loss decreased (0.027533 --> 0.027530).  Saving model ...\n",
            "train_loss:\n",
            "1.1568539579272707\n",
            "Epoch: 402 \tTraining Loss: 1.156854 \tValidation Loss: 0.027527\n",
            "Validation loss decreased (0.027530 --> 0.027527).  Saving model ...\n",
            "train_loss:\n",
            "1.1568247411713932\n",
            "Epoch: 403 \tTraining Loss: 1.156825 \tValidation Loss: 0.027525\n",
            "Validation loss decreased (0.027527 --> 0.027525).  Saving model ...\n",
            "train_loss:\n",
            "1.1567955654619377\n",
            "Epoch: 404 \tTraining Loss: 1.156796 \tValidation Loss: 0.027522\n",
            "Validation loss decreased (0.027525 --> 0.027522).  Saving model ...\n",
            "train_loss:\n",
            "1.1567664395321857\n",
            "Epoch: 405 \tTraining Loss: 1.156766 \tValidation Loss: 0.027519\n",
            "Validation loss decreased (0.027522 --> 0.027519).  Saving model ...\n",
            "train_loss:\n",
            "1.1567373904553089\n",
            "Epoch: 406 \tTraining Loss: 1.156737 \tValidation Loss: 0.027516\n",
            "Validation loss decreased (0.027519 --> 0.027516).  Saving model ...\n",
            "train_loss:\n",
            "1.1567083942147838\n",
            "Epoch: 407 \tTraining Loss: 1.156708 \tValidation Loss: 0.027514\n",
            "Validation loss decreased (0.027516 --> 0.027514).  Saving model ...\n",
            "train_loss:\n",
            "1.1566794831237512\n",
            "Epoch: 408 \tTraining Loss: 1.156679 \tValidation Loss: 0.027511\n",
            "Validation loss decreased (0.027514 --> 0.027511).  Saving model ...\n",
            "train_loss:\n",
            "1.1566505768360236\n",
            "Epoch: 409 \tTraining Loss: 1.156651 \tValidation Loss: 0.027508\n",
            "Validation loss decreased (0.027511 --> 0.027508).  Saving model ...\n",
            "train_loss:\n",
            "1.1566217102847256\n",
            "Epoch: 410 \tTraining Loss: 1.156622 \tValidation Loss: 0.027506\n",
            "Validation loss decreased (0.027508 --> 0.027506).  Saving model ...\n",
            "train_loss:\n",
            "1.1565929371795374\n",
            "Epoch: 411 \tTraining Loss: 1.156593 \tValidation Loss: 0.027503\n",
            "Validation loss decreased (0.027506 --> 0.027503).  Saving model ...\n",
            "train_loss:\n",
            "1.1565642422372169\n",
            "Epoch: 412 \tTraining Loss: 1.156564 \tValidation Loss: 0.027500\n",
            "Validation loss decreased (0.027503 --> 0.027500).  Saving model ...\n",
            "train_loss:\n",
            "1.156535566508115\n",
            "Epoch: 413 \tTraining Loss: 1.156536 \tValidation Loss: 0.027498\n",
            "Validation loss decreased (0.027500 --> 0.027498).  Saving model ...\n",
            "train_loss:\n",
            "1.1565069798584824\n",
            "Epoch: 414 \tTraining Loss: 1.156507 \tValidation Loss: 0.027495\n",
            "Validation loss decreased (0.027498 --> 0.027495).  Saving model ...\n",
            "train_loss:\n",
            "1.1564784067454354\n",
            "Epoch: 415 \tTraining Loss: 1.156478 \tValidation Loss: 0.027493\n",
            "Validation loss decreased (0.027495 --> 0.027493).  Saving model ...\n",
            "train_loss:\n",
            "1.1564498921453734\n",
            "Epoch: 416 \tTraining Loss: 1.156450 \tValidation Loss: 0.027490\n",
            "Validation loss decreased (0.027493 --> 0.027490).  Saving model ...\n",
            "train_loss:\n",
            "1.1564214185917334\n",
            "Epoch: 417 \tTraining Loss: 1.156421 \tValidation Loss: 0.027487\n",
            "Validation loss decreased (0.027490 --> 0.027487).  Saving model ...\n",
            "train_loss:\n",
            "1.1563930153410076\n",
            "Epoch: 418 \tTraining Loss: 1.156393 \tValidation Loss: 0.027485\n",
            "Validation loss decreased (0.027487 --> 0.027485).  Saving model ...\n",
            "train_loss:\n",
            "1.1563646609966571\n",
            "Epoch: 419 \tTraining Loss: 1.156365 \tValidation Loss: 0.027482\n",
            "Validation loss decreased (0.027485 --> 0.027482).  Saving model ...\n",
            "train_loss:\n",
            "1.1563363485720568\n",
            "Epoch: 420 \tTraining Loss: 1.156336 \tValidation Loss: 0.027480\n",
            "Validation loss decreased (0.027482 --> 0.027480).  Saving model ...\n",
            "train_loss:\n",
            "1.1563081121270036\n",
            "Epoch: 421 \tTraining Loss: 1.156308 \tValidation Loss: 0.027478\n",
            "Validation loss decreased (0.027480 --> 0.027478).  Saving model ...\n",
            "train_loss:\n",
            "1.15627991323506\n",
            "Epoch: 422 \tTraining Loss: 1.156280 \tValidation Loss: 0.027475\n",
            "Validation loss decreased (0.027478 --> 0.027475).  Saving model ...\n",
            "train_loss:\n",
            "1.1562517575728588\n",
            "Epoch: 423 \tTraining Loss: 1.156252 \tValidation Loss: 0.027473\n",
            "Validation loss decreased (0.027475 --> 0.027473).  Saving model ...\n",
            "train_loss:\n",
            "1.1562236442670717\n",
            "Epoch: 424 \tTraining Loss: 1.156224 \tValidation Loss: 0.027470\n",
            "Validation loss decreased (0.027473 --> 0.027470).  Saving model ...\n",
            "train_loss:\n",
            "1.156195567204402\n",
            "Epoch: 425 \tTraining Loss: 1.156196 \tValidation Loss: 0.027468\n",
            "Validation loss decreased (0.027470 --> 0.027468).  Saving model ...\n",
            "train_loss:\n",
            "1.1561675945044436\n",
            "Epoch: 426 \tTraining Loss: 1.156168 \tValidation Loss: 0.027466\n",
            "Validation loss decreased (0.027468 --> 0.027466).  Saving model ...\n",
            "train_loss:\n",
            "1.1561395820680556\n",
            "Epoch: 427 \tTraining Loss: 1.156140 \tValidation Loss: 0.027463\n",
            "Validation loss decreased (0.027466 --> 0.027463).  Saving model ...\n",
            "train_loss:\n",
            "1.156111641681238\n",
            "Epoch: 428 \tTraining Loss: 1.156112 \tValidation Loss: 0.027461\n",
            "Validation loss decreased (0.027463 --> 0.027461).  Saving model ...\n",
            "train_loss:\n",
            "1.1560837637373815\n",
            "Epoch: 429 \tTraining Loss: 1.156084 \tValidation Loss: 0.027459\n",
            "Validation loss decreased (0.027461 --> 0.027459).  Saving model ...\n",
            "train_loss:\n",
            "1.156055945179838\n",
            "Epoch: 430 \tTraining Loss: 1.156056 \tValidation Loss: 0.027456\n",
            "Validation loss decreased (0.027459 --> 0.027456).  Saving model ...\n",
            "train_loss:\n",
            "1.1560281231289817\n",
            "Epoch: 431 \tTraining Loss: 1.156028 \tValidation Loss: 0.027454\n",
            "Validation loss decreased (0.027456 --> 0.027454).  Saving model ...\n",
            "train_loss:\n",
            "1.1560003761843447\n",
            "Epoch: 432 \tTraining Loss: 1.156000 \tValidation Loss: 0.027452\n",
            "Validation loss decreased (0.027454 --> 0.027452).  Saving model ...\n",
            "train_loss:\n",
            "1.1559726763994267\n",
            "Epoch: 433 \tTraining Loss: 1.155973 \tValidation Loss: 0.027450\n",
            "Validation loss decreased (0.027452 --> 0.027450).  Saving model ...\n",
            "train_loss:\n",
            "1.1559450311975166\n",
            "Epoch: 434 \tTraining Loss: 1.155945 \tValidation Loss: 0.027447\n",
            "Validation loss decreased (0.027450 --> 0.027447).  Saving model ...\n",
            "train_loss:\n",
            "1.1559173471325046\n",
            "Epoch: 435 \tTraining Loss: 1.155917 \tValidation Loss: 0.027445\n",
            "Validation loss decreased (0.027447 --> 0.027445).  Saving model ...\n",
            "train_loss:\n",
            "1.1558897901367355\n",
            "Epoch: 436 \tTraining Loss: 1.155890 \tValidation Loss: 0.027443\n",
            "Validation loss decreased (0.027445 --> 0.027443).  Saving model ...\n",
            "train_loss:\n",
            "1.155862237507607\n",
            "Epoch: 437 \tTraining Loss: 1.155862 \tValidation Loss: 0.027441\n",
            "Validation loss decreased (0.027443 --> 0.027441).  Saving model ...\n",
            "train_loss:\n",
            "1.1558347136983067\n",
            "Epoch: 438 \tTraining Loss: 1.155835 \tValidation Loss: 0.027439\n",
            "Validation loss decreased (0.027441 --> 0.027439).  Saving model ...\n",
            "train_loss:\n",
            "1.1558072222021474\n",
            "Epoch: 439 \tTraining Loss: 1.155807 \tValidation Loss: 0.027437\n",
            "Validation loss decreased (0.027439 --> 0.027437).  Saving model ...\n",
            "train_loss:\n",
            "1.1557797874723161\n",
            "Epoch: 440 \tTraining Loss: 1.155780 \tValidation Loss: 0.027435\n",
            "Validation loss decreased (0.027437 --> 0.027435).  Saving model ...\n",
            "train_loss:\n",
            "1.1557524086354853\n",
            "Epoch: 441 \tTraining Loss: 1.155752 \tValidation Loss: 0.027433\n",
            "Validation loss decreased (0.027435 --> 0.027433).  Saving model ...\n",
            "train_loss:\n",
            "1.155725007092123\n",
            "Epoch: 442 \tTraining Loss: 1.155725 \tValidation Loss: 0.027431\n",
            "Validation loss decreased (0.027433 --> 0.027431).  Saving model ...\n",
            "train_loss:\n",
            "1.1556976771616674\n",
            "Epoch: 443 \tTraining Loss: 1.155698 \tValidation Loss: 0.027429\n",
            "Validation loss decreased (0.027431 --> 0.027429).  Saving model ...\n",
            "train_loss:\n",
            "1.1556703651344382\n",
            "Epoch: 444 \tTraining Loss: 1.155670 \tValidation Loss: 0.027427\n",
            "Validation loss decreased (0.027429 --> 0.027427).  Saving model ...\n",
            "train_loss:\n",
            "1.1556430932803032\n",
            "Epoch: 445 \tTraining Loss: 1.155643 \tValidation Loss: 0.027425\n",
            "Validation loss decreased (0.027427 --> 0.027425).  Saving model ...\n",
            "train_loss:\n",
            "1.1556158581059495\n",
            "Epoch: 446 \tTraining Loss: 1.155616 \tValidation Loss: 0.027423\n",
            "Validation loss decreased (0.027425 --> 0.027423).  Saving model ...\n",
            "train_loss:\n",
            "1.1555886434548066\n",
            "Epoch: 447 \tTraining Loss: 1.155589 \tValidation Loss: 0.027421\n",
            "Validation loss decreased (0.027423 --> 0.027421).  Saving model ...\n",
            "train_loss:\n",
            "1.1555614711600781\n",
            "Epoch: 448 \tTraining Loss: 1.155561 \tValidation Loss: 0.027419\n",
            "Validation loss decreased (0.027421 --> 0.027419).  Saving model ...\n",
            "train_loss:\n",
            "1.1555343359817951\n",
            "Epoch: 449 \tTraining Loss: 1.155534 \tValidation Loss: 0.027417\n",
            "Validation loss decreased (0.027419 --> 0.027417).  Saving model ...\n",
            "train_loss:\n",
            "1.1555071837736137\n",
            "Epoch: 450 \tTraining Loss: 1.155507 \tValidation Loss: 0.027415\n",
            "Validation loss decreased (0.027417 --> 0.027415).  Saving model ...\n",
            "train_loss:\n",
            "1.155480099248362\n",
            "Epoch: 451 \tTraining Loss: 1.155480 \tValidation Loss: 0.027414\n",
            "Validation loss decreased (0.027415 --> 0.027414).  Saving model ...\n",
            "train_loss:\n",
            "1.1554530208364076\n",
            "Epoch: 452 \tTraining Loss: 1.155453 \tValidation Loss: 0.027412\n",
            "Validation loss decreased (0.027414 --> 0.027412).  Saving model ...\n",
            "train_loss:\n",
            "1.1554259795408983\n",
            "Epoch: 453 \tTraining Loss: 1.155426 \tValidation Loss: 0.027410\n",
            "Validation loss decreased (0.027412 --> 0.027410).  Saving model ...\n",
            "train_loss:\n",
            "1.1553989552752875\n",
            "Epoch: 454 \tTraining Loss: 1.155399 \tValidation Loss: 0.027408\n",
            "Validation loss decreased (0.027410 --> 0.027408).  Saving model ...\n",
            "train_loss:\n",
            "1.1553719550262005\n",
            "Epoch: 455 \tTraining Loss: 1.155372 \tValidation Loss: 0.027407\n",
            "Validation loss decreased (0.027408 --> 0.027407).  Saving model ...\n",
            "train_loss:\n",
            "1.1553449766103165\n",
            "Epoch: 456 \tTraining Loss: 1.155345 \tValidation Loss: 0.027405\n",
            "Validation loss decreased (0.027407 --> 0.027405).  Saving model ...\n",
            "train_loss:\n",
            "1.1553180029977372\n",
            "Epoch: 457 \tTraining Loss: 1.155318 \tValidation Loss: 0.027403\n",
            "Validation loss decreased (0.027405 --> 0.027403).  Saving model ...\n",
            "train_loss:\n",
            "1.15529108440483\n",
            "Epoch: 458 \tTraining Loss: 1.155291 \tValidation Loss: 0.027402\n",
            "Validation loss decreased (0.027403 --> 0.027402).  Saving model ...\n",
            "train_loss:\n",
            "1.1552641706152276\n",
            "Epoch: 459 \tTraining Loss: 1.155264 \tValidation Loss: 0.027400\n",
            "Validation loss decreased (0.027402 --> 0.027400).  Saving model ...\n",
            "train_loss:\n",
            "1.155237235429086\n",
            "Epoch: 460 \tTraining Loss: 1.155237 \tValidation Loss: 0.027399\n",
            "Validation loss decreased (0.027400 --> 0.027399).  Saving model ...\n",
            "train_loss:\n",
            "1.1552103631225699\n",
            "Epoch: 461 \tTraining Loss: 1.155210 \tValidation Loss: 0.027397\n",
            "Validation loss decreased (0.027399 --> 0.027397).  Saving model ...\n",
            "train_loss:\n",
            "1.1551834646162096\n",
            "Epoch: 462 \tTraining Loss: 1.155183 \tValidation Loss: 0.027396\n",
            "Validation loss decreased (0.027397 --> 0.027396).  Saving model ...\n",
            "train_loss:\n",
            "1.1551566337927794\n",
            "Epoch: 463 \tTraining Loss: 1.155157 \tValidation Loss: 0.027394\n",
            "Validation loss decreased (0.027396 --> 0.027394).  Saving model ...\n",
            "train_loss:\n",
            "1.1551297732761927\n",
            "Epoch: 464 \tTraining Loss: 1.155130 \tValidation Loss: 0.027393\n",
            "Validation loss decreased (0.027394 --> 0.027393).  Saving model ...\n",
            "train_loss:\n",
            "1.1551029424527626\n",
            "Epoch: 465 \tTraining Loss: 1.155103 \tValidation Loss: 0.027391\n",
            "Validation loss decreased (0.027393 --> 0.027391).  Saving model ...\n",
            "train_loss:\n",
            "1.1550761234192621\n",
            "Epoch: 466 \tTraining Loss: 1.155076 \tValidation Loss: 0.027390\n",
            "Validation loss decreased (0.027391 --> 0.027390).  Saving model ...\n",
            "train_loss:\n",
            "1.155049327528957\n",
            "Epoch: 467 \tTraining Loss: 1.155049 \tValidation Loss: 0.027388\n",
            "Validation loss decreased (0.027390 --> 0.027388).  Saving model ...\n",
            "train_loss:\n",
            "1.1550224879722455\n",
            "Epoch: 468 \tTraining Loss: 1.155022 \tValidation Loss: 0.027387\n",
            "Validation loss decreased (0.027388 --> 0.027387).  Saving model ...\n",
            "train_loss:\n",
            "1.1549957182817843\n",
            "Epoch: 469 \tTraining Loss: 1.154996 \tValidation Loss: 0.027386\n",
            "Validation loss decreased (0.027387 --> 0.027386).  Saving model ...\n",
            "train_loss:\n",
            "1.1549689289414402\n",
            "Epoch: 470 \tTraining Loss: 1.154969 \tValidation Loss: 0.027384\n",
            "Validation loss decreased (0.027386 --> 0.027384).  Saving model ...\n",
            "train_loss:\n",
            "1.1549421343611272\n",
            "Epoch: 471 \tTraining Loss: 1.154942 \tValidation Loss: 0.027383\n",
            "Validation loss decreased (0.027384 --> 0.027383).  Saving model ...\n",
            "train_loss:\n",
            "1.1549153384708224\n",
            "Epoch: 472 \tTraining Loss: 1.154915 \tValidation Loss: 0.027382\n",
            "Validation loss decreased (0.027383 --> 0.027382).  Saving model ...\n",
            "train_loss:\n",
            "1.1548885722736737\n",
            "Epoch: 473 \tTraining Loss: 1.154889 \tValidation Loss: 0.027381\n",
            "Validation loss decreased (0.027382 --> 0.027381).  Saving model ...\n",
            "train_loss:\n",
            "1.1548617855533139\n",
            "Epoch: 474 \tTraining Loss: 1.154862 \tValidation Loss: 0.027379\n",
            "Validation loss decreased (0.027381 --> 0.027379).  Saving model ...\n",
            "train_loss:\n",
            "1.1548349818030557\n",
            "Epoch: 475 \tTraining Loss: 1.154835 \tValidation Loss: 0.027378\n",
            "Validation loss decreased (0.027379 --> 0.027378).  Saving model ...\n",
            "train_loss:\n",
            "1.154808208619282\n",
            "Epoch: 476 \tTraining Loss: 1.154808 \tValidation Loss: 0.027377\n",
            "Validation loss decreased (0.027378 --> 0.027377).  Saving model ...\n",
            "train_loss:\n",
            "1.154781400502383\n",
            "Epoch: 477 \tTraining Loss: 1.154781 \tValidation Loss: 0.027376\n",
            "Validation loss decreased (0.027377 --> 0.027376).  Saving model ...\n",
            "train_loss:\n",
            "1.1547546142186873\n",
            "Epoch: 478 \tTraining Loss: 1.154755 \tValidation Loss: 0.027375\n",
            "Validation loss decreased (0.027376 --> 0.027375).  Saving model ...\n",
            "train_loss:\n",
            "1.154727810031765\n",
            "Epoch: 479 \tTraining Loss: 1.154728 \tValidation Loss: 0.027374\n",
            "Validation loss decreased (0.027375 --> 0.027374).  Saving model ...\n",
            "train_loss:\n",
            "1.154700985758296\n",
            "Epoch: 480 \tTraining Loss: 1.154701 \tValidation Loss: 0.027373\n",
            "Validation loss decreased (0.027374 --> 0.027373).  Saving model ...\n",
            "train_loss:\n",
            "1.154674150131561\n",
            "Epoch: 481 \tTraining Loss: 1.154674 \tValidation Loss: 0.027372\n",
            "Validation loss decreased (0.027373 --> 0.027372).  Saving model ...\n",
            "train_loss:\n",
            "1.1546473236747714\n",
            "Epoch: 482 \tTraining Loss: 1.154647 \tValidation Loss: 0.027371\n",
            "Validation loss decreased (0.027372 --> 0.027371).  Saving model ...\n",
            "train_loss:\n",
            "1.1546204775680988\n",
            "Epoch: 483 \tTraining Loss: 1.154620 \tValidation Loss: 0.027370\n",
            "Validation loss decreased (0.027371 --> 0.027370).  Saving model ...\n",
            "train_loss:\n",
            "1.1545936166148483\n",
            "Epoch: 484 \tTraining Loss: 1.154594 \tValidation Loss: 0.027369\n",
            "Validation loss decreased (0.027370 --> 0.027369).  Saving model ...\n",
            "train_loss:\n",
            "1.1545667023885817\n",
            "Epoch: 485 \tTraining Loss: 1.154567 \tValidation Loss: 0.027369\n",
            "Validation loss decreased (0.027369 --> 0.027369).  Saving model ...\n",
            "train_loss:\n",
            "1.154539813488831\n",
            "Epoch: 486 \tTraining Loss: 1.154540 \tValidation Loss: 0.027368\n",
            "Validation loss decreased (0.027369 --> 0.027368).  Saving model ...\n",
            "train_loss:\n",
            "1.154512868259416\n",
            "Epoch: 487 \tTraining Loss: 1.154513 \tValidation Loss: 0.027367\n",
            "Validation loss decreased (0.027368 --> 0.027367).  Saving model ...\n",
            "train_loss:\n",
            "1.1544859483565166\n",
            "Epoch: 488 \tTraining Loss: 1.154486 \tValidation Loss: 0.027366\n",
            "Validation loss decreased (0.027367 --> 0.027366).  Saving model ...\n",
            "train_loss:\n",
            "1.154458924964234\n",
            "Epoch: 489 \tTraining Loss: 1.154459 \tValidation Loss: 0.027365\n",
            "Validation loss decreased (0.027366 --> 0.027365).  Saving model ...\n",
            "train_loss:\n",
            "1.1544319417450455\n",
            "Epoch: 490 \tTraining Loss: 1.154432 \tValidation Loss: 0.027365\n",
            "Validation loss decreased (0.027365 --> 0.027365).  Saving model ...\n",
            "train_loss:\n",
            "1.154404877743005\n",
            "Epoch: 491 \tTraining Loss: 1.154405 \tValidation Loss: 0.027364\n",
            "Validation loss decreased (0.027365 --> 0.027364).  Saving model ...\n",
            "train_loss:\n",
            "1.154377845617441\n",
            "Epoch: 492 \tTraining Loss: 1.154378 \tValidation Loss: 0.027364\n",
            "Validation loss decreased (0.027364 --> 0.027364).  Saving model ...\n",
            "train_loss:\n",
            "1.154350765022166\n",
            "Epoch: 493 \tTraining Loss: 1.154351 \tValidation Loss: 0.027363\n",
            "Validation loss decreased (0.027364 --> 0.027363).  Saving model ...\n",
            "train_loss:\n",
            "1.154323623730586\n",
            "Epoch: 494 \tTraining Loss: 1.154324 \tValidation Loss: 0.027362\n",
            "Validation loss decreased (0.027363 --> 0.027362).  Saving model ...\n",
            "train_loss:\n",
            "1.154296426109342\n",
            "Epoch: 495 \tTraining Loss: 1.154296 \tValidation Loss: 0.027362\n",
            "Validation loss decreased (0.027362 --> 0.027362).  Saving model ...\n",
            "train_loss:\n",
            "1.1542692538146133\n",
            "Epoch: 496 \tTraining Loss: 1.154269 \tValidation Loss: 0.027361\n",
            "Validation loss decreased (0.027362 --> 0.027361).  Saving model ...\n",
            "train_loss:\n",
            "1.1542420177669315\n",
            "Epoch: 497 \tTraining Loss: 1.154242 \tValidation Loss: 0.027361\n",
            "Validation loss decreased (0.027361 --> 0.027361).  Saving model ...\n",
            "train_loss:\n",
            "1.1542147349961949\n",
            "Epoch: 498 \tTraining Loss: 1.154215 \tValidation Loss: 0.027361\n",
            "Validation loss decreased (0.027361 --> 0.027361).  Saving model ...\n",
            "train_loss:\n",
            "1.1541873941491374\n",
            "Epoch: 499 \tTraining Loss: 1.154187 \tValidation Loss: 0.027360\n",
            "Validation loss decreased (0.027361 --> 0.027360).  Saving model ...\n",
            "train_loss:\n",
            "1.1541599869291423\n",
            "Epoch: 500 \tTraining Loss: 1.154160 \tValidation Loss: 0.027360\n",
            "Validation loss decreased (0.027360 --> 0.027360).  Saving model ...\n",
            "train_loss:\n",
            "1.15413256922921\n",
            "Epoch: 501 \tTraining Loss: 1.154133 \tValidation Loss: 0.027360\n",
            "Validation loss decreased (0.027360 --> 0.027360).  Saving model ...\n",
            "train_loss:\n",
            "1.154105081226363\n",
            "Epoch: 502 \tTraining Loss: 1.154105 \tValidation Loss: 0.027359\n",
            "Validation loss decreased (0.027360 --> 0.027359).  Saving model ...\n",
            "train_loss:\n",
            "1.15407752597725\n",
            "Epoch: 503 \tTraining Loss: 1.154078 \tValidation Loss: 0.027359\n",
            "Validation loss decreased (0.027359 --> 0.027359).  Saving model ...\n",
            "train_loss:\n",
            "1.1540499026085431\n",
            "Epoch: 504 \tTraining Loss: 1.154050 \tValidation Loss: 0.027359\n",
            "Validation loss decreased (0.027359 --> 0.027359).  Saving model ...\n",
            "train_loss:\n",
            "1.1540222469266954\n",
            "Epoch: 505 \tTraining Loss: 1.154022 \tValidation Loss: 0.027359\n",
            "Validation loss decreased (0.027359 --> 0.027359).  Saving model ...\n",
            "train_loss:\n",
            "1.1539945283652224\n",
            "Epoch: 506 \tTraining Loss: 1.153995 \tValidation Loss: 0.027359\n",
            "Validation loss decreased (0.027359 --> 0.027359).  Saving model ...\n",
            "train_loss:\n",
            "1.153966684044499\n",
            "Epoch: 507 \tTraining Loss: 1.153967 \tValidation Loss: 0.027359\n",
            "Validation loss decreased (0.027359 --> 0.027359).  Saving model ...\n",
            "train_loss:\n",
            "1.1539388043539864\n",
            "Epoch: 508 \tTraining Loss: 1.153939 \tValidation Loss: 0.027358\n",
            "Validation loss decreased (0.027359 --> 0.027358).  Saving model ...\n",
            "train_loss:\n",
            "1.1539108504305828\n",
            "Epoch: 509 \tTraining Loss: 1.153911 \tValidation Loss: 0.027358\n",
            "Validation loss decreased (0.027358 --> 0.027358).  Saving model ...\n",
            "train_loss:\n",
            "1.153882813541007\n",
            "Epoch: 510 \tTraining Loss: 1.153883 \tValidation Loss: 0.027359\n",
            "train_loss:\n",
            "1.1538546945585872\n",
            "Epoch: 511 \tTraining Loss: 1.153855 \tValidation Loss: 0.027359\n",
            "train_loss:\n",
            "1.153826505273253\n",
            "Epoch: 512 \tTraining Loss: 1.153827 \tValidation Loss: 0.027359\n",
            "train_loss:\n",
            "1.1537982386983794\n",
            "Epoch: 513 \tTraining Loss: 1.153798 \tValidation Loss: 0.027359\n",
            "train_loss:\n",
            "1.1537698607741695\n",
            "Epoch: 514 \tTraining Loss: 1.153770 \tValidation Loss: 0.027359\n",
            "train_loss:\n",
            "1.1537413850372091\n",
            "Epoch: 515 \tTraining Loss: 1.153741 \tValidation Loss: 0.027359\n",
            "train_loss:\n",
            "1.1537128053742014\n",
            "Epoch: 516 \tTraining Loss: 1.153713 \tValidation Loss: 0.027360\n",
            "train_loss:\n",
            "1.1536841012619354\n",
            "Epoch: 517 \tTraining Loss: 1.153684 \tValidation Loss: 0.027360\n",
            "train_loss:\n",
            "1.1536553307767317\n",
            "Epoch: 518 \tTraining Loss: 1.153655 \tValidation Loss: 0.027360\n",
            "train_loss:\n",
            "1.1536264546188242\n",
            "Epoch: 519 \tTraining Loss: 1.153626 \tValidation Loss: 0.027361\n",
            "train_loss:\n",
            "1.1535974339251116\n",
            "Epoch: 520 \tTraining Loss: 1.153597 \tValidation Loss: 0.027361\n",
            "train_loss:\n",
            "1.1535682778655392\n",
            "Epoch: 521 \tTraining Loss: 1.153568 \tValidation Loss: 0.027362\n",
            "train_loss:\n",
            "1.153539044079763\n",
            "Epoch: 522 \tTraining Loss: 1.153539 \tValidation Loss: 0.027362\n",
            "train_loss:\n",
            "1.153509673618135\n",
            "Epoch: 523 \tTraining Loss: 1.153510 \tValidation Loss: 0.027363\n",
            "train_loss:\n",
            "1.1534801909338424\n",
            "Epoch: 524 \tTraining Loss: 1.153480 \tValidation Loss: 0.027363\n",
            "train_loss:\n",
            "1.1534505191740099\n",
            "Epoch: 525 \tTraining Loss: 1.153451 \tValidation Loss: 0.027364\n",
            "train_loss:\n",
            "1.15342072951488\n",
            "Epoch: 526 \tTraining Loss: 1.153421 \tValidation Loss: 0.027365\n",
            "train_loss:\n",
            "1.15339080230657\n",
            "Epoch: 527 \tTraining Loss: 1.153391 \tValidation Loss: 0.027366\n",
            "train_loss:\n",
            "1.15336078165215\n",
            "Epoch: 528 \tTraining Loss: 1.153361 \tValidation Loss: 0.027367\n",
            "train_loss:\n",
            "1.1533305784721515\n",
            "Epoch: 529 \tTraining Loss: 1.153331 \tValidation Loss: 0.027368\n",
            "train_loss:\n",
            "1.153300208923144\n",
            "Epoch: 530 \tTraining Loss: 1.153300 \tValidation Loss: 0.027369\n",
            "train_loss:\n",
            "1.153269690035027\n",
            "Epoch: 531 \tTraining Loss: 1.153270 \tValidation Loss: 0.027370\n",
            "train_loss:\n",
            "1.153239070277511\n",
            "Epoch: 532 \tTraining Loss: 1.153239 \tValidation Loss: 0.027371\n",
            "train_loss:\n",
            "1.1532083732304557\n",
            "Epoch: 533 \tTraining Loss: 1.153208 \tValidation Loss: 0.027372\n",
            "train_loss:\n",
            "1.1531774879811885\n",
            "Epoch: 534 \tTraining Loss: 1.153177 \tValidation Loss: 0.027373\n",
            "train_loss:\n",
            "1.1531464053597642\n",
            "Epoch: 535 \tTraining Loss: 1.153146 \tValidation Loss: 0.027374\n",
            "train_loss:\n",
            "1.1531151476360502\n",
            "Epoch: 536 \tTraining Loss: 1.153115 \tValidation Loss: 0.027376\n",
            "train_loss:\n",
            "1.1530839554119459\n",
            "Epoch: 537 \tTraining Loss: 1.153084 \tValidation Loss: 0.027377\n",
            "train_loss:\n",
            "1.1530527361146696\n",
            "Epoch: 538 \tTraining Loss: 1.153053 \tValidation Loss: 0.027379\n",
            "train_loss:\n",
            "1.1530213089652988\n",
            "Epoch: 539 \tTraining Loss: 1.153021 \tValidation Loss: 0.027380\n",
            "train_loss:\n",
            "1.1529896691605286\n",
            "Epoch: 540 \tTraining Loss: 1.152990 \tValidation Loss: 0.027382\n",
            "train_loss:\n",
            "1.152958060795571\n",
            "Epoch: 541 \tTraining Loss: 1.152958 \tValidation Loss: 0.027384\n",
            "train_loss:\n",
            "1.1529266829892393\n",
            "Epoch: 542 \tTraining Loss: 1.152927 \tValidation Loss: 0.027385\n",
            "train_loss:\n",
            "1.1528955121616742\n",
            "Epoch: 543 \tTraining Loss: 1.152896 \tValidation Loss: 0.027387\n",
            "train_loss:\n",
            "1.1528642243081397\n",
            "Epoch: 544 \tTraining Loss: 1.152864 \tValidation Loss: 0.027389\n",
            "train_loss:\n",
            "1.1528327499990498\n",
            "Epoch: 545 \tTraining Loss: 1.152833 \tValidation Loss: 0.027391\n",
            "train_loss:\n",
            "1.1528016036246722\n",
            "Epoch: 546 \tTraining Loss: 1.152802 \tValidation Loss: 0.027393\n",
            "train_loss:\n",
            "1.1527711668293992\n",
            "Epoch: 547 \tTraining Loss: 1.152771 \tValidation Loss: 0.027396\n",
            "train_loss:\n",
            "1.1527412588343078\n",
            "Epoch: 548 \tTraining Loss: 1.152741 \tValidation Loss: 0.027398\n",
            "train_loss:\n",
            "1.152711490135053\n",
            "Epoch: 549 \tTraining Loss: 1.152711 \tValidation Loss: 0.027400\n",
            "train_loss:\n",
            "1.1526817899920565\n",
            "Epoch: 550 \tTraining Loss: 1.152682 \tValidation Loss: 0.027402\n",
            "train_loss:\n",
            "1.1526527422251718\n",
            "Epoch: 551 \tTraining Loss: 1.152653 \tValidation Loss: 0.027405\n",
            "train_loss:\n",
            "1.152624826728206\n",
            "Epoch: 552 \tTraining Loss: 1.152625 \tValidation Loss: 0.027407\n",
            "train_loss:\n",
            "1.1525979212352209\n",
            "Epoch: 553 \tTraining Loss: 1.152598 \tValidation Loss: 0.027409\n",
            "train_loss:\n",
            "1.1525714777328155\n",
            "Epoch: 554 \tTraining Loss: 1.152571 \tValidation Loss: 0.027412\n",
            "train_loss:\n",
            "1.1525455250408188\n",
            "Epoch: 555 \tTraining Loss: 1.152546 \tValidation Loss: 0.027414\n",
            "train_loss:\n",
            "1.1525203120577467\n",
            "Epoch: 556 \tTraining Loss: 1.152520 \tValidation Loss: 0.027416\n",
            "train_loss:\n",
            "1.152496133968507\n",
            "Epoch: 557 \tTraining Loss: 1.152496 \tValidation Loss: 0.027418\n",
            "train_loss:\n",
            "1.152472892087021\n",
            "Epoch: 558 \tTraining Loss: 1.152473 \tValidation Loss: 0.027420\n",
            "train_loss:\n",
            "1.1524502204888032\n",
            "Epoch: 559 \tTraining Loss: 1.152450 \tValidation Loss: 0.027422\n",
            "train_loss:\n",
            "1.1524280126278217\n",
            "Epoch: 560 \tTraining Loss: 1.152428 \tValidation Loss: 0.027424\n",
            "train_loss:\n",
            "1.1524062475442014\n",
            "Epoch: 561 \tTraining Loss: 1.152406 \tValidation Loss: 0.027426\n",
            "train_loss:\n",
            "1.1523848453284182\n",
            "Epoch: 562 \tTraining Loss: 1.152385 \tValidation Loss: 0.027428\n",
            "train_loss:\n",
            "1.1523637911338946\n",
            "Epoch: 563 \tTraining Loss: 1.152364 \tValidation Loss: 0.027429\n",
            "train_loss:\n",
            "1.152342867501926\n",
            "Epoch: 564 \tTraining Loss: 1.152343 \tValidation Loss: 0.027431\n",
            "train_loss:\n",
            "1.1523219766197623\n",
            "Epoch: 565 \tTraining Loss: 1.152322 \tValidation Loss: 0.027432\n",
            "train_loss:\n",
            "1.1523010149980202\n",
            "Epoch: 566 \tTraining Loss: 1.152301 \tValidation Loss: 0.027433\n",
            "train_loss:\n",
            "1.1522799123337855\n",
            "Epoch: 567 \tTraining Loss: 1.152280 \tValidation Loss: 0.027434\n",
            "train_loss:\n",
            "1.1522585860975496\n",
            "Epoch: 568 \tTraining Loss: 1.152259 \tValidation Loss: 0.027435\n",
            "train_loss:\n",
            "1.1522370694757817\n",
            "Epoch: 569 \tTraining Loss: 1.152237 \tValidation Loss: 0.027436\n",
            "train_loss:\n",
            "1.1522151965361376\n",
            "Epoch: 570 \tTraining Loss: 1.152215 \tValidation Loss: 0.027437\n",
            "train_loss:\n",
            "1.1521930323415623\n",
            "Epoch: 571 \tTraining Loss: 1.152193 \tValidation Loss: 0.027438\n",
            "train_loss:\n",
            "1.1521705292956732\n",
            "Epoch: 572 \tTraining Loss: 1.152171 \tValidation Loss: 0.027439\n",
            "train_loss:\n",
            "1.1521476332521263\n",
            "Epoch: 573 \tTraining Loss: 1.152148 \tValidation Loss: 0.027440\n",
            "train_loss:\n",
            "1.1521244494469611\n",
            "Epoch: 574 \tTraining Loss: 1.152124 \tValidation Loss: 0.027440\n",
            "train_loss:\n",
            "1.1521008975339897\n",
            "Epoch: 575 \tTraining Loss: 1.152101 \tValidation Loss: 0.027441\n",
            "train_loss:\n",
            "1.1520770338428763\n",
            "Epoch: 576 \tTraining Loss: 1.152077 \tValidation Loss: 0.027442\n",
            "train_loss:\n",
            "1.1520528706002149\n",
            "Epoch: 577 \tTraining Loss: 1.152053 \tValidation Loss: 0.027443\n",
            "train_loss:\n",
            "1.1520284104259897\n",
            "Epoch: 578 \tTraining Loss: 1.152028 \tValidation Loss: 0.027443\n",
            "train_loss:\n",
            "1.15200363498031\n",
            "Epoch: 579 \tTraining Loss: 1.152004 \tValidation Loss: 0.027444\n",
            "train_loss:\n",
            "1.1519786302859967\n",
            "Epoch: 580 \tTraining Loss: 1.151979 \tValidation Loss: 0.027445\n",
            "train_loss:\n",
            "1.1519533408867133\n",
            "Epoch: 581 \tTraining Loss: 1.151953 \tValidation Loss: 0.027445\n",
            "train_loss:\n",
            "1.1519278654685388\n",
            "Epoch: 582 \tTraining Loss: 1.151928 \tValidation Loss: 0.027446\n",
            "train_loss:\n",
            "1.151902178704957\n",
            "Epoch: 583 \tTraining Loss: 1.151902 \tValidation Loss: 0.027446\n",
            "train_loss:\n",
            "1.1518762504661477\n",
            "Epoch: 584 \tTraining Loss: 1.151876 \tValidation Loss: 0.027447\n",
            "train_loss:\n",
            "1.1518501894814628\n",
            "Epoch: 585 \tTraining Loss: 1.151850 \tValidation Loss: 0.027448\n",
            "train_loss:\n",
            "1.1518239232646676\n",
            "Epoch: 586 \tTraining Loss: 1.151824 \tValidation Loss: 0.027448\n",
            "train_loss:\n",
            "1.1517975365285908\n",
            "Epoch: 587 \tTraining Loss: 1.151798 \tValidation Loss: 0.027449\n",
            "train_loss:\n",
            "1.1517709982700837\n",
            "Epoch: 588 \tTraining Loss: 1.151771 \tValidation Loss: 0.027450\n",
            "train_loss:\n",
            "1.1517442949525603\n",
            "Epoch: 589 \tTraining Loss: 1.151744 \tValidation Loss: 0.027450\n",
            "train_loss:\n",
            "1.1517175012455756\n",
            "Epoch: 590 \tTraining Loss: 1.151718 \tValidation Loss: 0.027451\n",
            "train_loss:\n",
            "1.1516905704260745\n",
            "Epoch: 591 \tTraining Loss: 1.151691 \tValidation Loss: 0.027451\n",
            "train_loss:\n",
            "1.1516635448504717\n",
            "Epoch: 592 \tTraining Loss: 1.151664 \tValidation Loss: 0.027452\n",
            "train_loss:\n",
            "1.1516364227721105\n",
            "Epoch: 593 \tTraining Loss: 1.151636 \tValidation Loss: 0.027453\n",
            "train_loss:\n",
            "1.151609229517507\n",
            "Epoch: 594 \tTraining Loss: 1.151609 \tValidation Loss: 0.027453\n",
            "train_loss:\n",
            "1.151581938450153\n",
            "Epoch: 595 \tTraining Loss: 1.151582 \tValidation Loss: 0.027454\n",
            "train_loss:\n",
            "1.1515545469500643\n",
            "Epoch: 596 \tTraining Loss: 1.151555 \tValidation Loss: 0.027455\n",
            "train_loss:\n",
            "1.1515271139668894\n",
            "Epoch: 597 \tTraining Loss: 1.151527 \tValidation Loss: 0.027455\n",
            "train_loss:\n",
            "1.1514996098074721\n",
            "Epoch: 598 \tTraining Loss: 1.151500 \tValidation Loss: 0.027456\n",
            "train_loss:\n",
            "1.151472005651984\n",
            "Epoch: 599 \tTraining Loss: 1.151472 \tValidation Loss: 0.027456\n",
            "train_loss:\n",
            "1.151444388396574\n",
            "Epoch: 600 \tTraining Loss: 1.151444 \tValidation Loss: 0.027457\n",
            "train_loss:\n",
            "1.151416675948398\n",
            "Epoch: 601 \tTraining Loss: 1.151417 \tValidation Loss: 0.027457\n",
            "train_loss:\n",
            "1.1513889307504173\n",
            "Epoch: 602 \tTraining Loss: 1.151389 \tValidation Loss: 0.027458\n",
            "train_loss:\n",
            "1.1513611296594362\n",
            "Epoch: 603 \tTraining Loss: 1.151361 \tValidation Loss: 0.027459\n",
            "train_loss:\n",
            "1.1513332988752987\n",
            "Epoch: 604 \tTraining Loss: 1.151333 \tValidation Loss: 0.027459\n",
            "train_loss:\n",
            "1.1513054030282157\n",
            "Epoch: 605 \tTraining Loss: 1.151305 \tValidation Loss: 0.027460\n",
            "train_loss:\n",
            "1.1512774879679137\n",
            "Epoch: 606 \tTraining Loss: 1.151277 \tValidation Loss: 0.027460\n",
            "train_loss:\n",
            "1.15124950391469\n",
            "Epoch: 607 \tTraining Loss: 1.151250 \tValidation Loss: 0.027461\n",
            "train_loss:\n",
            "1.151221493661622\n",
            "Epoch: 608 \tTraining Loss: 1.151221 \tValidation Loss: 0.027461\n",
            "train_loss:\n",
            "1.1511934786052493\n",
            "Epoch: 609 \tTraining Loss: 1.151193 \tValidation Loss: 0.027461\n",
            "train_loss:\n",
            "1.1511653526362045\n",
            "Epoch: 610 \tTraining Loss: 1.151165 \tValidation Loss: 0.027462\n",
            "train_loss:\n",
            "1.1511372912934412\n",
            "Epoch: 611 \tTraining Loss: 1.151137 \tValidation Loss: 0.027462\n",
            "train_loss:\n",
            "1.151109136067904\n",
            "Epoch: 612 \tTraining Loss: 1.151109 \tValidation Loss: 0.027463\n",
            "train_loss:\n",
            "1.1510809869556637\n",
            "Epoch: 613 \tTraining Loss: 1.151081 \tValidation Loss: 0.027463\n",
            "train_loss:\n",
            "1.1510527828237513\n",
            "Epoch: 614 \tTraining Loss: 1.151053 \tValidation Loss: 0.027463\n",
            "train_loss:\n",
            "1.1510245542386512\n",
            "Epoch: 615 \tTraining Loss: 1.151025 \tValidation Loss: 0.027464\n",
            "train_loss:\n",
            "1.150996283733801\n",
            "Epoch: 616 \tTraining Loss: 1.150996 \tValidation Loss: 0.027464\n",
            "train_loss:\n",
            "1.1509680442320995\n",
            "Epoch: 617 \tTraining Loss: 1.150968 \tValidation Loss: 0.027464\n",
            "train_loss:\n",
            "1.1509397392307883\n",
            "Epoch: 618 \tTraining Loss: 1.150940 \tValidation Loss: 0.027465\n",
            "train_loss:\n",
            "1.150911448202727\n",
            "Epoch: 619 \tTraining Loss: 1.150911 \tValidation Loss: 0.027465\n",
            "train_loss:\n",
            "1.1508830777018062\n",
            "Epoch: 620 \tTraining Loss: 1.150883 \tValidation Loss: 0.027465\n",
            "train_loss:\n",
            "1.1508547080742133\n",
            "Epoch: 621 \tTraining Loss: 1.150855 \tValidation Loss: 0.027466\n",
            "train_loss:\n",
            "1.1508263187967378\n",
            "Epoch: 622 \tTraining Loss: 1.150826 \tValidation Loss: 0.027466\n",
            "train_loss:\n",
            "1.150797927772606\n",
            "Epoch: 623 \tTraining Loss: 1.150798 \tValidation Loss: 0.027466\n",
            "train_loss:\n",
            "1.150769517535255\n",
            "Epoch: 624 \tTraining Loss: 1.150770 \tValidation Loss: 0.027466\n",
            "train_loss:\n",
            "1.150741044854943\n",
            "Epoch: 625 \tTraining Loss: 1.150741 \tValidation Loss: 0.027466\n",
            "train_loss:\n",
            "1.1507125883312015\n",
            "Epoch: 626 \tTraining Loss: 1.150713 \tValidation Loss: 0.027466\n",
            "train_loss:\n",
            "1.1506841139042334\n",
            "Epoch: 627 \tTraining Loss: 1.150684 \tValidation Loss: 0.027466\n",
            "train_loss:\n",
            "1.150655645153898\n",
            "Epoch: 628 \tTraining Loss: 1.150656 \tValidation Loss: 0.027467\n",
            "train_loss:\n",
            "1.1506271309905\n",
            "Epoch: 629 \tTraining Loss: 1.150627 \tValidation Loss: 0.027467\n",
            "train_loss:\n",
            "1.1505985919372503\n",
            "Epoch: 630 \tTraining Loss: 1.150599 \tValidation Loss: 0.027467\n",
            "train_loss:\n",
            "1.15057007646386\n",
            "Epoch: 631 \tTraining Loss: 1.150570 \tValidation Loss: 0.027467\n",
            "train_loss:\n",
            "1.1505415653571105\n",
            "Epoch: 632 \tTraining Loss: 1.150542 \tValidation Loss: 0.027467\n",
            "train_loss:\n",
            "1.1505129913707355\n",
            "Epoch: 633 \tTraining Loss: 1.150513 \tValidation Loss: 0.027467\n",
            "train_loss:\n",
            "1.1504843876912043\n",
            "Epoch: 634 \tTraining Loss: 1.150484 \tValidation Loss: 0.027467\n",
            "train_loss:\n",
            "1.1504558193814622\n",
            "Epoch: 635 \tTraining Loss: 1.150456 \tValidation Loss: 0.027467\n",
            "train_loss:\n",
            "1.1504271999820248\n",
            "Epoch: 636 \tTraining Loss: 1.150427 \tValidation Loss: 0.027467\n",
            "train_loss:\n",
            "1.1503985329862043\n",
            "Epoch: 637 \tTraining Loss: 1.150399 \tValidation Loss: 0.027466\n",
            "train_loss:\n",
            "1.1503699279966808\n",
            "Epoch: 638 \tTraining Loss: 1.150370 \tValidation Loss: 0.027466\n",
            "train_loss:\n",
            "1.1503412365476726\n",
            "Epoch: 639 \tTraining Loss: 1.150341 \tValidation Loss: 0.027466\n",
            "train_loss:\n",
            "1.1503125857084224\n",
            "Epoch: 640 \tTraining Loss: 1.150313 \tValidation Loss: 0.027466\n",
            "train_loss:\n",
            "1.1502838663129142\n",
            "Epoch: 641 \tTraining Loss: 1.150284 \tValidation Loss: 0.027466\n",
            "train_loss:\n",
            "1.1502551656939608\n",
            "Epoch: 642 \tTraining Loss: 1.150255 \tValidation Loss: 0.027466\n",
            "train_loss:\n",
            "1.1502264414951477\n",
            "Epoch: 643 \tTraining Loss: 1.150226 \tValidation Loss: 0.027465\n",
            "train_loss:\n",
            "1.1501976635866549\n",
            "Epoch: 644 \tTraining Loss: 1.150198 \tValidation Loss: 0.027465\n",
            "train_loss:\n",
            "1.150168931964553\n",
            "Epoch: 645 \tTraining Loss: 1.150169 \tValidation Loss: 0.027465\n",
            "train_loss:\n",
            "1.1501401243629037\n",
            "Epoch: 646 \tTraining Loss: 1.150140 \tValidation Loss: 0.027465\n",
            "train_loss:\n",
            "1.1501113040979964\n",
            "Epoch: 647 \tTraining Loss: 1.150111 \tValidation Loss: 0.027464\n",
            "train_loss:\n",
            "1.1500824833964254\n",
            "Epoch: 648 \tTraining Loss: 1.150082 \tValidation Loss: 0.027464\n",
            "train_loss:\n",
            "1.1500536102951664\n",
            "Epoch: 649 \tTraining Loss: 1.150054 \tValidation Loss: 0.027464\n",
            "train_loss:\n",
            "1.1500247498571654\n",
            "Epoch: 650 \tTraining Loss: 1.150025 \tValidation Loss: 0.027463\n",
            "train_loss:\n",
            "1.149995804706336\n",
            "Epoch: 651 \tTraining Loss: 1.149996 \tValidation Loss: 0.027463\n",
            "train_loss:\n",
            "1.1499668669787955\n",
            "Epoch: 652 \tTraining Loss: 1.149967 \tValidation Loss: 0.027462\n",
            "train_loss:\n",
            "1.149937928814591\n",
            "Epoch: 653 \tTraining Loss: 1.149938 \tValidation Loss: 0.027462\n",
            "train_loss:\n",
            "1.1499089268974332\n",
            "Epoch: 654 \tTraining Loss: 1.149909 \tValidation Loss: 0.027461\n",
            "train_loss:\n",
            "1.1498798917938065\n",
            "Epoch: 655 \tTraining Loss: 1.149880 \tValidation Loss: 0.027461\n",
            "train_loss:\n",
            "1.149850851013547\n",
            "Epoch: 656 \tTraining Loss: 1.149851 \tValidation Loss: 0.027460\n",
            "train_loss:\n",
            "1.1498217993166857\n",
            "Epoch: 657 \tTraining Loss: 1.149822 \tValidation Loss: 0.027460\n",
            "train_loss:\n",
            "1.1497926712036133\n",
            "Epoch: 658 \tTraining Loss: 1.149793 \tValidation Loss: 0.027459\n",
            "train_loss:\n",
            "1.1497635409072204\n",
            "Epoch: 659 \tTraining Loss: 1.149764 \tValidation Loss: 0.027459\n",
            "train_loss:\n",
            "1.149734342927898\n",
            "Epoch: 660 \tTraining Loss: 1.149734 \tValidation Loss: 0.027458\n",
            "train_loss:\n",
            "1.1497051628518018\n",
            "Epoch: 661 \tTraining Loss: 1.149705 \tValidation Loss: 0.027457\n",
            "train_loss:\n",
            "1.1496759059228303\n",
            "Epoch: 662 \tTraining Loss: 1.149676 \tValidation Loss: 0.027457\n",
            "train_loss:\n",
            "1.1496466455005465\n",
            "Epoch: 663 \tTraining Loss: 1.149647 \tValidation Loss: 0.027456\n",
            "train_loss:\n",
            "1.1496173763449813\n",
            "Epoch: 664 \tTraining Loss: 1.149617 \tValidation Loss: 0.027455\n",
            "train_loss:\n",
            "1.1495880277165562\n",
            "Epoch: 665 \tTraining Loss: 1.149588 \tValidation Loss: 0.027454\n",
            "train_loss:\n",
            "1.1495586651148813\n",
            "Epoch: 666 \tTraining Loss: 1.149559 \tValidation Loss: 0.027453\n",
            "train_loss:\n",
            "1.1495292391969172\n",
            "Epoch: 667 \tTraining Loss: 1.149529 \tValidation Loss: 0.027453\n",
            "train_loss:\n",
            "1.1494998198289137\n",
            "Epoch: 668 \tTraining Loss: 1.149500 \tValidation Loss: 0.027452\n",
            "train_loss:\n",
            "1.1494703664011134\n",
            "Epoch: 669 \tTraining Loss: 1.149470 \tValidation Loss: 0.027451\n",
            "train_loss:\n",
            "1.1494408278238206\n",
            "Epoch: 670 \tTraining Loss: 1.149441 \tValidation Loss: 0.027450\n",
            "train_loss:\n",
            "1.1494113372795747\n",
            "Epoch: 671 \tTraining Loss: 1.149411 \tValidation Loss: 0.027449\n",
            "train_loss:\n",
            "1.1493817305826879\n",
            "Epoch: 672 \tTraining Loss: 1.149382 \tValidation Loss: 0.027448\n",
            "train_loss:\n",
            "1.1493521282524417\n",
            "Epoch: 673 \tTraining Loss: 1.149352 \tValidation Loss: 0.027447\n",
            "train_loss:\n",
            "1.1493224547459528\n",
            "Epoch: 674 \tTraining Loss: 1.149322 \tValidation Loss: 0.027446\n",
            "train_loss:\n",
            "1.149292820975894\n",
            "Epoch: 675 \tTraining Loss: 1.149293 \tValidation Loss: 0.027444\n",
            "train_loss:\n",
            "1.1492630754198347\n",
            "Epoch: 676 \tTraining Loss: 1.149263 \tValidation Loss: 0.027443\n",
            "train_loss:\n",
            "1.1492333464570097\n",
            "Epoch: 677 \tTraining Loss: 1.149233 \tValidation Loss: 0.027442\n",
            "train_loss:\n",
            "1.1492035755744348\n",
            "Epoch: 678 \tTraining Loss: 1.149204 \tValidation Loss: 0.027441\n",
            "train_loss:\n",
            "1.149173779802008\n",
            "Epoch: 679 \tTraining Loss: 1.149174 \tValidation Loss: 0.027439\n",
            "train_loss:\n",
            "1.1491438980067605\n",
            "Epoch: 680 \tTraining Loss: 1.149144 \tValidation Loss: 0.027438\n",
            "train_loss:\n",
            "1.1491140507079742\n",
            "Epoch: 681 \tTraining Loss: 1.149114 \tValidation Loss: 0.027437\n",
            "train_loss:\n",
            "1.1490840981731485\n",
            "Epoch: 682 \tTraining Loss: 1.149084 \tValidation Loss: 0.027435\n",
            "train_loss:\n",
            "1.1490541932347056\n",
            "Epoch: 683 \tTraining Loss: 1.149054 \tValidation Loss: 0.027434\n",
            "train_loss:\n",
            "1.1490241319705279\n",
            "Epoch: 684 \tTraining Loss: 1.149024 \tValidation Loss: 0.027432\n",
            "train_loss:\n",
            "1.1489942108755147\n",
            "Epoch: 685 \tTraining Loss: 1.148994 \tValidation Loss: 0.027431\n",
            "train_loss:\n",
            "1.1489640312753755\n",
            "Epoch: 686 \tTraining Loss: 1.148964 \tValidation Loss: 0.027429\n",
            "train_loss:\n",
            "1.1489341062503857\n",
            "Epoch: 687 \tTraining Loss: 1.148934 \tValidation Loss: 0.027427\n",
            "train_loss:\n",
            "1.1489037655212067\n",
            "Epoch: 688 \tTraining Loss: 1.148904 \tValidation Loss: 0.027425\n",
            "train_loss:\n",
            "1.1488738902759204\n",
            "Epoch: 689 \tTraining Loss: 1.148874 \tValidation Loss: 0.027424\n",
            "train_loss:\n",
            "1.148843364401178\n",
            "Epoch: 690 \tTraining Loss: 1.148843 \tValidation Loss: 0.027422\n",
            "train_loss:\n",
            "1.1488136074918531\n",
            "Epoch: 691 \tTraining Loss: 1.148814 \tValidation Loss: 0.027420\n",
            "train_loss:\n",
            "1.1487827514990783\n",
            "Epoch: 692 \tTraining Loss: 1.148783 \tValidation Loss: 0.027418\n",
            "train_loss:\n",
            "1.1487532640114808\n",
            "Epoch: 693 \tTraining Loss: 1.148753 \tValidation Loss: 0.027416\n",
            "train_loss:\n",
            "1.1487219041083758\n",
            "Epoch: 694 \tTraining Loss: 1.148722 \tValidation Loss: 0.027414\n",
            "train_loss:\n",
            "1.1486929170377962\n",
            "Epoch: 695 \tTraining Loss: 1.148693 \tValidation Loss: 0.027412\n",
            "train_loss:\n",
            "1.148660699526469\n",
            "Epoch: 696 \tTraining Loss: 1.148661 \tValidation Loss: 0.027410\n",
            "train_loss:\n",
            "1.1486327272631747\n",
            "Epoch: 697 \tTraining Loss: 1.148633 \tValidation Loss: 0.027408\n",
            "train_loss:\n",
            "1.1485990285873413\n",
            "Epoch: 698 \tTraining Loss: 1.148599 \tValidation Loss: 0.027405\n",
            "train_loss:\n",
            "1.1485729527560782\n",
            "Epoch: 699 \tTraining Loss: 1.148573 \tValidation Loss: 0.027403\n",
            "train_loss:\n",
            "1.1485364410903427\n",
            "Epoch: 700 \tTraining Loss: 1.148536 \tValidation Loss: 0.027400\n",
            "train_loss:\n",
            "1.1485141105267591\n",
            "Epoch: 701 \tTraining Loss: 1.148514 \tValidation Loss: 0.027399\n",
            "train_loss:\n",
            "1.148472309985877\n",
            "Epoch: 702 \tTraining Loss: 1.148472 \tValidation Loss: 0.027395\n",
            "train_loss:\n",
            "1.1484573616649643\n",
            "Epoch: 703 \tTraining Loss: 1.148457 \tValidation Loss: 0.027394\n",
            "train_loss:\n",
            "1.1484052829253368\n",
            "Epoch: 704 \tTraining Loss: 1.148405 \tValidation Loss: 0.027390\n",
            "train_loss:\n",
            "1.1484050641566406\n",
            "Epoch: 705 \tTraining Loss: 1.148405 \tValidation Loss: 0.027389\n",
            "train_loss:\n",
            "1.1483328818401575\n",
            "Epoch: 706 \tTraining Loss: 1.148333 \tValidation Loss: 0.027383\n",
            "train_loss:\n",
            "1.1483626780492482\n",
            "Epoch: 707 \tTraining Loss: 1.148363 \tValidation Loss: 0.027385\n",
            "train_loss:\n",
            "1.1482514518521207\n",
            "Epoch: 708 \tTraining Loss: 1.148251 \tValidation Loss: 0.027376\n",
            "train_loss:\n",
            "1.1483422212111645\n",
            "Epoch: 709 \tTraining Loss: 1.148342 \tValidation Loss: 0.027382\n",
            "train_loss:\n",
            "1.148160266352224\n",
            "Epoch: 710 \tTraining Loss: 1.148160 \tValidation Loss: 0.027367\n",
            "train_loss:\n",
            "1.148361365437071\n",
            "Epoch: 711 \tTraining Loss: 1.148361 \tValidation Loss: 0.027382\n",
            "train_loss:\n",
            "1.1480763430123802\n",
            "Epoch: 712 \tTraining Loss: 1.148076 \tValidation Loss: 0.027359\n",
            "train_loss:\n",
            "1.148388982692481\n",
            "Epoch: 713 \tTraining Loss: 1.148389 \tValidation Loss: 0.027382\n",
            "train_loss:\n",
            "1.1480197884660936\n",
            "Epoch: 714 \tTraining Loss: 1.148020 \tValidation Loss: 0.027352\n",
            "Validation loss decreased (0.027358 --> 0.027352).  Saving model ...\n",
            "train_loss:\n",
            "1.148324076945965\n",
            "Epoch: 715 \tTraining Loss: 1.148324 \tValidation Loss: 0.027375\n",
            "train_loss:\n",
            "1.147958245032873\n",
            "Epoch: 716 \tTraining Loss: 1.147958 \tValidation Loss: 0.027345\n",
            "Validation loss decreased (0.027352 --> 0.027345).  Saving model ...\n",
            "train_loss:\n",
            "1.1482651709636926\n",
            "Epoch: 717 \tTraining Loss: 1.148265 \tValidation Loss: 0.027368\n",
            "train_loss:\n",
            "1.1478999176304856\n",
            "Epoch: 718 \tTraining Loss: 1.147900 \tValidation Loss: 0.027338\n",
            "Validation loss decreased (0.027345 --> 0.027338).  Saving model ...\n",
            "train_loss:\n",
            "1.1482044951819674\n",
            "Epoch: 719 \tTraining Loss: 1.148204 \tValidation Loss: 0.027361\n",
            "train_loss:\n",
            "1.1478358480956528\n",
            "Epoch: 720 \tTraining Loss: 1.147836 \tValidation Loss: 0.027330\n",
            "Validation loss decreased (0.027338 --> 0.027330).  Saving model ...\n",
            "train_loss:\n",
            "1.1481437678738828\n",
            "Epoch: 721 \tTraining Loss: 1.148144 \tValidation Loss: 0.027353\n",
            "train_loss:\n",
            "1.147776158738049\n",
            "Epoch: 722 \tTraining Loss: 1.147776 \tValidation Loss: 0.027322\n",
            "Validation loss decreased (0.027330 --> 0.027322).  Saving model ...\n",
            "train_loss:\n",
            "1.148084044456482\n",
            "Epoch: 723 \tTraining Loss: 1.148084 \tValidation Loss: 0.027346\n",
            "train_loss:\n",
            "1.1477115507964248\n",
            "Epoch: 724 \tTraining Loss: 1.147712 \tValidation Loss: 0.027314\n",
            "Validation loss decreased (0.027322 --> 0.027314).  Saving model ...\n",
            "train_loss:\n",
            "1.1480217477777501\n",
            "Epoch: 725 \tTraining Loss: 1.148022 \tValidation Loss: 0.027337\n",
            "train_loss:\n",
            "1.1476517693027035\n",
            "Epoch: 726 \tTraining Loss: 1.147652 \tValidation Loss: 0.027306\n",
            "Validation loss decreased (0.027314 --> 0.027306).  Saving model ...\n",
            "train_loss:\n",
            "1.147962144006303\n",
            "Epoch: 727 \tTraining Loss: 1.147962 \tValidation Loss: 0.027329\n",
            "train_loss:\n",
            "1.1475859125018557\n",
            "Epoch: 728 \tTraining Loss: 1.147586 \tValidation Loss: 0.027296\n",
            "Validation loss decreased (0.027306 --> 0.027296).  Saving model ...\n",
            "train_loss:\n",
            "1.1478976613872653\n",
            "Epoch: 729 \tTraining Loss: 1.147898 \tValidation Loss: 0.027320\n",
            "train_loss:\n",
            "1.1475266912481288\n",
            "Epoch: 730 \tTraining Loss: 1.147527 \tValidation Loss: 0.027288\n",
            "Validation loss decreased (0.027296 --> 0.027288).  Saving model ...\n",
            "train_loss:\n",
            "1.1478383327141786\n",
            "Epoch: 731 \tTraining Loss: 1.147838 \tValidation Loss: 0.027311\n",
            "train_loss:\n",
            "1.1474574830505875\n",
            "Epoch: 732 \tTraining Loss: 1.147457 \tValidation Loss: 0.027277\n",
            "Validation loss decreased (0.027288 --> 0.027277).  Saving model ...\n",
            "train_loss:\n",
            "1.1477697105198117\n",
            "Epoch: 733 \tTraining Loss: 1.147770 \tValidation Loss: 0.027301\n",
            "train_loss:\n",
            "1.1474004149873616\n",
            "Epoch: 734 \tTraining Loss: 1.147400 \tValidation Loss: 0.027269\n",
            "Validation loss decreased (0.027277 --> 0.027269).  Saving model ...\n",
            "train_loss:\n",
            "1.1477121333062867\n",
            "Epoch: 735 \tTraining Loss: 1.147712 \tValidation Loss: 0.027293\n",
            "train_loss:\n",
            "1.1473223538625807\n",
            "Epoch: 736 \tTraining Loss: 1.147322 \tValidation Loss: 0.027257\n",
            "Validation loss decreased (0.027269 --> 0.027257).  Saving model ...\n",
            "train_loss:\n",
            "1.1476339647621463\n",
            "Epoch: 737 \tTraining Loss: 1.147634 \tValidation Loss: 0.027281\n",
            "train_loss:\n",
            "1.1472731409492074\n",
            "Epoch: 738 \tTraining Loss: 1.147273 \tValidation Loss: 0.027249\n",
            "Validation loss decreased (0.027257 --> 0.027249).  Saving model ...\n",
            "train_loss:\n",
            "1.1475849391776562\n",
            "Epoch: 739 \tTraining Loss: 1.147585 \tValidation Loss: 0.027274\n",
            "train_loss:\n",
            "1.147169951554183\n",
            "Epoch: 740 \tTraining Loss: 1.147170 \tValidation Loss: 0.027234\n",
            "Validation loss decreased (0.027249 --> 0.027234).  Saving model ...\n",
            "train_loss:\n",
            "1.147482151076907\n",
            "Epoch: 741 \tTraining Loss: 1.147482 \tValidation Loss: 0.027259\n",
            "train_loss:\n",
            "1.1471495113093337\n",
            "Epoch: 742 \tTraining Loss: 1.147150 \tValidation Loss: 0.027230\n",
            "Validation loss decreased (0.027234 --> 0.027230).  Saving model ...\n",
            "train_loss:\n",
            "1.1474722986256247\n",
            "Epoch: 743 \tTraining Loss: 1.147472 \tValidation Loss: 0.027257\n",
            "train_loss:\n",
            "1.1469783355028202\n",
            "Epoch: 744 \tTraining Loss: 1.146978 \tValidation Loss: 0.027208\n",
            "Validation loss decreased (0.027230 --> 0.027208).  Saving model ...\n",
            "train_loss:\n",
            "1.1473055008130195\n",
            "Epoch: 745 \tTraining Loss: 1.147306 \tValidation Loss: 0.027233\n",
            "train_loss:\n",
            "1.1470221329084682\n",
            "Epoch: 746 \tTraining Loss: 1.147022 \tValidation Loss: 0.027212\n",
            "train_loss:\n",
            "1.1473499549614203\n",
            "Epoch: 747 \tTraining Loss: 1.147350 \tValidation Loss: 0.027241\n",
            "train_loss:\n",
            "1.1467024756001902\n",
            "Epoch: 748 \tTraining Loss: 1.146702 \tValidation Loss: 0.027176\n",
            "Validation loss decreased (0.027208 --> 0.027176).  Saving model ...\n",
            "train_loss:\n",
            "1.147427472002777\n",
            "Epoch: 749 \tTraining Loss: 1.147427 \tValidation Loss: 0.027241\n",
            "train_loss:\n",
            "1.1463965417701245\n",
            "Epoch: 750 \tTraining Loss: 1.146397 \tValidation Loss: 0.027165\n",
            "Validation loss decreased (0.027176 --> 0.027165).  Saving model ...\n",
            "train_loss:\n",
            "1.1472313417183173\n",
            "Epoch: 751 \tTraining Loss: 1.147231 \tValidation Loss: 0.027223\n",
            "train_loss:\n",
            "1.1466329128314288\n",
            "Epoch: 752 \tTraining Loss: 1.146633 \tValidation Loss: 0.027179\n",
            "train_loss:\n",
            "1.146462823445107\n",
            "Epoch: 753 \tTraining Loss: 1.146463 \tValidation Loss: 0.027151\n",
            "Validation loss decreased (0.027165 --> 0.027151).  Saving model ...\n",
            "train_loss:\n",
            "1.1483120699941893\n",
            "Epoch: 754 \tTraining Loss: 1.148312 \tValidation Loss: 0.027232\n",
            "train_loss:\n",
            "1.1470260139786717\n",
            "Epoch: 755 \tTraining Loss: 1.147026 \tValidation Loss: 0.027266\n",
            "train_loss:\n",
            "1.1463854509395557\n",
            "Epoch: 756 \tTraining Loss: 1.146385 \tValidation Loss: 0.027221\n",
            "train_loss:\n",
            "1.1459408417726173\n",
            "Epoch: 757 \tTraining Loss: 1.145941 \tValidation Loss: 0.027185\n",
            "train_loss:\n",
            "1.1460992722284227\n",
            "Epoch: 758 \tTraining Loss: 1.146099 \tValidation Loss: 0.027193\n",
            "train_loss:\n",
            "1.1454708986666613\n",
            "Epoch: 759 \tTraining Loss: 1.145471 \tValidation Loss: 0.027142\n",
            "Validation loss decreased (0.027151 --> 0.027142).  Saving model ...\n",
            "train_loss:\n",
            "1.1458034144216405\n",
            "Epoch: 760 \tTraining Loss: 1.145803 \tValidation Loss: 0.027166\n",
            "train_loss:\n",
            "1.1449402081660736\n",
            "Epoch: 761 \tTraining Loss: 1.144940 \tValidation Loss: 0.027101\n",
            "Validation loss decreased (0.027142 --> 0.027101).  Saving model ...\n",
            "train_loss:\n",
            "1.1449920794902704\n",
            "Epoch: 762 \tTraining Loss: 1.144992 \tValidation Loss: 0.027110\n",
            "train_loss:\n",
            "1.1442326656628004\n",
            "Epoch: 763 \tTraining Loss: 1.144233 \tValidation Loss: 0.027050\n",
            "Validation loss decreased (0.027101 --> 0.027050).  Saving model ...\n",
            "train_loss:\n",
            "1.1443518780089996\n",
            "Epoch: 764 \tTraining Loss: 1.144352 \tValidation Loss: 0.027061\n",
            "train_loss:\n",
            "1.1435315678844522\n",
            "Epoch: 765 \tTraining Loss: 1.143532 \tValidation Loss: 0.026984\n",
            "Validation loss decreased (0.027050 --> 0.026984).  Saving model ...\n",
            "train_loss:\n",
            "1.1437417377919068\n",
            "Epoch: 766 \tTraining Loss: 1.143742 \tValidation Loss: 0.026997\n",
            "train_loss:\n",
            "1.1427730138485248\n",
            "Epoch: 767 \tTraining Loss: 1.142773 \tValidation Loss: 0.026918\n",
            "Validation loss decreased (0.026984 --> 0.026918).  Saving model ...\n",
            "train_loss:\n",
            "1.144771042760912\n",
            "Epoch: 768 \tTraining Loss: 1.144771 \tValidation Loss: 0.027061\n",
            "train_loss:\n",
            "1.1426597314003186\n",
            "Epoch: 769 \tTraining Loss: 1.142660 \tValidation Loss: 0.027010\n",
            "train_loss:\n",
            "1.1430624212537492\n",
            "Epoch: 770 \tTraining Loss: 1.143062 \tValidation Loss: 0.027062\n",
            "train_loss:\n",
            "1.1422579716413448\n",
            "Epoch: 771 \tTraining Loss: 1.142258 \tValidation Loss: 0.027027\n",
            "train_loss:\n",
            "1.1413653115213136\n",
            "Epoch: 772 \tTraining Loss: 1.141365 \tValidation Loss: 0.026971\n",
            "train_loss:\n",
            "1.1416005385227692\n",
            "Epoch: 773 \tTraining Loss: 1.141601 \tValidation Loss: 0.026992\n",
            "train_loss:\n",
            "1.1408202787895343\n",
            "Epoch: 774 \tTraining Loss: 1.140820 \tValidation Loss: 0.026921\n",
            "train_loss:\n",
            "1.1410365650505374\n",
            "Epoch: 775 \tTraining Loss: 1.141037 \tValidation Loss: 0.026937\n",
            "train_loss:\n",
            "1.140804254528367\n",
            "Epoch: 776 \tTraining Loss: 1.140804 \tValidation Loss: 0.026938\n",
            "train_loss:\n",
            "1.1403639770689464\n",
            "Epoch: 777 \tTraining Loss: 1.140364 \tValidation Loss: 0.026936\n",
            "train_loss:\n",
            "1.1408623470054877\n",
            "Epoch: 778 \tTraining Loss: 1.140862 \tValidation Loss: 0.026983\n",
            "train_loss:\n",
            "1.1396401583493412\n",
            "Epoch: 779 \tTraining Loss: 1.139640 \tValidation Loss: 0.026978\n",
            "train_loss:\n",
            "1.1389293190323826\n",
            "Epoch: 780 \tTraining Loss: 1.138929 \tValidation Loss: 0.026914\n",
            "Validation loss decreased (0.026918 --> 0.026914).  Saving model ...\n",
            "train_loss:\n",
            "1.1397796262314905\n",
            "Epoch: 781 \tTraining Loss: 1.139780 \tValidation Loss: 0.026976\n",
            "train_loss:\n",
            "1.138601689548283\n",
            "Epoch: 782 \tTraining Loss: 1.138602 \tValidation Loss: 0.026881\n",
            "Validation loss decreased (0.026914 --> 0.026881).  Saving model ...\n",
            "train_loss:\n",
            "1.1412825396646074\n",
            "Epoch: 783 \tTraining Loss: 1.141283 \tValidation Loss: 0.026991\n",
            "train_loss:\n",
            "1.139019427718697\n",
            "Epoch: 784 \tTraining Loss: 1.139019 \tValidation Loss: 0.027035\n",
            "train_loss:\n",
            "1.1379930558222118\n",
            "Epoch: 785 \tTraining Loss: 1.137993 \tValidation Loss: 0.026968\n",
            "train_loss:\n",
            "1.1388358484694372\n",
            "Epoch: 786 \tTraining Loss: 1.138836 \tValidation Loss: 0.027050\n",
            "train_loss:\n",
            "1.1377592719954885\n",
            "Epoch: 787 \tTraining Loss: 1.137759 \tValidation Loss: 0.026985\n",
            "train_loss:\n",
            "1.1375502487678668\n",
            "Epoch: 788 \tTraining Loss: 1.137550 \tValidation Loss: 0.026979\n",
            "train_loss:\n",
            "1.1378874132484744\n",
            "Epoch: 789 \tTraining Loss: 1.137887 \tValidation Loss: 0.027019\n",
            "train_loss:\n",
            "1.1374043881238163\n",
            "Epoch: 790 \tTraining Loss: 1.137404 \tValidation Loss: 0.026987\n",
            "train_loss:\n",
            "1.1371203460099497\n",
            "Epoch: 791 \tTraining Loss: 1.137120 \tValidation Loss: 0.026986\n",
            "train_loss:\n",
            "1.1368076622267782\n",
            "Epoch: 792 \tTraining Loss: 1.136808 \tValidation Loss: 0.026962\n",
            "train_loss:\n",
            "1.1377452275691888\n",
            "Epoch: 793 \tTraining Loss: 1.137745 \tValidation Loss: 0.027031\n",
            "train_loss:\n",
            "1.1372923396882557\n",
            "Epoch: 794 \tTraining Loss: 1.137292 \tValidation Loss: 0.027036\n",
            "train_loss:\n",
            "1.136205541345226\n",
            "Epoch: 795 \tTraining Loss: 1.136206 \tValidation Loss: 0.026957\n",
            "train_loss:\n",
            "1.136006399388715\n",
            "Epoch: 796 \tTraining Loss: 1.136006 \tValidation Loss: 0.026947\n",
            "train_loss:\n",
            "1.1365330000936766\n",
            "Epoch: 797 \tTraining Loss: 1.136533 \tValidation Loss: 0.026991\n",
            "train_loss:\n",
            "1.1360947413322253\n",
            "Epoch: 798 \tTraining Loss: 1.136095 \tValidation Loss: 0.026957\n",
            "train_loss:\n",
            "1.1356623098527119\n",
            "Epoch: 799 \tTraining Loss: 1.135662 \tValidation Loss: 0.026938\n",
            "train_loss:\n",
            "1.1355621840927628\n",
            "Epoch: 800 \tTraining Loss: 1.135562 \tValidation Loss: 0.026927\n",
            "train_loss:\n",
            "1.1363746364474734\n",
            "Epoch: 801 \tTraining Loss: 1.136375 \tValidation Loss: 0.026988\n",
            "train_loss:\n",
            "1.136038901168348\n",
            "Epoch: 802 \tTraining Loss: 1.136039 \tValidation Loss: 0.026989\n",
            "train_loss:\n",
            "1.1349260999169541\n",
            "Epoch: 803 \tTraining Loss: 1.134926 \tValidation Loss: 0.026904\n",
            "train_loss:\n",
            "1.1352893310588794\n",
            "Epoch: 804 \tTraining Loss: 1.135289 \tValidation Loss: 0.026944\n",
            "train_loss:\n",
            "1.1346580340312078\n",
            "Epoch: 805 \tTraining Loss: 1.134658 \tValidation Loss: 0.026886\n",
            "train_loss:\n",
            "1.1358832522626325\n",
            "Epoch: 806 \tTraining Loss: 1.135883 \tValidation Loss: 0.026975\n",
            "train_loss:\n",
            "1.1351592143376668\n",
            "Epoch: 807 \tTraining Loss: 1.135159 \tValidation Loss: 0.026952\n",
            "train_loss:\n",
            "1.1341801571758676\n",
            "Epoch: 808 \tTraining Loss: 1.134180 \tValidation Loss: 0.026876\n",
            "Validation loss decreased (0.026881 --> 0.026876).  Saving model ...\n",
            "train_loss:\n",
            "1.13445664587475\n",
            "Epoch: 809 \tTraining Loss: 1.134457 \tValidation Loss: 0.026891\n",
            "train_loss:\n",
            "1.1342559319276075\n",
            "Epoch: 810 \tTraining Loss: 1.134256 \tValidation Loss: 0.026882\n",
            "train_loss:\n",
            "1.1341155466142592\n",
            "Epoch: 811 \tTraining Loss: 1.134116 \tValidation Loss: 0.026855\n",
            "Validation loss decreased (0.026876 --> 0.026855).  Saving model ...\n",
            "train_loss:\n",
            "1.135565493133042\n",
            "Epoch: 812 \tTraining Loss: 1.135565 \tValidation Loss: 0.026924\n",
            "train_loss:\n",
            "1.1348378007665223\n",
            "Epoch: 813 \tTraining Loss: 1.134838 \tValidation Loss: 0.026925\n",
            "train_loss:\n",
            "1.13460642907209\n",
            "Epoch: 814 \tTraining Loss: 1.134606 \tValidation Loss: 0.026899\n",
            "train_loss:\n",
            "1.1337013423661173\n",
            "Epoch: 815 \tTraining Loss: 1.133701 \tValidation Loss: 0.026848\n",
            "Validation loss decreased (0.026855 --> 0.026848).  Saving model ...\n",
            "train_loss:\n",
            "1.1330428656204279\n",
            "Epoch: 816 \tTraining Loss: 1.133043 \tValidation Loss: 0.026741\n",
            "Validation loss decreased (0.026848 --> 0.026741).  Saving model ...\n",
            "train_loss:\n",
            "1.1352240532745808\n",
            "Epoch: 817 \tTraining Loss: 1.135224 \tValidation Loss: 0.026857\n",
            "train_loss:\n",
            "1.1344215821870518\n",
            "Epoch: 818 \tTraining Loss: 1.134422 \tValidation Loss: 0.026914\n",
            "train_loss:\n",
            "1.1342010991477267\n",
            "Epoch: 819 \tTraining Loss: 1.134201 \tValidation Loss: 0.026927\n",
            "train_loss:\n",
            "1.1330905109098106\n",
            "Epoch: 820 \tTraining Loss: 1.133091 \tValidation Loss: 0.026835\n",
            "train_loss:\n",
            "1.1329132667827955\n",
            "Epoch: 821 \tTraining Loss: 1.132913 \tValidation Loss: 0.026831\n",
            "train_loss:\n",
            "1.1328631010684338\n",
            "Epoch: 822 \tTraining Loss: 1.132863 \tValidation Loss: 0.026797\n",
            "train_loss:\n",
            "1.1341368374807057\n",
            "Epoch: 823 \tTraining Loss: 1.134137 \tValidation Loss: 0.026842\n",
            "train_loss:\n",
            "1.1338771957180875\n",
            "Epoch: 824 \tTraining Loss: 1.133877 \tValidation Loss: 0.026846\n",
            "train_loss:\n",
            "1.1332875385389223\n",
            "Epoch: 825 \tTraining Loss: 1.133288 \tValidation Loss: 0.026805\n",
            "train_loss:\n",
            "1.1326290858097565\n",
            "Epoch: 826 \tTraining Loss: 1.132629 \tValidation Loss: 0.026760\n",
            "train_loss:\n",
            "1.1325560275451605\n",
            "Epoch: 827 \tTraining Loss: 1.132556 \tValidation Loss: 0.026743\n",
            "train_loss:\n",
            "1.132012514404325\n",
            "Epoch: 828 \tTraining Loss: 1.132013 \tValidation Loss: 0.026696\n",
            "Validation loss decreased (0.026741 --> 0.026696).  Saving model ...\n",
            "train_loss:\n",
            "1.1335212297928638\n",
            "Epoch: 829 \tTraining Loss: 1.133521 \tValidation Loss: 0.026782\n",
            "train_loss:\n",
            "1.1327234148542522\n",
            "Epoch: 830 \tTraining Loss: 1.132723 \tValidation Loss: 0.026830\n",
            "train_loss:\n",
            "1.1315057627010694\n",
            "Epoch: 831 \tTraining Loss: 1.131506 \tValidation Loss: 0.026727\n",
            "train_loss:\n",
            "1.133617042185186\n",
            "Epoch: 832 \tTraining Loss: 1.133617 \tValidation Loss: 0.026850\n",
            "train_loss:\n",
            "1.1321532551622215\n",
            "Epoch: 833 \tTraining Loss: 1.132153 \tValidation Loss: 0.026745\n",
            "train_loss:\n",
            "1.1334651132206341\n",
            "Epoch: 834 \tTraining Loss: 1.133465 \tValidation Loss: 0.026832\n",
            "train_loss:\n",
            "1.1314059085461683\n",
            "Epoch: 835 \tTraining Loss: 1.131406 \tValidation Loss: 0.026695\n",
            "Validation loss decreased (0.026696 --> 0.026695).  Saving model ...\n",
            "train_loss:\n",
            "1.1321433428879624\n",
            "Epoch: 836 \tTraining Loss: 1.132143 \tValidation Loss: 0.026728\n",
            "train_loss:\n",
            "1.1330583899012416\n",
            "Epoch: 837 \tTraining Loss: 1.133058 \tValidation Loss: 0.026802\n",
            "train_loss:\n",
            "1.1306060146499466\n",
            "Epoch: 838 \tTraining Loss: 1.130606 \tValidation Loss: 0.026657\n",
            "Validation loss decreased (0.026695 --> 0.026657).  Saving model ...\n",
            "train_loss:\n",
            "1.1325438040080087\n",
            "Epoch: 839 \tTraining Loss: 1.132544 \tValidation Loss: 0.026766\n",
            "train_loss:\n",
            "1.131319564340752\n",
            "Epoch: 840 \tTraining Loss: 1.131320 \tValidation Loss: 0.026663\n",
            "train_loss:\n",
            "1.134967742385445\n",
            "Epoch: 841 \tTraining Loss: 1.134968 \tValidation Loss: 0.026728\n",
            "train_loss:\n",
            "1.1334901790479164\n",
            "Epoch: 842 \tTraining Loss: 1.133490 \tValidation Loss: 0.026896\n",
            "train_loss:\n",
            "1.1319587440281125\n",
            "Epoch: 843 \tTraining Loss: 1.131959 \tValidation Loss: 0.026846\n",
            "train_loss:\n",
            "1.132693194207691\n",
            "Epoch: 844 \tTraining Loss: 1.132693 \tValidation Loss: 0.026883\n",
            "train_loss:\n",
            "1.1320313660653083\n",
            "Epoch: 845 \tTraining Loss: 1.132031 \tValidation Loss: 0.026835\n",
            "train_loss:\n",
            "1.130946621790037\n",
            "Epoch: 846 \tTraining Loss: 1.130947 \tValidation Loss: 0.026756\n",
            "train_loss:\n",
            "1.1311621561155214\n",
            "Epoch: 847 \tTraining Loss: 1.131162 \tValidation Loss: 0.026763\n",
            "train_loss:\n",
            "1.131260437406463\n",
            "Epoch: 848 \tTraining Loss: 1.131260 \tValidation Loss: 0.026736\n",
            "train_loss:\n",
            "1.1337626163776104\n",
            "Epoch: 849 \tTraining Loss: 1.133763 \tValidation Loss: 0.026838\n",
            "train_loss:\n",
            "1.1324879651541238\n",
            "Epoch: 850 \tTraining Loss: 1.132488 \tValidation Loss: 0.026872\n",
            "train_loss:\n",
            "1.1317340681404422\n",
            "Epoch: 851 \tTraining Loss: 1.131734 \tValidation Loss: 0.026831\n",
            "train_loss:\n",
            "1.1319378404826907\n",
            "Epoch: 852 \tTraining Loss: 1.131938 \tValidation Loss: 0.026853\n",
            "train_loss:\n",
            "1.131726214737246\n",
            "Epoch: 853 \tTraining Loss: 1.131726 \tValidation Loss: 0.026828\n",
            "train_loss:\n",
            "1.132331987877032\n",
            "Epoch: 854 \tTraining Loss: 1.132332 \tValidation Loss: 0.026865\n",
            "train_loss:\n",
            "1.1319306184957316\n",
            "Epoch: 855 \tTraining Loss: 1.131931 \tValidation Loss: 0.026852\n",
            "train_loss:\n",
            "1.1306125288044577\n",
            "Epoch: 856 \tTraining Loss: 1.130613 \tValidation Loss: 0.026764\n",
            "train_loss:\n",
            "1.131048628262111\n",
            "Epoch: 857 \tTraining Loss: 1.131049 \tValidation Loss: 0.026765\n",
            "train_loss:\n",
            "1.1300814968325716\n",
            "Epoch: 858 \tTraining Loss: 1.130081 \tValidation Loss: 0.026681\n",
            "train_loss:\n",
            "1.1305010668087354\n",
            "Epoch: 859 \tTraining Loss: 1.130501 \tValidation Loss: 0.026683\n",
            "train_loss:\n",
            "1.131303310394287\n",
            "Epoch: 860 \tTraining Loss: 1.131303 \tValidation Loss: 0.026737\n",
            "train_loss:\n",
            "1.1312538931221316\n",
            "Epoch: 861 \tTraining Loss: 1.131254 \tValidation Loss: 0.026760\n",
            "train_loss:\n",
            "1.1301101481084859\n",
            "Epoch: 862 \tTraining Loss: 1.130110 \tValidation Loss: 0.026666\n",
            "train_loss:\n",
            "1.1313134728770553\n",
            "Epoch: 863 \tTraining Loss: 1.131313 \tValidation Loss: 0.026742\n",
            "train_loss:\n",
            "1.1316042557740822\n",
            "Epoch: 864 \tTraining Loss: 1.131604 \tValidation Loss: 0.026801\n",
            "train_loss:\n",
            "1.1299352584741054\n",
            "Epoch: 865 \tTraining Loss: 1.129935 \tValidation Loss: 0.026704\n",
            "train_loss:\n",
            "1.1296818339344346\n",
            "Epoch: 866 \tTraining Loss: 1.129682 \tValidation Loss: 0.026631\n",
            "Validation loss decreased (0.026657 --> 0.026631).  Saving model ...\n",
            "train_loss:\n",
            "1.1306583632479656\n",
            "Epoch: 867 \tTraining Loss: 1.130658 \tValidation Loss: 0.026708\n",
            "train_loss:\n",
            "1.1295663830124851\n",
            "Epoch: 868 \tTraining Loss: 1.129566 \tValidation Loss: 0.026604\n",
            "Validation loss decreased (0.026631 --> 0.026604).  Saving model ...\n",
            "train_loss:\n",
            "1.1321159566278423\n",
            "Epoch: 869 \tTraining Loss: 1.132116 \tValidation Loss: 0.026744\n",
            "train_loss:\n",
            "1.1307049443870236\n",
            "Epoch: 870 \tTraining Loss: 1.130705 \tValidation Loss: 0.026742\n",
            "train_loss:\n",
            "1.1308564563373944\n",
            "Epoch: 871 \tTraining Loss: 1.130856 \tValidation Loss: 0.026766\n",
            "train_loss:\n",
            "1.1309630879552373\n",
            "Epoch: 872 \tTraining Loss: 1.130963 \tValidation Loss: 0.026749\n",
            "train_loss:\n",
            "1.130430831140651\n",
            "Epoch: 873 \tTraining Loss: 1.130431 \tValidation Loss: 0.026735\n",
            "train_loss:\n",
            "1.1293635809377873\n",
            "Epoch: 874 \tTraining Loss: 1.129364 \tValidation Loss: 0.026658\n",
            "train_loss:\n",
            "1.129764688320649\n",
            "Epoch: 875 \tTraining Loss: 1.129765 \tValidation Loss: 0.026640\n",
            "train_loss:\n",
            "1.1293246567030966\n",
            "Epoch: 876 \tTraining Loss: 1.129325 \tValidation Loss: 0.026605\n",
            "train_loss:\n",
            "1.129539837767353\n",
            "Epoch: 877 \tTraining Loss: 1.129540 \tValidation Loss: 0.026580\n",
            "Validation loss decreased (0.026604 --> 0.026580).  Saving model ...\n",
            "train_loss:\n",
            "1.131452046034537\n",
            "Epoch: 878 \tTraining Loss: 1.131452 \tValidation Loss: 0.026681\n",
            "train_loss:\n",
            "1.131506736461933\n",
            "Epoch: 879 \tTraining Loss: 1.131507 \tValidation Loss: 0.026780\n",
            "train_loss:\n",
            "1.1303130711391296\n",
            "Epoch: 880 \tTraining Loss: 1.130313 \tValidation Loss: 0.026750\n",
            "train_loss:\n",
            "1.1303285858133336\n",
            "Epoch: 881 \tTraining Loss: 1.130329 \tValidation Loss: 0.026720\n",
            "train_loss:\n",
            "1.1299124209435432\n",
            "Epoch: 882 \tTraining Loss: 1.129912 \tValidation Loss: 0.026709\n",
            "train_loss:\n",
            "1.1290647363488053\n",
            "Epoch: 883 \tTraining Loss: 1.129065 \tValidation Loss: 0.026641\n",
            "train_loss:\n",
            "1.1293068425559298\n",
            "Epoch: 884 \tTraining Loss: 1.129307 \tValidation Loss: 0.026616\n",
            "train_loss:\n",
            "1.128090425288721\n",
            "Epoch: 885 \tTraining Loss: 1.128090 \tValidation Loss: 0.026532\n",
            "Validation loss decreased (0.026580 --> 0.026532).  Saving model ...\n",
            "train_loss:\n",
            "1.129624910843678\n",
            "Epoch: 886 \tTraining Loss: 1.129625 \tValidation Loss: 0.026612\n",
            "train_loss:\n",
            "1.128367351524996\n",
            "Epoch: 887 \tTraining Loss: 1.128367 \tValidation Loss: 0.026491\n",
            "Validation loss decreased (0.026532 --> 0.026491).  Saving model ...\n",
            "train_loss:\n",
            "1.1316506993639601\n",
            "Epoch: 888 \tTraining Loss: 1.131651 \tValidation Loss: 0.026688\n",
            "train_loss:\n",
            "1.1300301656618224\n",
            "Epoch: 889 \tTraining Loss: 1.130030 \tValidation Loss: 0.026664\n",
            "train_loss:\n",
            "1.1307858224316831\n",
            "Epoch: 890 \tTraining Loss: 1.130786 \tValidation Loss: 0.026704\n",
            "train_loss:\n",
            "1.1297747455673777\n",
            "Epoch: 891 \tTraining Loss: 1.129775 \tValidation Loss: 0.026689\n",
            "train_loss:\n",
            "1.1282849648059943\n",
            "Epoch: 892 \tTraining Loss: 1.128285 \tValidation Loss: 0.026593\n",
            "train_loss:\n",
            "1.1283164954447484\n",
            "Epoch: 893 \tTraining Loss: 1.128316 \tValidation Loss: 0.026561\n",
            "train_loss:\n",
            "1.1283355073614434\n",
            "Epoch: 894 \tTraining Loss: 1.128336 \tValidation Loss: 0.026561\n",
            "train_loss:\n",
            "1.1282678183181818\n",
            "Epoch: 895 \tTraining Loss: 1.128268 \tValidation Loss: 0.026521\n",
            "train_loss:\n",
            "1.1315469938320117\n",
            "Epoch: 896 \tTraining Loss: 1.131547 \tValidation Loss: 0.026644\n",
            "train_loss:\n",
            "1.1292569209367802\n",
            "Epoch: 897 \tTraining Loss: 1.129257 \tValidation Loss: 0.026666\n",
            "train_loss:\n",
            "1.1294335209843003\n",
            "Epoch: 898 \tTraining Loss: 1.129434 \tValidation Loss: 0.026657\n",
            "train_loss:\n",
            "1.1302777961060242\n",
            "Epoch: 899 \tTraining Loss: 1.130278 \tValidation Loss: 0.026710\n",
            "train_loss:\n",
            "1.128933759399386\n",
            "Epoch: 900 \tTraining Loss: 1.128934 \tValidation Loss: 0.026669\n",
            "train_loss:\n",
            "1.129408760821863\n",
            "Epoch: 901 \tTraining Loss: 1.129409 \tValidation Loss: 0.026645\n",
            "train_loss:\n",
            "1.1285633150037828\n",
            "Epoch: 902 \tTraining Loss: 1.128563 \tValidation Loss: 0.026599\n",
            "train_loss:\n",
            "1.1290021243986192\n",
            "Epoch: 903 \tTraining Loss: 1.129002 \tValidation Loss: 0.026591\n",
            "train_loss:\n",
            "1.129776418864072\n",
            "Epoch: 904 \tTraining Loss: 1.129776 \tValidation Loss: 0.026632\n",
            "train_loss:\n",
            "1.12727151903914\n",
            "Epoch: 905 \tTraining Loss: 1.127272 \tValidation Loss: 0.026480\n",
            "Validation loss decreased (0.026491 --> 0.026480).  Saving model ...\n",
            "train_loss:\n",
            "1.1283026396573246\n",
            "Epoch: 906 \tTraining Loss: 1.128303 \tValidation Loss: 0.026460\n",
            "Validation loss decreased (0.026480 --> 0.026460).  Saving model ...\n",
            "train_loss:\n",
            "1.131965586990664\n",
            "Epoch: 907 \tTraining Loss: 1.131966 \tValidation Loss: 0.026558\n",
            "train_loss:\n",
            "1.1278575383699858\n",
            "Epoch: 908 \tTraining Loss: 1.127858 \tValidation Loss: 0.026549\n",
            "train_loss:\n",
            "1.1279957857760754\n",
            "Epoch: 909 \tTraining Loss: 1.127996 \tValidation Loss: 0.026497\n",
            "train_loss:\n",
            "1.1285960486520341\n",
            "Epoch: 910 \tTraining Loss: 1.128596 \tValidation Loss: 0.026540\n",
            "train_loss:\n",
            "1.129235273315793\n",
            "Epoch: 911 \tTraining Loss: 1.129235 \tValidation Loss: 0.026574\n",
            "train_loss:\n",
            "1.1273001122387338\n",
            "Epoch: 912 \tTraining Loss: 1.127300 \tValidation Loss: 0.026482\n",
            "train_loss:\n",
            "1.1309679253197416\n",
            "Epoch: 913 \tTraining Loss: 1.130968 \tValidation Loss: 0.026593\n",
            "train_loss:\n",
            "1.1284098219085525\n",
            "Epoch: 914 \tTraining Loss: 1.128410 \tValidation Loss: 0.026571\n",
            "train_loss:\n",
            "1.1298793379640404\n",
            "Epoch: 915 \tTraining Loss: 1.129879 \tValidation Loss: 0.026652\n",
            "train_loss:\n",
            "1.1309027951278967\n",
            "Epoch: 916 \tTraining Loss: 1.130903 \tValidation Loss: 0.026720\n",
            "train_loss:\n",
            "1.1288529333177504\n",
            "Epoch: 917 \tTraining Loss: 1.128853 \tValidation Loss: 0.026667\n",
            "train_loss:\n",
            "1.129064678709149\n",
            "Epoch: 918 \tTraining Loss: 1.129065 \tValidation Loss: 0.026656\n",
            "train_loss:\n",
            "1.1276895157146802\n",
            "Epoch: 919 \tTraining Loss: 1.127690 \tValidation Loss: 0.026573\n",
            "train_loss:\n",
            "1.1273692208768684\n",
            "Epoch: 920 \tTraining Loss: 1.127369 \tValidation Loss: 0.026535\n",
            "train_loss:\n",
            "1.1274427791218182\n",
            "Epoch: 921 \tTraining Loss: 1.127443 \tValidation Loss: 0.026515\n",
            "train_loss:\n",
            "1.1267213511379648\n",
            "Epoch: 922 \tTraining Loss: 1.126721 \tValidation Loss: 0.026453\n",
            "Validation loss decreased (0.026460 --> 0.026453).  Saving model ...\n",
            "train_loss:\n",
            "1.127310044162876\n",
            "Epoch: 923 \tTraining Loss: 1.127310 \tValidation Loss: 0.026450\n",
            "Validation loss decreased (0.026453 --> 0.026450).  Saving model ...\n",
            "train_loss:\n",
            "1.1278969590917176\n",
            "Epoch: 924 \tTraining Loss: 1.127897 \tValidation Loss: 0.026520\n",
            "train_loss:\n",
            "1.125812047348791\n",
            "Epoch: 925 \tTraining Loss: 1.125812 \tValidation Loss: 0.026356\n",
            "Validation loss decreased (0.026450 --> 0.026356).  Saving model ...\n",
            "train_loss:\n",
            "1.1274405770249418\n",
            "Epoch: 926 \tTraining Loss: 1.127441 \tValidation Loss: 0.026459\n",
            "train_loss:\n",
            "1.1257059685476534\n",
            "Epoch: 927 \tTraining Loss: 1.125706 \tValidation Loss: 0.026333\n",
            "Validation loss decreased (0.026356 --> 0.026333).  Saving model ...\n",
            "train_loss:\n",
            "1.12893323016254\n",
            "Epoch: 928 \tTraining Loss: 1.128933 \tValidation Loss: 0.026520\n",
            "train_loss:\n",
            "1.1278177338205415\n",
            "Epoch: 929 \tTraining Loss: 1.127818 \tValidation Loss: 0.026503\n",
            "train_loss:\n",
            "1.1291217153325623\n",
            "Epoch: 930 \tTraining Loss: 1.129122 \tValidation Loss: 0.026565\n",
            "train_loss:\n",
            "1.1290041072901351\n",
            "Epoch: 931 \tTraining Loss: 1.129004 \tValidation Loss: 0.026620\n",
            "train_loss:\n",
            "1.1269306219541109\n",
            "Epoch: 932 \tTraining Loss: 1.126931 \tValidation Loss: 0.026518\n",
            "train_loss:\n",
            "1.1266126667623555\n",
            "Epoch: 933 \tTraining Loss: 1.126613 \tValidation Loss: 0.026413\n",
            "train_loss:\n",
            "1.130502230955131\n",
            "Epoch: 934 \tTraining Loss: 1.130502 \tValidation Loss: 0.026576\n",
            "train_loss:\n",
            "1.1279131925149715\n",
            "Epoch: 935 \tTraining Loss: 1.127913 \tValidation Loss: 0.026565\n",
            "train_loss:\n",
            "1.1272359020980722\n",
            "Epoch: 936 \tTraining Loss: 1.127236 \tValidation Loss: 0.026547\n",
            "train_loss:\n",
            "1.1268331794948367\n",
            "Epoch: 937 \tTraining Loss: 1.126833 \tValidation Loss: 0.026496\n",
            "train_loss:\n",
            "1.1267775904127966\n",
            "Epoch: 938 \tTraining Loss: 1.126778 \tValidation Loss: 0.026464\n",
            "train_loss:\n",
            "1.1285238475589963\n",
            "Epoch: 939 \tTraining Loss: 1.128524 \tValidation Loss: 0.026532\n",
            "train_loss:\n",
            "1.1262523119266217\n",
            "Epoch: 940 \tTraining Loss: 1.126252 \tValidation Loss: 0.026472\n",
            "train_loss:\n",
            "1.1259043085706102\n",
            "Epoch: 941 \tTraining Loss: 1.125904 \tValidation Loss: 0.026399\n",
            "train_loss:\n",
            "1.1257431522393837\n",
            "Epoch: 942 \tTraining Loss: 1.125743 \tValidation Loss: 0.026324\n",
            "Validation loss decreased (0.026333 --> 0.026324).  Saving model ...\n",
            "train_loss:\n",
            "1.1257521684789833\n",
            "Epoch: 943 \tTraining Loss: 1.125752 \tValidation Loss: 0.026354\n",
            "train_loss:\n",
            "1.1251669421300783\n",
            "Epoch: 944 \tTraining Loss: 1.125167 \tValidation Loss: 0.026261\n",
            "Validation loss decreased (0.026324 --> 0.026261).  Saving model ...\n",
            "train_loss:\n",
            "1.1282046544246185\n",
            "Epoch: 945 \tTraining Loss: 1.128205 \tValidation Loss: 0.026406\n",
            "train_loss:\n",
            "1.127352849031106\n",
            "Epoch: 946 \tTraining Loss: 1.127353 \tValidation Loss: 0.026497\n",
            "train_loss:\n",
            "1.1262683619509686\n",
            "Epoch: 947 \tTraining Loss: 1.126268 \tValidation Loss: 0.026423\n",
            "train_loss:\n",
            "1.1260591321812445\n",
            "Epoch: 948 \tTraining Loss: 1.126059 \tValidation Loss: 0.026426\n",
            "train_loss:\n",
            "1.125358753806942\n",
            "Epoch: 949 \tTraining Loss: 1.125359 \tValidation Loss: 0.026356\n",
            "train_loss:\n",
            "1.127948100750263\n",
            "Epoch: 950 \tTraining Loss: 1.127948 \tValidation Loss: 0.026455\n",
            "train_loss:\n",
            "1.1255937052733733\n",
            "Epoch: 951 \tTraining Loss: 1.125594 \tValidation Loss: 0.026350\n",
            "train_loss:\n",
            "1.1247316500642797\n",
            "Epoch: 952 \tTraining Loss: 1.124732 \tValidation Loss: 0.026275\n",
            "train_loss:\n",
            "1.126266815941849\n",
            "Epoch: 953 \tTraining Loss: 1.126267 \tValidation Loss: 0.026336\n",
            "train_loss:\n",
            "1.1264151557461246\n",
            "Epoch: 954 \tTraining Loss: 1.126415 \tValidation Loss: 0.026384\n",
            "train_loss:\n",
            "1.1249693361394135\n",
            "Epoch: 955 \tTraining Loss: 1.124969 \tValidation Loss: 0.026245\n",
            "Validation loss decreased (0.026261 --> 0.026245).  Saving model ...\n",
            "train_loss:\n",
            "1.1246530090496216\n",
            "Epoch: 956 \tTraining Loss: 1.124653 \tValidation Loss: 0.026157\n",
            "Validation loss decreased (0.026245 --> 0.026157).  Saving model ...\n",
            "train_loss:\n",
            "1.1257269456709698\n",
            "Epoch: 957 \tTraining Loss: 1.125727 \tValidation Loss: 0.026225\n",
            "train_loss:\n",
            "1.12778882927947\n",
            "Epoch: 958 \tTraining Loss: 1.127789 \tValidation Loss: 0.026292\n",
            "train_loss:\n",
            "1.1253554002269284\n",
            "Epoch: 959 \tTraining Loss: 1.125355 \tValidation Loss: 0.026270\n",
            "train_loss:\n",
            "1.1306911265893733\n",
            "Epoch: 960 \tTraining Loss: 1.130691 \tValidation Loss: 0.026457\n",
            "train_loss:\n",
            "1.1269520136899565\n",
            "Epoch: 961 \tTraining Loss: 1.126952 \tValidation Loss: 0.026403\n",
            "train_loss:\n",
            "1.1281678798871162\n",
            "Epoch: 962 \tTraining Loss: 1.128168 \tValidation Loss: 0.026509\n",
            "train_loss:\n",
            "1.125218731361431\n",
            "Epoch: 963 \tTraining Loss: 1.125219 \tValidation Loss: 0.026378\n",
            "train_loss:\n",
            "1.1292785423579232\n",
            "Epoch: 964 \tTraining Loss: 1.129279 \tValidation Loss: 0.026504\n",
            "train_loss:\n",
            "1.128110639778249\n",
            "Epoch: 965 \tTraining Loss: 1.128111 \tValidation Loss: 0.026512\n",
            "train_loss:\n",
            "1.1262934972078373\n",
            "Epoch: 966 \tTraining Loss: 1.126293 \tValidation Loss: 0.026459\n",
            "train_loss:\n",
            "1.1266123886073465\n",
            "Epoch: 967 \tTraining Loss: 1.126612 \tValidation Loss: 0.026459\n",
            "train_loss:\n",
            "1.1262743791817746\n",
            "Epoch: 968 \tTraining Loss: 1.126274 \tValidation Loss: 0.026412\n",
            "train_loss:\n",
            "1.1269905750568097\n",
            "Epoch: 969 \tTraining Loss: 1.126991 \tValidation Loss: 0.026426\n",
            "train_loss:\n",
            "1.1274146016264137\n",
            "Epoch: 970 \tTraining Loss: 1.127415 \tValidation Loss: 0.026433\n",
            "train_loss:\n",
            "1.124680227630741\n",
            "Epoch: 971 \tTraining Loss: 1.124680 \tValidation Loss: 0.026340\n",
            "train_loss:\n",
            "1.125730111703768\n",
            "Epoch: 972 \tTraining Loss: 1.125730 \tValidation Loss: 0.026321\n",
            "train_loss:\n",
            "1.1256924917846372\n",
            "Epoch: 973 \tTraining Loss: 1.125692 \tValidation Loss: 0.026345\n",
            "train_loss:\n",
            "1.1268447853706696\n",
            "Epoch: 974 \tTraining Loss: 1.126845 \tValidation Loss: 0.026353\n",
            "train_loss:\n",
            "1.1246436054889972\n",
            "Epoch: 975 \tTraining Loss: 1.124644 \tValidation Loss: 0.026281\n",
            "train_loss:\n",
            "1.127499948054443\n",
            "Epoch: 976 \tTraining Loss: 1.127500 \tValidation Loss: 0.026369\n",
            "train_loss:\n",
            "1.125769466052562\n",
            "Epoch: 977 \tTraining Loss: 1.125769 \tValidation Loss: 0.026338\n",
            "train_loss:\n",
            "1.1267354363923545\n",
            "Epoch: 978 \tTraining Loss: 1.126735 \tValidation Loss: 0.026306\n",
            "train_loss:\n",
            "1.1266944864730695\n",
            "Epoch: 979 \tTraining Loss: 1.126694 \tValidation Loss: 0.026285\n",
            "train_loss:\n",
            "1.1272300732878102\n",
            "Epoch: 980 \tTraining Loss: 1.127230 \tValidation Loss: 0.026376\n",
            "train_loss:\n",
            "1.1249893822949448\n",
            "Epoch: 981 \tTraining Loss: 1.124989 \tValidation Loss: 0.026324\n",
            "train_loss:\n",
            "1.124020734767774\n",
            "Epoch: 982 \tTraining Loss: 1.124021 \tValidation Loss: 0.026230\n",
            "train_loss:\n",
            "1.1237148531190642\n",
            "Epoch: 983 \tTraining Loss: 1.123715 \tValidation Loss: 0.026150\n",
            "Validation loss decreased (0.026157 --> 0.026150).  Saving model ...\n",
            "train_loss:\n",
            "1.1244721986871935\n",
            "Epoch: 984 \tTraining Loss: 1.124472 \tValidation Loss: 0.026102\n",
            "Validation loss decreased (0.026150 --> 0.026102).  Saving model ...\n",
            "train_loss:\n",
            "1.1268271528757536\n",
            "Epoch: 985 \tTraining Loss: 1.126827 \tValidation Loss: 0.026295\n",
            "train_loss:\n",
            "1.1250488655471103\n",
            "Epoch: 986 \tTraining Loss: 1.125049 \tValidation Loss: 0.026264\n",
            "train_loss:\n",
            "1.126679376804785\n",
            "Epoch: 987 \tTraining Loss: 1.126679 \tValidation Loss: 0.026344\n",
            "train_loss:\n",
            "1.1251926221253672\n",
            "Epoch: 988 \tTraining Loss: 1.125193 \tValidation Loss: 0.026318\n",
            "train_loss:\n",
            "1.1247426513787155\n",
            "Epoch: 989 \tTraining Loss: 1.124743 \tValidation Loss: 0.026289\n",
            "train_loss:\n",
            "1.1236260542066105\n",
            "Epoch: 990 \tTraining Loss: 1.123626 \tValidation Loss: 0.026154\n",
            "train_loss:\n",
            "1.1255974715009278\n",
            "Epoch: 991 \tTraining Loss: 1.125597 \tValidation Loss: 0.026242\n",
            "train_loss:\n",
            "1.1228798503404136\n",
            "Epoch: 992 \tTraining Loss: 1.122880 \tValidation Loss: 0.026116\n",
            "train_loss:\n",
            "1.123231013179262\n",
            "Epoch: 993 \tTraining Loss: 1.123231 \tValidation Loss: 0.026051\n",
            "Validation loss decreased (0.026102 --> 0.026051).  Saving model ...\n",
            "train_loss:\n",
            "1.1244315627293708\n",
            "Epoch: 994 \tTraining Loss: 1.124432 \tValidation Loss: 0.026138\n",
            "train_loss:\n",
            "1.1224700653509343\n",
            "Epoch: 995 \tTraining Loss: 1.122470 \tValidation Loss: 0.026023\n",
            "Validation loss decreased (0.026051 --> 0.026023).  Saving model ...\n",
            "train_loss:\n",
            "1.1233384358140575\n",
            "Epoch: 996 \tTraining Loss: 1.123338 \tValidation Loss: 0.026035\n",
            "train_loss:\n",
            "1.121449351092398\n",
            "Epoch: 997 \tTraining Loss: 1.121449 \tValidation Loss: 0.025930\n",
            "Validation loss decreased (0.026023 --> 0.025930).  Saving model ...\n",
            "train_loss:\n",
            "1.1245806767390325\n",
            "Epoch: 998 \tTraining Loss: 1.124581 \tValidation Loss: 0.026076\n",
            "train_loss:\n",
            "1.1224826991339742\n",
            "Epoch: 999 \tTraining Loss: 1.122483 \tValidation Loss: 0.025962\n",
            "---------------------------------------------\n",
            "Test Loss: 1.300576\n",
            "\n",
            "---------------------------------------------\n",
            "Learning rate: 5\n",
            "train_loss:\n",
            "4.2927480687151895\n",
            "Epoch: 0 \tTraining Loss: 4.292748 \tValidation Loss: 0.060486\n",
            "Validation loss decreased (inf --> 0.060486).  Saving model ...\n",
            "train_loss:\n",
            "2.3087456894444895\n",
            "Epoch: 1 \tTraining Loss: 2.308746 \tValidation Loss: 0.060469\n",
            "Validation loss decreased (0.060486 --> 0.060469).  Saving model ...\n",
            "train_loss:\n",
            "2.3090404996941816\n",
            "Epoch: 2 \tTraining Loss: 2.309040 \tValidation Loss: 0.060532\n",
            "train_loss:\n",
            "2.3091748363369113\n",
            "Epoch: 3 \tTraining Loss: 2.309175 \tValidation Loss: 0.060460\n",
            "Validation loss decreased (0.060469 --> 0.060460).  Saving model ...\n",
            "train_loss:\n",
            "2.310377141494891\n",
            "Epoch: 4 \tTraining Loss: 2.310377 \tValidation Loss: 0.060554\n",
            "train_loss:\n",
            "2.30877362153469\n",
            "Epoch: 5 \tTraining Loss: 2.308774 \tValidation Loss: 0.060530\n",
            "train_loss:\n",
            "2.330169751967266\n",
            "Epoch: 6 \tTraining Loss: 2.330170 \tValidation Loss: 0.060766\n",
            "train_loss:\n",
            "2.3087332986650013\n",
            "Epoch: 7 \tTraining Loss: 2.308733 \tValidation Loss: 0.060615\n",
            "train_loss:\n",
            "2.3091051368922977\n",
            "Epoch: 8 \tTraining Loss: 2.309105 \tValidation Loss: 0.060610\n",
            "train_loss:\n",
            "2.3087371784252126\n",
            "Epoch: 9 \tTraining Loss: 2.308737 \tValidation Loss: 0.060647\n",
            "train_loss:\n",
            "2.309661397130498\n",
            "Epoch: 10 \tTraining Loss: 2.309661 \tValidation Loss: 0.060607\n",
            "train_loss:\n",
            "2.308443934926183\n",
            "Epoch: 11 \tTraining Loss: 2.308444 \tValidation Loss: 0.060674\n",
            "train_loss:\n",
            "2.3163629464614086\n",
            "Epoch: 12 \tTraining Loss: 2.316363 \tValidation Loss: 0.060728\n",
            "train_loss:\n",
            "2.3083996646133533\n",
            "Epoch: 13 \tTraining Loss: 2.308400 \tValidation Loss: 0.060503\n",
            "train_loss:\n",
            "2.3087538284259836\n",
            "Epoch: 14 \tTraining Loss: 2.308754 \tValidation Loss: 0.060459\n",
            "Validation loss decreased (0.060460 --> 0.060459).  Saving model ...\n",
            "train_loss:\n",
            "2.310478533580626\n",
            "Epoch: 15 \tTraining Loss: 2.310479 \tValidation Loss: 0.060602\n",
            "train_loss:\n",
            "2.3092417254116073\n",
            "Epoch: 16 \tTraining Loss: 2.309242 \tValidation Loss: 0.060596\n",
            "train_loss:\n",
            "2.309146788530734\n",
            "Epoch: 17 \tTraining Loss: 2.309147 \tValidation Loss: 0.060583\n",
            "train_loss:\n",
            "2.308897947653746\n",
            "Epoch: 18 \tTraining Loss: 2.308898 \tValidation Loss: 0.060542\n",
            "train_loss:\n",
            "2.3082263059231822\n",
            "Epoch: 19 \tTraining Loss: 2.308226 \tValidation Loss: 0.060433\n",
            "Validation loss decreased (0.060459 --> 0.060433).  Saving model ...\n",
            "train_loss:\n",
            "2.31035048359043\n",
            "Epoch: 20 \tTraining Loss: 2.310350 \tValidation Loss: 0.060555\n",
            "train_loss:\n",
            "2.3251837627354996\n",
            "Epoch: 21 \tTraining Loss: 2.325184 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3089695902534455\n",
            "Epoch: 22 \tTraining Loss: 2.308970 \tValidation Loss: 0.060566\n",
            "train_loss:\n",
            "2.31526945798825\n",
            "Epoch: 23 \tTraining Loss: 2.315269 \tValidation Loss: 0.060617\n",
            "train_loss:\n",
            "2.309392986280141\n",
            "Epoch: 24 \tTraining Loss: 2.309393 \tValidation Loss: 0.060614\n",
            "train_loss:\n",
            "2.3093996353638477\n",
            "Epoch: 25 \tTraining Loss: 2.309400 \tValidation Loss: 0.060613\n",
            "train_loss:\n",
            "2.3093946141637725\n",
            "Epoch: 26 \tTraining Loss: 2.309395 \tValidation Loss: 0.060613\n",
            "train_loss:\n",
            "2.30938876810528\n",
            "Epoch: 27 \tTraining Loss: 2.309389 \tValidation Loss: 0.060613\n",
            "train_loss:\n",
            "2.3093817168539696\n",
            "Epoch: 28 \tTraining Loss: 2.309382 \tValidation Loss: 0.060613\n",
            "train_loss:\n",
            "2.309372651271331\n",
            "Epoch: 29 \tTraining Loss: 2.309373 \tValidation Loss: 0.060613\n",
            "train_loss:\n",
            "2.309360525110266\n",
            "Epoch: 30 \tTraining Loss: 2.309361 \tValidation Loss: 0.060612\n",
            "train_loss:\n",
            "2.3093427327963023\n",
            "Epoch: 31 \tTraining Loss: 2.309343 \tValidation Loss: 0.060612\n",
            "train_loss:\n",
            "2.3093131383260093\n",
            "Epoch: 32 \tTraining Loss: 2.309313 \tValidation Loss: 0.060612\n",
            "train_loss:\n",
            "2.3092532192831072\n",
            "Epoch: 33 \tTraining Loss: 2.309253 \tValidation Loss: 0.060610\n",
            "train_loss:\n",
            "2.309059562263908\n",
            "Epoch: 34 \tTraining Loss: 2.309060 \tValidation Loss: 0.060600\n",
            "train_loss:\n",
            "2.309219185685937\n",
            "Epoch: 35 \tTraining Loss: 2.309219 \tValidation Loss: 0.060596\n",
            "train_loss:\n",
            "2.309924605128529\n",
            "Epoch: 36 \tTraining Loss: 2.309925 \tValidation Loss: 0.060611\n",
            "train_loss:\n",
            "2.3094182307030255\n",
            "Epoch: 37 \tTraining Loss: 2.309418 \tValidation Loss: 0.060611\n",
            "train_loss:\n",
            "2.309417601906773\n",
            "Epoch: 38 \tTraining Loss: 2.309418 \tValidation Loss: 0.060611\n",
            "train_loss:\n",
            "2.309416888397692\n",
            "Epoch: 39 \tTraining Loss: 2.309417 \tValidation Loss: 0.060611\n",
            "train_loss:\n",
            "2.3094163089444786\n",
            "Epoch: 40 \tTraining Loss: 2.309416 \tValidation Loss: 0.060610\n",
            "train_loss:\n",
            "2.3094156766549134\n",
            "Epoch: 41 \tTraining Loss: 2.309416 \tValidation Loss: 0.060610\n",
            "train_loss:\n",
            "2.309415116851583\n",
            "Epoch: 42 \tTraining Loss: 2.309415 \tValidation Loss: 0.060610\n",
            "train_loss:\n",
            "2.309414510325198\n",
            "Epoch: 43 \tTraining Loss: 2.309415 \tValidation Loss: 0.060610\n",
            "train_loss:\n",
            "2.309413982398344\n",
            "Epoch: 44 \tTraining Loss: 2.309414 \tValidation Loss: 0.060610\n",
            "train_loss:\n",
            "2.3094134623314435\n",
            "Epoch: 45 \tTraining Loss: 2.309413 \tValidation Loss: 0.060610\n",
            "train_loss:\n",
            "2.30941296235109\n",
            "Epoch: 46 \tTraining Loss: 2.309413 \tValidation Loss: 0.060610\n",
            "train_loss:\n",
            "2.3094124907539006\n",
            "Epoch: 47 \tTraining Loss: 2.309412 \tValidation Loss: 0.060610\n",
            "train_loss:\n",
            "2.309412037059938\n",
            "Epoch: 48 \tTraining Loss: 2.309412 \tValidation Loss: 0.060610\n",
            "train_loss:\n",
            "2.309411597339225\n",
            "Epoch: 49 \tTraining Loss: 2.309412 \tValidation Loss: 0.060610\n",
            "train_loss:\n",
            "2.3094111532518715\n",
            "Epoch: 50 \tTraining Loss: 2.309411 \tValidation Loss: 0.060610\n",
            "train_loss:\n",
            "2.3094107379843463\n",
            "Epoch: 51 \tTraining Loss: 2.309411 \tValidation Loss: 0.060609\n",
            "train_loss:\n",
            "2.3094103729331885\n",
            "Epoch: 52 \tTraining Loss: 2.309410 \tValidation Loss: 0.060609\n",
            "train_loss:\n",
            "2.309409982555515\n",
            "Epoch: 53 \tTraining Loss: 2.309410 \tValidation Loss: 0.060609\n",
            "train_loss:\n",
            "2.3094095581180447\n",
            "Epoch: 54 \tTraining Loss: 2.309410 \tValidation Loss: 0.060609\n",
            "train_loss:\n",
            "2.3094091629370666\n",
            "Epoch: 55 \tTraining Loss: 2.309409 \tValidation Loss: 0.060609\n",
            "train_loss:\n",
            "2.3094088712454717\n",
            "Epoch: 56 \tTraining Loss: 2.309409 \tValidation Loss: 0.060609\n",
            "train_loss:\n",
            "2.3094083878583525\n",
            "Epoch: 57 \tTraining Loss: 2.309408 \tValidation Loss: 0.060609\n",
            "train_loss:\n",
            "2.3094081031533826\n",
            "Epoch: 58 \tTraining Loss: 2.309408 \tValidation Loss: 0.060609\n",
            "train_loss:\n",
            "2.3094078171384202\n",
            "Epoch: 59 \tTraining Loss: 2.309408 \tValidation Loss: 0.060609\n",
            "train_loss:\n",
            "2.3094074634405284\n",
            "Epoch: 60 \tTraining Loss: 2.309407 \tValidation Loss: 0.060609\n",
            "train_loss:\n",
            "2.3094072224019646\n",
            "Epoch: 61 \tTraining Loss: 2.309407 \tValidation Loss: 0.060609\n",
            "train_loss:\n",
            "2.309406877874018\n",
            "Epoch: 62 \tTraining Loss: 2.309407 \tValidation Loss: 0.060609\n",
            "train_loss:\n",
            "2.3094065547426106\n",
            "Epoch: 63 \tTraining Loss: 2.309407 \tValidation Loss: 0.060609\n",
            "train_loss:\n",
            "2.3094062936175\n",
            "Epoch: 64 \tTraining Loss: 2.309406 \tValidation Loss: 0.060609\n",
            "train_loss:\n",
            "2.3094060014892412\n",
            "Epoch: 65 \tTraining Loss: 2.309406 \tValidation Loss: 0.060609\n",
            "train_loss:\n",
            "2.309405681851146\n",
            "Epoch: 66 \tTraining Loss: 2.309406 \tValidation Loss: 0.060609\n",
            "train_loss:\n",
            "2.3094054067527856\n",
            "Epoch: 67 \tTraining Loss: 2.309405 \tValidation Loss: 0.060609\n",
            "train_loss:\n",
            "2.3094051513043077\n",
            "Epoch: 68 \tTraining Loss: 2.309405 \tValidation Loss: 0.060609\n",
            "train_loss:\n",
            "2.309404955678807\n",
            "Epoch: 69 \tTraining Loss: 2.309405 \tValidation Loss: 0.060608\n",
            "train_loss:\n",
            "2.3094046107141963\n",
            "Epoch: 70 \tTraining Loss: 2.309405 \tValidation Loss: 0.060608\n",
            "train_loss:\n",
            "2.309404358759031\n",
            "Epoch: 71 \tTraining Loss: 2.309404 \tValidation Loss: 0.060608\n",
            "train_loss:\n",
            "2.3094041574568975\n",
            "Epoch: 72 \tTraining Loss: 2.309404 \tValidation Loss: 0.060608\n",
            "train_loss:\n",
            "2.3094038103089662\n",
            "Epoch: 73 \tTraining Loss: 2.309404 \tValidation Loss: 0.060608\n",
            "train_loss:\n",
            "2.3094036002735514\n",
            "Epoch: 74 \tTraining Loss: 2.309404 \tValidation Loss: 0.060608\n",
            "train_loss:\n",
            "2.3094033007220034\n",
            "Epoch: 75 \tTraining Loss: 2.309403 \tValidation Loss: 0.060608\n",
            "train_loss:\n",
            "2.3094030762766744\n",
            "Epoch: 76 \tTraining Loss: 2.309403 \tValidation Loss: 0.060608\n",
            "train_loss:\n",
            "2.309402835674775\n",
            "Epoch: 77 \tTraining Loss: 2.309403 \tValidation Loss: 0.060608\n",
            "train_loss:\n",
            "2.309402657079173\n",
            "Epoch: 78 \tTraining Loss: 2.309403 \tValidation Loss: 0.060608\n",
            "train_loss:\n",
            "2.3094024658203125\n",
            "Epoch: 79 \tTraining Loss: 2.309402 \tValidation Loss: 0.060608\n",
            "train_loss:\n",
            "2.3094021662687645\n",
            "Epoch: 80 \tTraining Loss: 2.309402 \tValidation Loss: 0.060608\n",
            "train_loss:\n",
            "2.3094019501200527\n",
            "Epoch: 81 \tTraining Loss: 2.309402 \tValidation Loss: 0.060608\n",
            "train_loss:\n",
            "2.309401709518153\n",
            "Epoch: 82 \tTraining Loss: 2.309402 \tValidation Loss: 0.060608\n",
            "train_loss:\n",
            "2.3094015379091757\n",
            "Epoch: 83 \tTraining Loss: 2.309402 \tValidation Loss: 0.060608\n",
            "train_loss:\n",
            "2.3094012702341047\n",
            "Epoch: 84 \tTraining Loss: 2.309401 \tValidation Loss: 0.060608\n",
            "train_loss:\n",
            "2.309401061508682\n",
            "Epoch: 85 \tTraining Loss: 2.309401 \tValidation Loss: 0.060608\n",
            "train_loss:\n",
            "2.3094008536565873\n",
            "Epoch: 86 \tTraining Loss: 2.309401 \tValidation Loss: 0.060608\n",
            "train_loss:\n",
            "2.309400643184508\n",
            "Epoch: 87 \tTraining Loss: 2.309401 \tValidation Loss: 0.060608\n",
            "train_loss:\n",
            "2.3094003912293433\n",
            "Epoch: 88 \tTraining Loss: 2.309400 \tValidation Loss: 0.060608\n",
            "train_loss:\n",
            "2.3094001497541154\n",
            "Epoch: 89 \tTraining Loss: 2.309400 \tValidation Loss: 0.060608\n",
            "train_loss:\n",
            "2.3093999397187006\n",
            "Epoch: 90 \tTraining Loss: 2.309400 \tValidation Loss: 0.060608\n",
            "train_loss:\n",
            "2.309399718766684\n",
            "Epoch: 91 \tTraining Loss: 2.309400 \tValidation Loss: 0.060608\n",
            "train_loss:\n",
            "2.3093995454110505\n",
            "Epoch: 92 \tTraining Loss: 2.309400 \tValidation Loss: 0.060608\n",
            "train_loss:\n",
            "2.3093992441128464\n",
            "Epoch: 93 \tTraining Loss: 2.309399 \tValidation Loss: 0.060608\n",
            "train_loss:\n",
            "2.3093990493606733\n",
            "Epoch: 94 \tTraining Loss: 2.309399 \tValidation Loss: 0.060608\n",
            "train_loss:\n",
            "2.3093987825589304\n",
            "Epoch: 95 \tTraining Loss: 2.309399 \tValidation Loss: 0.060608\n",
            "train_loss:\n",
            "2.3093985546202886\n",
            "Epoch: 96 \tTraining Loss: 2.309399 \tValidation Loss: 0.060608\n",
            "train_loss:\n",
            "2.309398379954663\n",
            "Epoch: 97 \tTraining Loss: 2.309398 \tValidation Loss: 0.060608\n",
            "train_loss:\n",
            "2.3093981725392325\n",
            "Epoch: 98 \tTraining Loss: 2.309398 \tValidation Loss: 0.060608\n",
            "train_loss:\n",
            "2.309397945037255\n",
            "Epoch: 99 \tTraining Loss: 2.309398 \tValidation Loss: 0.060608\n",
            "train_loss:\n",
            "2.3093976904621054\n",
            "Epoch: 100 \tTraining Loss: 2.309398 \tValidation Loss: 0.060608\n",
            "train_loss:\n",
            "2.30939749221662\n",
            "Epoch: 101 \tTraining Loss: 2.309397 \tValidation Loss: 0.060608\n",
            "train_loss:\n",
            "2.3093972385147987\n",
            "Epoch: 102 \tTraining Loss: 2.309397 \tValidation Loss: 0.060608\n",
            "train_loss:\n",
            "2.3093969970395714\n",
            "Epoch: 103 \tTraining Loss: 2.309397 \tValidation Loss: 0.060608\n",
            "train_loss:\n",
            "2.309396804470719\n",
            "Epoch: 104 \tTraining Loss: 2.309397 \tValidation Loss: 0.060608\n",
            "train_loss:\n",
            "2.309396525879046\n",
            "Epoch: 105 \tTraining Loss: 2.309397 \tValidation Loss: 0.060608\n",
            "train_loss:\n",
            "2.3093963189002795\n",
            "Epoch: 106 \tTraining Loss: 2.309396 \tValidation Loss: 0.060608\n",
            "train_loss:\n",
            "2.309396125021435\n",
            "Epoch: 107 \tTraining Loss: 2.309396 \tValidation Loss: 0.060608\n",
            "train_loss:\n",
            "2.3093957892267696\n",
            "Epoch: 108 \tTraining Loss: 2.309396 \tValidation Loss: 0.060608\n",
            "train_loss:\n",
            "2.3093955363982763\n",
            "Epoch: 109 \tTraining Loss: 2.309396 \tValidation Loss: 0.060608\n",
            "train_loss:\n",
            "2.30939527570983\n",
            "Epoch: 110 \tTraining Loss: 2.309395 \tValidation Loss: 0.060608\n",
            "train_loss:\n",
            "2.3093949866382193\n",
            "Epoch: 111 \tTraining Loss: 2.309395 \tValidation Loss: 0.060608\n",
            "train_loss:\n",
            "2.309394753896273\n",
            "Epoch: 112 \tTraining Loss: 2.309395 \tValidation Loss: 0.060608\n",
            "train_loss:\n",
            "2.309394429454873\n",
            "Epoch: 113 \tTraining Loss: 2.309394 \tValidation Loss: 0.060608\n",
            "train_loss:\n",
            "2.309394129029997\n",
            "Epoch: 114 \tTraining Loss: 2.309394 \tValidation Loss: 0.060608\n",
            "train_loss:\n",
            "2.3093938753281757\n",
            "Epoch: 115 \tTraining Loss: 2.309394 \tValidation Loss: 0.060608\n",
            "train_loss:\n",
            "2.3093935072203697\n",
            "Epoch: 116 \tTraining Loss: 2.309394 \tValidation Loss: 0.060608\n",
            "train_loss:\n",
            "2.309393225135384\n",
            "Epoch: 117 \tTraining Loss: 2.309393 \tValidation Loss: 0.060608\n",
            "train_loss:\n",
            "2.309392962263617\n",
            "Epoch: 118 \tTraining Loss: 2.309393 \tValidation Loss: 0.060608\n",
            "train_loss:\n",
            "2.3093925736326\n",
            "Epoch: 119 \tTraining Loss: 2.309393 \tValidation Loss: 0.060608\n",
            "train_loss:\n",
            "2.3093922173147234\n",
            "Epoch: 120 \tTraining Loss: 2.309392 \tValidation Loss: 0.060608\n",
            "train_loss:\n",
            "2.3093918631801675\n",
            "Epoch: 121 \tTraining Loss: 2.309392 \tValidation Loss: 0.060608\n",
            "train_loss:\n",
            "2.309391513848916\n",
            "Epoch: 122 \tTraining Loss: 2.309392 \tValidation Loss: 0.060608\n",
            "train_loss:\n",
            "2.3093911230345787\n",
            "Epoch: 123 \tTraining Loss: 2.309391 \tValidation Loss: 0.060608\n",
            "train_loss:\n",
            "2.309390747066819\n",
            "Epoch: 124 \tTraining Loss: 2.309391 \tValidation Loss: 0.060608\n",
            "train_loss:\n",
            "2.3093903152060595\n",
            "Epoch: 125 \tTraining Loss: 2.309390 \tValidation Loss: 0.060608\n",
            "train_loss:\n",
            "2.3093898374955732\n",
            "Epoch: 126 \tTraining Loss: 2.309390 \tValidation Loss: 0.060608\n",
            "train_loss:\n",
            "2.309389355418446\n",
            "Epoch: 127 \tTraining Loss: 2.309389 \tValidation Loss: 0.060608\n",
            "train_loss:\n",
            "2.3093887951784517\n",
            "Epoch: 128 \tTraining Loss: 2.309389 \tValidation Loss: 0.060608\n",
            "train_loss:\n",
            "2.3093883091713483\n",
            "Epoch: 129 \tTraining Loss: 2.309388 \tValidation Loss: 0.060608\n",
            "train_loss:\n",
            "2.3093877065749395\n",
            "Epoch: 130 \tTraining Loss: 2.309388 \tValidation Loss: 0.060608\n",
            "train_loss:\n",
            "2.309387179958078\n",
            "Epoch: 131 \tTraining Loss: 2.309387 \tValidation Loss: 0.060608\n",
            "train_loss:\n",
            "2.30938660967481\n",
            "Epoch: 132 \tTraining Loss: 2.309387 \tValidation Loss: 0.060608\n",
            "train_loss:\n",
            "2.3093858625425963\n",
            "Epoch: 133 \tTraining Loss: 2.309386 \tValidation Loss: 0.060608\n",
            "train_loss:\n",
            "2.3093851184670307\n",
            "Epoch: 134 \tTraining Loss: 2.309385 \tValidation Loss: 0.060608\n",
            "train_loss:\n",
            "2.3093843045252145\n",
            "Epoch: 135 \tTraining Loss: 2.309384 \tValidation Loss: 0.060608\n",
            "train_loss:\n",
            "2.3093834787934693\n",
            "Epoch: 136 \tTraining Loss: 2.309383 \tValidation Loss: 0.060608\n",
            "train_loss:\n",
            "2.3093825508823325\n",
            "Epoch: 137 \tTraining Loss: 2.309383 \tValidation Loss: 0.060608\n",
            "train_loss:\n",
            "2.3093815570349223\n",
            "Epoch: 138 \tTraining Loss: 2.309382 \tValidation Loss: 0.060608\n",
            "train_loss:\n",
            "2.3093804387382537\n",
            "Epoch: 139 \tTraining Loss: 2.309380 \tValidation Loss: 0.060608\n",
            "train_loss:\n",
            "2.309379218698858\n",
            "Epoch: 140 \tTraining Loss: 2.309379 \tValidation Loss: 0.060608\n",
            "train_loss:\n",
            "2.309377894733415\n",
            "Epoch: 141 \tTraining Loss: 2.309378 \tValidation Loss: 0.060608\n",
            "train_loss:\n",
            "2.3093763209961273\n",
            "Epoch: 142 \tTraining Loss: 2.309376 \tValidation Loss: 0.060608\n",
            "train_loss:\n",
            "2.3093744743437994\n",
            "Epoch: 143 \tTraining Loss: 2.309374 \tValidation Loss: 0.060608\n",
            "train_loss:\n",
            "2.309372518088791\n",
            "Epoch: 144 \tTraining Loss: 2.309373 \tValidation Loss: 0.060608\n",
            "train_loss:\n",
            "2.309370218179165\n",
            "Epoch: 145 \tTraining Loss: 2.309370 \tValidation Loss: 0.060608\n",
            "train_loss:\n",
            "2.3093674047525985\n",
            "Epoch: 146 \tTraining Loss: 2.309367 \tValidation Loss: 0.060608\n",
            "train_loss:\n",
            "2.3093640891623584\n",
            "Epoch: 147 \tTraining Loss: 2.309364 \tValidation Loss: 0.060609\n",
            "train_loss:\n",
            "2.3093600565697248\n",
            "Epoch: 148 \tTraining Loss: 2.309360 \tValidation Loss: 0.060609\n",
            "train_loss:\n",
            "2.3093551301257516\n",
            "Epoch: 149 \tTraining Loss: 2.309355 \tValidation Loss: 0.060609\n",
            "train_loss:\n",
            "2.3093486954441\n",
            "Epoch: 150 \tTraining Loss: 2.309349 \tValidation Loss: 0.060609\n",
            "train_loss:\n",
            "2.309340145125057\n",
            "Epoch: 151 \tTraining Loss: 2.309340 \tValidation Loss: 0.060610\n",
            "train_loss:\n",
            "2.3093280687436955\n",
            "Epoch: 152 \tTraining Loss: 2.309328 \tValidation Loss: 0.060610\n",
            "train_loss:\n",
            "2.309309858105558\n",
            "Epoch: 153 \tTraining Loss: 2.309310 \tValidation Loss: 0.060611\n",
            "train_loss:\n",
            "2.309279046215854\n",
            "Epoch: 154 \tTraining Loss: 2.309279 \tValidation Loss: 0.060612\n",
            "train_loss:\n",
            "2.309214719049223\n",
            "Epoch: 155 \tTraining Loss: 2.309215 \tValidation Loss: 0.060614\n",
            "train_loss:\n",
            "2.308999022721371\n",
            "Epoch: 156 \tTraining Loss: 2.308999 \tValidation Loss: 0.060615\n",
            "train_loss:\n",
            "2.3090660659384814\n",
            "Epoch: 157 \tTraining Loss: 2.309066 \tValidation Loss: 0.060656\n",
            "train_loss:\n",
            "2.3214927850625453\n",
            "Epoch: 158 \tTraining Loss: 2.321493 \tValidation Loss: 0.060607\n",
            "train_loss:\n",
            "2.3093976371890896\n",
            "Epoch: 159 \tTraining Loss: 2.309398 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.309399038880736\n",
            "Epoch: 160 \tTraining Loss: 2.309399 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093989939043373\n",
            "Epoch: 161 \tTraining Loss: 2.309399 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093988467485476\n",
            "Epoch: 162 \tTraining Loss: 2.309399 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093987685856803\n",
            "Epoch: 163 \tTraining Loss: 2.309399 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093986886761564\n",
            "Epoch: 164 \tTraining Loss: 2.309399 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.309398610513289\n",
            "Epoch: 165 \tTraining Loss: 2.309399 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093984672874757\n",
            "Epoch: 166 \tTraining Loss: 2.309398 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.309398441524296\n",
            "Epoch: 167 \tTraining Loss: 2.309398 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093983293016316\n",
            "Epoch: 168 \tTraining Loss: 2.309398 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.309398284325233\n",
            "Epoch: 169 \tTraining Loss: 2.309398 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.309398291748522\n",
            "Epoch: 170 \tTraining Loss: 2.309398 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093980782197945\n",
            "Epoch: 171 \tTraining Loss: 2.309398 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093979895769894\n",
            "Epoch: 172 \tTraining Loss: 2.309398 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093979882669973\n",
            "Epoch: 173 \tTraining Loss: 2.309398 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.309397945037255\n",
            "Epoch: 174 \tTraining Loss: 2.309398 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093977035620274\n",
            "Epoch: 175 \tTraining Loss: 2.309398 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.309397658585629\n",
            "Epoch: 176 \tTraining Loss: 2.309398 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093975550962456\n",
            "Epoch: 177 \tTraining Loss: 2.309398 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093974660167764\n",
            "Epoch: 178 \tTraining Loss: 2.309397 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.309397361654065\n",
            "Epoch: 179 \tTraining Loss: 2.309397 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.309397273447924\n",
            "Epoch: 180 \tTraining Loss: 2.309397 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.309397135898744\n",
            "Epoch: 181 \tTraining Loss: 2.309397 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093969926729305\n",
            "Epoch: 182 \tTraining Loss: 2.309397 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093970070828447\n",
            "Epoch: 183 \tTraining Loss: 2.309397 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093967760875547\n",
            "Epoch: 184 \tTraining Loss: 2.309397 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.30939671539125\n",
            "Epoch: 185 \tTraining Loss: 2.309397 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093966263117807\n",
            "Epoch: 186 \tTraining Loss: 2.309397 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093965031725148\n",
            "Epoch: 187 \tTraining Loss: 2.309397 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.309396383963225\n",
            "Epoch: 188 \tTraining Loss: 2.309396 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093962429207324\n",
            "Epoch: 189 \tTraining Loss: 2.309396 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093961079915366\n",
            "Epoch: 190 \tTraining Loss: 2.309396 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.309396063015138\n",
            "Epoch: 191 \tTraining Loss: 2.309396 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093959586524266\n",
            "Epoch: 192 \tTraining Loss: 2.309396 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.309395806693332\n",
            "Epoch: 193 \tTraining Loss: 2.309396 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093957202338475\n",
            "Epoch: 194 \tTraining Loss: 2.309396 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093955979679093\n",
            "Epoch: 195 \tTraining Loss: 2.309396 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093954481921353\n",
            "Epoch: 196 \tTraining Loss: 2.309395 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093952198168295\n",
            "Epoch: 197 \tTraining Loss: 2.309395 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.309395058251126\n",
            "Epoch: 198 \tTraining Loss: 2.309395 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093950067247664\n",
            "Epoch: 199 \tTraining Loss: 2.309395 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.309394889262133\n",
            "Epoch: 200 \tTraining Loss: 2.309395 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.309394665253468\n",
            "Epoch: 201 \tTraining Loss: 2.309395 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093944691913033\n",
            "Epoch: 202 \tTraining Loss: 2.309394 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.309394296708998\n",
            "Epoch: 203 \tTraining Loss: 2.309394 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.309394115056747\n",
            "Epoch: 204 \tTraining Loss: 2.309394 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.309393884498121\n",
            "Epoch: 205 \tTraining Loss: 2.309394 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.309393808955238\n",
            "Epoch: 206 \tTraining Loss: 2.309394 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.309393665292761\n",
            "Epoch: 207 \tTraining Loss: 2.309394 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093933897577363\n",
            "Epoch: 208 \tTraining Loss: 2.309393 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.309393132125938\n",
            "Epoch: 209 \tTraining Loss: 2.309393 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093929103005935\n",
            "Epoch: 210 \tTraining Loss: 2.309393 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093926273422802\n",
            "Epoch: 211 \tTraining Loss: 2.309393 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.309392335214021\n",
            "Epoch: 212 \tTraining Loss: 2.309392 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.309392119501973\n",
            "Epoch: 213 \tTraining Loss: 2.309392 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.309391898113293\n",
            "Epoch: 214 \tTraining Loss: 2.309392 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.309391615591643\n",
            "Epoch: 215 \tTraining Loss: 2.309392 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093912570904465\n",
            "Epoch: 216 \tTraining Loss: 2.309391 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.309390916055812\n",
            "Epoch: 217 \tTraining Loss: 2.309391 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093906274208655\n",
            "Epoch: 218 \tTraining Loss: 2.309391 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093902195766294\n",
            "Epoch: 219 \tTraining Loss: 2.309390 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.309389834438925\n",
            "Epoch: 220 \tTraining Loss: 2.309390 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309389374195001\n",
            "Epoch: 221 \tTraining Loss: 2.309389 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093888886245617\n",
            "Epoch: 222 \tTraining Loss: 2.309389 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093883799109265\n",
            "Epoch: 223 \tTraining Loss: 2.309388 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093878004577135\n",
            "Epoch: 224 \tTraining Loss: 2.309388 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093873463270866\n",
            "Epoch: 225 \tTraining Loss: 2.309387 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309386657707857\n",
            "Epoch: 226 \tTraining Loss: 2.309387 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093859380854793\n",
            "Epoch: 227 \tTraining Loss: 2.309386 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093852464096014\n",
            "Epoch: 228 \tTraining Loss: 2.309385 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093843927313555\n",
            "Epoch: 229 \tTraining Loss: 2.309384 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309383430323758\n",
            "Epoch: 230 \tTraining Loss: 2.309383 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093823871333083\n",
            "Epoch: 231 \tTraining Loss: 2.309382 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093811810671627\n",
            "Epoch: 232 \tTraining Loss: 2.309381 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.30937982915522\n",
            "Epoch: 233 \tTraining Loss: 2.309380 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093782641512135\n",
            "Epoch: 234 \tTraining Loss: 2.309378 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093765196782767\n",
            "Epoch: 235 \tTraining Loss: 2.309377 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093743533878537\n",
            "Epoch: 236 \tTraining Loss: 2.309374 \tValidation Loss: 0.060604\n",
            "train_loss:\n",
            "2.309371782309843\n",
            "Epoch: 237 \tTraining Loss: 2.309372 \tValidation Loss: 0.060604\n",
            "train_loss:\n",
            "2.3093686750083617\n",
            "Epoch: 238 \tTraining Loss: 2.309369 \tValidation Loss: 0.060604\n",
            "train_loss:\n",
            "2.3093650057202293\n",
            "Epoch: 239 \tTraining Loss: 2.309365 \tValidation Loss: 0.060604\n",
            "train_loss:\n",
            "2.3093600692329828\n",
            "Epoch: 240 \tTraining Loss: 2.309360 \tValidation Loss: 0.060603\n",
            "train_loss:\n",
            "2.309353539795229\n",
            "Epoch: 241 \tTraining Loss: 2.309354 \tValidation Loss: 0.060603\n",
            "train_loss:\n",
            "2.309344624425029\n",
            "Epoch: 242 \tTraining Loss: 2.309345 \tValidation Loss: 0.060602\n",
            "train_loss:\n",
            "2.309331308791052\n",
            "Epoch: 243 \tTraining Loss: 2.309331 \tValidation Loss: 0.060601\n",
            "train_loss:\n",
            "2.309309592613807\n",
            "Epoch: 244 \tTraining Loss: 2.309310 \tValidation Loss: 0.060599\n",
            "train_loss:\n",
            "2.3092677187133623\n",
            "Epoch: 245 \tTraining Loss: 2.309268 \tValidation Loss: 0.060595\n",
            "train_loss:\n",
            "2.3091541712100687\n",
            "Epoch: 246 \tTraining Loss: 2.309154 \tValidation Loss: 0.060579\n",
            "train_loss:\n",
            "2.3088340759277344\n",
            "Epoch: 247 \tTraining Loss: 2.308834 \tValidation Loss: 0.060513\n",
            "train_loss:\n",
            "2.349308023085961\n",
            "Epoch: 248 \tTraining Loss: 2.349308 \tValidation Loss: 0.060631\n",
            "train_loss:\n",
            "2.3093047797024906\n",
            "Epoch: 249 \tTraining Loss: 2.309305 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3094000087116227\n",
            "Epoch: 250 \tTraining Loss: 2.309400 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093999995416774\n",
            "Epoch: 251 \tTraining Loss: 2.309400 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.309400025741521\n",
            "Epoch: 252 \tTraining Loss: 2.309400 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.309399964171888\n",
            "Epoch: 253 \tTraining Loss: 2.309400 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.309399982948443\n",
            "Epoch: 254 \tTraining Loss: 2.309400 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.309399973778498\n",
            "Epoch: 255 \tTraining Loss: 2.309400 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093999322954115\n",
            "Epoch: 256 \tTraining Loss: 2.309400 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093999532552867\n",
            "Epoch: 257 \tTraining Loss: 2.309400 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.309399956748599\n",
            "Epoch: 258 \tTraining Loss: 2.309400 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093999130821925\n",
            "Epoch: 259 \tTraining Loss: 2.309400 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093998707257786\n",
            "Epoch: 260 \tTraining Loss: 2.309400 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.30939988164238\n",
            "Epoch: 261 \tTraining Loss: 2.309400 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.309399922252138\n",
            "Epoch: 262 \tTraining Loss: 2.309400 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093999113355363\n",
            "Epoch: 263 \tTraining Loss: 2.309400 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093998977989503\n",
            "Epoch: 264 \tTraining Loss: 2.309400 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093997899429266\n",
            "Epoch: 265 \tTraining Loss: 2.309400 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093997973662157\n",
            "Epoch: 266 \tTraining Loss: 2.309400 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093997733496923\n",
            "Epoch: 267 \tTraining Loss: 2.309400 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093997750963484\n",
            "Epoch: 268 \tTraining Loss: 2.309400 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093997567564575\n",
            "Epoch: 269 \tTraining Loss: 2.309400 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093997218233326\n",
            "Epoch: 270 \tTraining Loss: 2.309400 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093997013001215\n",
            "Epoch: 271 \tTraining Loss: 2.309400 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093997209500046\n",
            "Epoch: 272 \tTraining Loss: 2.309400 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093996681136524\n",
            "Epoch: 273 \tTraining Loss: 2.309400 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.30939965283041\n",
            "Epoch: 274 \tTraining Loss: 2.309400 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093997414732153\n",
            "Epoch: 275 \tTraining Loss: 2.309400 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.309399637983832\n",
            "Epoch: 276 \tTraining Loss: 2.309400 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093996008673865\n",
            "Epoch: 277 \tTraining Loss: 2.309400 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093996397304886\n",
            "Epoch: 278 \tTraining Loss: 2.309400 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.309399556327652\n",
            "Epoch: 279 \tTraining Loss: 2.309400 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093996196439415\n",
            "Epoch: 280 \tTraining Loss: 2.309400 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093995925707693\n",
            "Epoch: 281 \tTraining Loss: 2.309400 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093996231372538\n",
            "Epoch: 282 \tTraining Loss: 2.309400 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093995458477146\n",
            "Epoch: 283 \tTraining Loss: 2.309400 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093995436643944\n",
            "Epoch: 284 \tTraining Loss: 2.309400 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.309399505674621\n",
            "Epoch: 285 \tTraining Loss: 2.309400 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093995296911443\n",
            "Epoch: 286 \tTraining Loss: 2.309400 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093994633182064\n",
            "Epoch: 287 \tTraining Loss: 2.309399 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.309399493884691\n",
            "Epoch: 288 \tTraining Loss: 2.309399 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093995034913\n",
            "Epoch: 289 \tTraining Loss: 2.309400 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093994820947614\n",
            "Epoch: 290 \tTraining Loss: 2.309399 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093994860247378\n",
            "Epoch: 291 \tTraining Loss: 2.309399 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093994681215113\n",
            "Epoch: 292 \tTraining Loss: 2.309399 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093993698720965\n",
            "Epoch: 293 \tTraining Loss: 2.309399 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093994196518\n",
            "Epoch: 294 \tTraining Loss: 2.309399 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093993751120654\n",
            "Epoch: 295 \tTraining Loss: 2.309399 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093993620121434\n",
            "Epoch: 296 \tTraining Loss: 2.309399 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.309399359392159\n",
            "Epoch: 297 \tTraining Loss: 2.309399 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.309399413538503\n",
            "Epoch: 298 \tTraining Loss: 2.309399 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093993218390496\n",
            "Epoch: 299 \tTraining Loss: 2.309399 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.309399336685628\n",
            "Epoch: 300 \tTraining Loss: 2.309399 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093993336289795\n",
            "Epoch: 301 \tTraining Loss: 2.309399 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093993227123777\n",
            "Epoch: 302 \tTraining Loss: 2.309399 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093992803559633\n",
            "Epoch: 303 \tTraining Loss: 2.309399 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093992275196116\n",
            "Epoch: 304 \tTraining Loss: 2.309399 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.309399271186018\n",
            "Epoch: 305 \tTraining Loss: 2.309399 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.309399275115995\n",
            "Epoch: 306 \tTraining Loss: 2.309399 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.309399255466112\n",
            "Epoch: 307 \tTraining Loss: 2.309399 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.309399200009776\n",
            "Epoch: 308 \tTraining Loss: 2.309399 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.309399197826455\n",
            "Epoch: 309 \tTraining Loss: 2.309399 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.309399217039674\n",
            "Epoch: 310 \tTraining Loss: 2.309399 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.309399242802854\n",
            "Epoch: 311 \tTraining Loss: 2.309399 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093991925864867\n",
            "Epoch: 312 \tTraining Loss: 2.309399 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.309399126650213\n",
            "Epoch: 313 \tTraining Loss: 2.309399 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.309399114860283\n",
            "Epoch: 314 \tTraining Loss: 2.309399 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093991650766506\n",
            "Epoch: 315 \tTraining Loss: 2.309399 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.309399140623463\n",
            "Epoch: 316 \tTraining Loss: 2.309399 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.30939912097358\n",
            "Epoch: 317 \tTraining Loss: 2.309399 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.309399113550291\n",
            "Epoch: 318 \tTraining Loss: 2.309399 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.309399058967283\n",
            "Epoch: 319 \tTraining Loss: 2.309399 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093990738138612\n",
            "Epoch: 320 \tTraining Loss: 2.309399 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093990829838065\n",
            "Epoch: 321 \tTraining Loss: 2.309399 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.309399038880736\n",
            "Epoch: 322 \tTraining Loss: 2.309399 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093990620239313\n",
            "Epoch: 323 \tTraining Loss: 2.309399 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.309398991721017\n",
            "Epoch: 324 \tTraining Loss: 2.309399 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093989891010325\n",
            "Epoch: 325 \tTraining Loss: 2.309399 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093990157375406\n",
            "Epoch: 326 \tTraining Loss: 2.309399 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.309398973381126\n",
            "Epoch: 327 \tTraining Loss: 2.309399 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093989886643684\n",
            "Epoch: 328 \tTraining Loss: 2.309399 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.309399032330775\n",
            "Epoch: 329 \tTraining Loss: 2.309399 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.309398959407876\n",
            "Epoch: 330 \tTraining Loss: 2.309399 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093989393213294\n",
            "Epoch: 331 \tTraining Loss: 2.309399 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.309398978184431\n",
            "Epoch: 332 \tTraining Loss: 2.309399 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.309398962901189\n",
            "Epoch: 333 \tTraining Loss: 2.309399 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093989554778998\n",
            "Epoch: 334 \tTraining Loss: 2.309399 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.309398950674595\n",
            "Epoch: 335 \tTraining Loss: 2.309399 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093989227280947\n",
            "Epoch: 336 \tTraining Loss: 2.309399 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.30939895984454\n",
            "Epoch: 337 \tTraining Loss: 2.309399 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.309398910938165\n",
            "Epoch: 338 \tTraining Loss: 2.309399 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.309398904388204\n",
            "Epoch: 339 \tTraining Loss: 2.309399 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.309398950237931\n",
            "Epoch: 340 \tTraining Loss: 2.309399 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093989253480793\n",
            "Epoch: 341 \tTraining Loss: 2.309399 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.309398903514876\n",
            "Epoch: 342 \tTraining Loss: 2.309399 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.309398904824868\n",
            "Epoch: 343 \tTraining Loss: 2.309399 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.309398841508579\n",
            "Epoch: 344 \tTraining Loss: 2.309399 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.30939888779497\n",
            "Epoch: 345 \tTraining Loss: 2.309399 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.309398862905118\n",
            "Epoch: 346 \tTraining Loss: 2.309399 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093987969688445\n",
            "Epoch: 347 \tTraining Loss: 2.309399 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.309398810068766\n",
            "Epoch: 348 \tTraining Loss: 2.309399 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.309398787798899\n",
            "Epoch: 349 \tTraining Loss: 2.309399 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.309398827535329\n",
            "Epoch: 350 \tTraining Loss: 2.309399 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.309398761162391\n",
            "Epoch: 351 \tTraining Loss: 2.309399 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093987720789926\n",
            "Epoch: 352 \tTraining Loss: 2.309399 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093987764456334\n",
            "Epoch: 353 \tTraining Loss: 2.309399 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093987847422506\n",
            "Epoch: 354 \tTraining Loss: 2.309399 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.309398793475532\n",
            "Epoch: 355 \tTraining Loss: 2.309399 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.309398788672227\n",
            "Epoch: 356 \tTraining Loss: 2.309399 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.309398733215891\n",
            "Epoch: 357 \tTraining Loss: 2.309399 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.309398733215891\n",
            "Epoch: 358 \tTraining Loss: 2.309399 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093987083260394\n",
            "Epoch: 359 \tTraining Loss: 2.309399 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.309398745442485\n",
            "Epoch: 360 \tTraining Loss: 2.309399 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093986624763128\n",
            "Epoch: 361 \tTraining Loss: 2.309399 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093987847422506\n",
            "Epoch: 362 \tTraining Loss: 2.309399 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093987432591643\n",
            "Epoch: 363 \tTraining Loss: 2.309399 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093986685896093\n",
            "Epoch: 364 \tTraining Loss: 2.309399 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093986878028283\n",
            "Epoch: 365 \tTraining Loss: 2.309399 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093986550530237\n",
            "Epoch: 366 \tTraining Loss: 2.309399 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.309398656363016\n",
            "Epoch: 367 \tTraining Loss: 2.309399 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093986057099842\n",
            "Epoch: 368 \tTraining Loss: 2.309399 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.30939860309\n",
            "Epoch: 369 \tTraining Loss: 2.309399 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093986096399606\n",
            "Epoch: 370 \tTraining Loss: 2.309399 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093986161899216\n",
            "Epoch: 371 \tTraining Loss: 2.309399 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093985812567968\n",
            "Epoch: 372 \tTraining Loss: 2.309399 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.309398598286695\n",
            "Epoch: 373 \tTraining Loss: 2.309399 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.309398569903531\n",
            "Epoch: 374 \tTraining Loss: 2.309399 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093985873700933\n",
            "Epoch: 375 \tTraining Loss: 2.309399 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.309398571650187\n",
            "Epoch: 376 \tTraining Loss: 2.309399 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093985733968436\n",
            "Epoch: 377 \tTraining Loss: 2.309399 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.309398569030203\n",
            "Epoch: 378 \tTraining Loss: 2.309399 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093985664102186\n",
            "Epoch: 379 \tTraining Loss: 2.309399 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.309398562480242\n",
            "Epoch: 380 \tTraining Loss: 2.309399 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093985559302808\n",
            "Epoch: 381 \tTraining Loss: 2.309399 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093985445770153\n",
            "Epoch: 382 \tTraining Loss: 2.309399 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093984851907026\n",
            "Epoch: 383 \tTraining Loss: 2.309398 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.309398550253648\n",
            "Epoch: 384 \tTraining Loss: 2.309399 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093984996006167\n",
            "Epoch: 385 \tTraining Loss: 2.309398 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.30939848169739\n",
            "Epoch: 386 \tTraining Loss: 2.309398 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.30939848169739\n",
            "Epoch: 387 \tTraining Loss: 2.309398 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093985057139133\n",
            "Epoch: 388 \tTraining Loss: 2.309399 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093984734007726\n",
            "Epoch: 389 \tTraining Loss: 2.309398 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.309398433664343\n",
            "Epoch: 390 \tTraining Loss: 2.309398 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093984677241397\n",
            "Epoch: 391 \tTraining Loss: 2.309398 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.309398454624218\n",
            "Epoch: 392 \tTraining Loss: 2.309398 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.309398448074257\n",
            "Epoch: 393 \tTraining Loss: 2.309398 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.30939843017103\n",
            "Epoch: 394 \tTraining Loss: 2.309398 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.309398401351202\n",
            "Epoch: 395 \tTraining Loss: 2.309398 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093984105211476\n",
            "Epoch: 396 \tTraining Loss: 2.309398 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.309398434101007\n",
            "Epoch: 397 \tTraining Loss: 2.309398 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.309398355501475\n",
            "Epoch: 398 \tTraining Loss: 2.309398 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.309398446764265\n",
            "Epoch: 399 \tTraining Loss: 2.309398 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.309398361178108\n",
            "Epoch: 400 \tTraining Loss: 2.309398 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093983633614283\n",
            "Epoch: 401 \tTraining Loss: 2.309398 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093983716580455\n",
            "Epoch: 402 \tTraining Loss: 2.309398 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093983236249986\n",
            "Epoch: 403 \tTraining Loss: 2.309398 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.3093983786446706\n",
            "Epoch: 404 \tTraining Loss: 2.309398 \tValidation Loss: 0.060606\n",
            "train_loss:\n",
            "2.30939835812146\n",
            "Epoch: 405 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309398340654897\n",
            "Epoch: 406 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093983428382177\n",
            "Epoch: 407 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093982528854204\n",
            "Epoch: 408 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093983690380613\n",
            "Epoch: 409 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093983026651235\n",
            "Epoch: 410 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093982961151625\n",
            "Epoch: 411 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309398272098639\n",
            "Epoch: 412 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309398285198561\n",
            "Epoch: 413 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309398258125389\n",
            "Epoch: 414 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093982410954905\n",
            "Epoch: 415 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309398175595881\n",
            "Epoch: 416 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093981865124826\n",
            "Epoch: 417 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093982380388423\n",
            "Epoch: 418 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093981878224747\n",
            "Epoch: 419 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309398200922397\n",
            "Epoch: 420 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093981873858107\n",
            "Epoch: 421 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309398210529006\n",
            "Epoch: 422 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309398169919248\n",
            "Epoch: 423 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309398182145842\n",
            "Epoch: 424 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093981450293963\n",
            "Epoch: 425 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093981690459198\n",
            "Epoch: 426 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093981603126386\n",
            "Epoch: 427 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093981048563026\n",
            "Epoch: 428 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093981376061072\n",
            "Epoch: 429 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093981616226307\n",
            "Epoch: 430 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309398128872826\n",
            "Epoch: 431 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093980939397007\n",
            "Epoch: 432 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.30939811751956\n",
            "Epoch: 433 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093980830230993\n",
            "Epoch: 434 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093981328028024\n",
            "Epoch: 435 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093981061662947\n",
            "Epoch: 436 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093980799664506\n",
            "Epoch: 437 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093980886997323\n",
            "Epoch: 438 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093980939397007\n",
            "Epoch: 439 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309398055076599\n",
            "Epoch: 440 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309398102672982\n",
            "Epoch: 441 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309398039793357\n",
            "Epoch: 442 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093980572599193\n",
            "Epoch: 443 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093979983102706\n",
            "Epoch: 444 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093980873897397\n",
            "Epoch: 445 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093980376100367\n",
            "Epoch: 446 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309398052019951\n",
            "Epoch: 447 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093980825864353\n",
            "Epoch: 448 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309398054639935\n",
            "Epoch: 449 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309398028440091\n",
            "Epoch: 450 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309398068613185\n",
            "Epoch: 451 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309398066429865\n",
            "Epoch: 452 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093980642465444\n",
            "Epoch: 453 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309398034553388\n",
            "Epoch: 454 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093980511466223\n",
            "Epoch: 455 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093980655565365\n",
            "Epoch: 456 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093980087902084\n",
            "Epoch: 457 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309397980407044\n",
            "Epoch: 458 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309397984773685\n",
            "Epoch: 459 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309397971673763\n",
            "Epoch: 460 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309397993506966\n",
            "Epoch: 461 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093979956902864\n",
            "Epoch: 462 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093980044235676\n",
            "Epoch: 463 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309397986083677\n",
            "Epoch: 464 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093980057335597\n",
            "Epoch: 465 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093979943802943\n",
            "Epoch: 466 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309397918400747\n",
            "Epoch: 467 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093979572638488\n",
            "Epoch: 468 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309397979097052\n",
            "Epoch: 469 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093979546438645\n",
            "Epoch: 470 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093979437272627\n",
            "Epoch: 471 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093979288806845\n",
            "Epoch: 472 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093979349939815\n",
            "Epoch: 473 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093979328106613\n",
            "Epoch: 474 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093979415439425\n",
            "Epoch: 475 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093979764770673\n",
            "Epoch: 476 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309397978660388\n",
            "Epoch: 477 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093979411072785\n",
            "Epoch: 478 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.30939791228745\n",
            "Epoch: 479 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093979192740752\n",
            "Epoch: 480 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093978681843796\n",
            "Epoch: 481 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093978507178172\n",
            "Epoch: 482 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309397890454247\n",
            "Epoch: 483 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093978812843017\n",
            "Epoch: 484 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309397857267778\n",
            "Epoch: 485 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309397831067934\n",
            "Epoch: 486 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093978546477936\n",
            "Epoch: 487 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093978393645513\n",
            "Epoch: 488 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093978262646293\n",
            "Epoch: 489 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093978524644734\n",
            "Epoch: 490 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309397837181231\n",
            "Epoch: 491 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093978450411843\n",
            "Epoch: 492 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309397817094684\n",
            "Epoch: 493 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093977839082154\n",
            "Epoch: 494 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093978472245045\n",
            "Epoch: 495 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093978515911453\n",
            "Epoch: 496 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093977860915356\n",
            "Epoch: 497 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309397814038036\n",
            "Epoch: 498 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093977812882307\n",
            "Epoch: 499 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093978005014497\n",
            "Epoch: 500 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309397761201684\n",
            "Epoch: 501 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309397730635199\n",
            "Epoch: 502 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093977874015277\n",
            "Epoch: 503 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309397745918442\n",
            "Epoch: 504 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309397715351957\n",
            "Epoch: 505 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309397715351957\n",
            "Epoch: 506 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309397728451879\n",
            "Epoch: 507 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093976843488084\n",
            "Epoch: 508 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309397734128512\n",
            "Epoch: 509 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093977253952307\n",
            "Epoch: 510 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093977170986135\n",
            "Epoch: 511 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093976839121444\n",
            "Epoch: 512 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093977127319727\n",
            "Epoch: 513 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093977053086836\n",
            "Epoch: 514 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309397707928668\n",
            "Epoch: 515 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093976965754024\n",
            "Epoch: 516 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309397665572254\n",
            "Epoch: 517 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093977603283555\n",
            "Epoch: 518 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093976577123008\n",
            "Epoch: 519 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309397652472332\n",
            "Epoch: 520 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093976786721755\n",
            "Epoch: 521 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093977005053787\n",
            "Epoch: 522 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093976843488084\n",
            "Epoch: 523 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093976930820896\n",
            "Epoch: 524 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093976782355115\n",
            "Epoch: 525 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093976297658005\n",
            "Epoch: 526 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093976315124567\n",
            "Epoch: 527 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.30939766731891\n",
            "Epoch: 528 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093976367524256\n",
            "Epoch: 529 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309397660768949\n",
            "Epoch: 530 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093976843488084\n",
            "Epoch: 531 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309397633695777\n",
            "Epoch: 532 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309397606622605\n",
            "Epoch: 533 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093976179758706\n",
            "Epoch: 534 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093976205958553\n",
            "Epoch: 535 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093976511623397\n",
            "Epoch: 536 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309397659895621\n",
            "Epoch: 537 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093976140458943\n",
            "Epoch: 538 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093976577123008\n",
            "Epoch: 539 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093976376257537\n",
            "Epoch: 540 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093976354424335\n",
            "Epoch: 541 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309397600509308\n",
            "Epoch: 542 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309397574309464\n",
            "Epoch: 543 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309397583042746\n",
            "Epoch: 544 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093976223425114\n",
            "Epoch: 545 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309397577802777\n",
            "Epoch: 546 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309397633259113\n",
            "Epoch: 547 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093975756194567\n",
            "Epoch: 548 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093975494196126\n",
            "Epoch: 549 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309397591776027\n",
            "Epoch: 550 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093976179758706\n",
            "Epoch: 551 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309397597015996\n",
            "Epoch: 552 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309397590902699\n",
            "Epoch: 553 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093976040026205\n",
            "Epoch: 554 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309397593522683\n",
            "Epoch: 555 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093975751827927\n",
            "Epoch: 556 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309397573436136\n",
            "Epoch: 557 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093974686367607\n",
            "Epoch: 558 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093975550962456\n",
            "Epoch: 559 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309397564266191\n",
            "Epoch: 560 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093975005132377\n",
            "Epoch: 561 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093974743133936\n",
            "Epoch: 562 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093974786800344\n",
            "Epoch: 563 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093975009499017\n",
            "Epoch: 564 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093974459302293\n",
            "Epoch: 565 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093974721300734\n",
            "Epoch: 566 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093974852299954\n",
            "Epoch: 567 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093974520435263\n",
            "Epoch: 568 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309397456410167\n",
            "Epoch: 569 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093974786800344\n",
            "Epoch: 570 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093974852299954\n",
            "Epoch: 571 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093974865399876\n",
            "Epoch: 572 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093974865399876\n",
            "Epoch: 573 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309397512739831\n",
            "Epoch: 574 \tTraining Loss: 2.309398 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093974668901045\n",
            "Epoch: 575 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.30939743632362\n",
            "Epoch: 576 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309397410123776\n",
            "Epoch: 577 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093974581568233\n",
            "Epoch: 578 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093974642701203\n",
            "Epoch: 579 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093973725706665\n",
            "Epoch: 580 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093973856705885\n",
            "Epoch: 581 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309397420603714\n",
            "Epoch: 582 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093973944038697\n",
            "Epoch: 583 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309397361654065\n",
            "Epoch: 584 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309397431520315\n",
            "Epoch: 585 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093973703873463\n",
            "Epoch: 586 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309397348554143\n",
            "Epoch: 587 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309397348554143\n",
            "Epoch: 588 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309397366020706\n",
            "Epoch: 589 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093973769373073\n",
            "Epoch: 590 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309397361217401\n",
            "Epoch: 591 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093974005171667\n",
            "Epoch: 592 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309397374317323\n",
            "Epoch: 593 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093974468035574\n",
            "Epoch: 594 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.30939735248412\n",
            "Epoch: 595 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309397348117479\n",
            "Epoch: 596 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.30939735466744\n",
            "Epoch: 597 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309397350300799\n",
            "Epoch: 598 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309397348554143\n",
            "Epoch: 599 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093973852339245\n",
            "Epoch: 600 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.30939735466744\n",
            "Epoch: 601 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093973328342368\n",
            "Epoch: 602 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093973188609866\n",
            "Epoch: 603 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093972826178693\n",
            "Epoch: 604 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309397265151307\n",
            "Epoch: 605 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309397260784666\n",
            "Epoch: 606 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093973088177133\n",
            "Epoch: 607 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309397335017557\n",
            "Epoch: 608 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309397278251229\n",
            "Epoch: 609 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093973131843537\n",
            "Epoch: 610 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.30939728698451\n",
            "Epoch: 611 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093973105643695\n",
            "Epoch: 612 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.30939727301126\n",
            "Epoch: 613 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093972703912757\n",
            "Epoch: 614 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309397294407799\n",
            "Epoch: 615 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093972769412368\n",
            "Epoch: 616 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093972638413147\n",
            "Epoch: 617 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309397272574596\n",
            "Epoch: 618 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309397294407799\n",
            "Epoch: 619 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309397252488049\n",
            "Epoch: 620 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093972900411583\n",
            "Epoch: 621 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309397278687893\n",
            "Epoch: 622 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093972280348614\n",
            "Epoch: 623 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309397260784666\n",
            "Epoch: 624 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.30939725903801\n",
            "Epoch: 625 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309397272137932\n",
            "Epoch: 626 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093972284715254\n",
            "Epoch: 627 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093972472480804\n",
            "Epoch: 628 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093972341481583\n",
            "Epoch: 629 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309397217554924\n",
            "Epoch: 630 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309397198341705\n",
            "Epoch: 631 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309397178255158\n",
            "Epoch: 632 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093971957217208\n",
            "Epoch: 633 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.30939719135508\n",
            "Epoch: 634 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093971826217987\n",
            "Epoch: 635 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309397183931791\n",
            "Epoch: 636 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309397178255158\n",
            "Epoch: 637 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093971751985096\n",
            "Epoch: 638 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309397157731947\n",
            "Epoch: 639 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309397192665072\n",
            "Epoch: 640 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093971489986655\n",
            "Epoch: 641 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093971690852126\n",
            "Epoch: 642 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309397210131635\n",
            "Epoch: 643 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.30939717956515\n",
            "Epoch: 644 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309397136772072\n",
            "Epoch: 645 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093971551119625\n",
            "Epoch: 646 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093971420120405\n",
            "Epoch: 647 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093971551119625\n",
            "Epoch: 648 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093971559852906\n",
            "Epoch: 649 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093971813118066\n",
            "Epoch: 650 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309397183931791\n",
            "Epoch: 651 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093971690852126\n",
            "Epoch: 652 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093971555486266\n",
            "Epoch: 653 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093971468153454\n",
            "Epoch: 654 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309397124982142\n",
            "Epoch: 655 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309397090049017\n",
            "Epoch: 656 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309397138082064\n",
            "Epoch: 657 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309397103585603\n",
            "Epoch: 658 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309397097035642\n",
            "Epoch: 659 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093970795690795\n",
            "Epoch: 660 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309397097035642\n",
            "Epoch: 661 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093971053322595\n",
            "Epoch: 662 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093970856823764\n",
            "Epoch: 663 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309397138082064\n",
            "Epoch: 664 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093970660324934\n",
            "Epoch: 665 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093971206155017\n",
            "Epoch: 666 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093970660324934\n",
            "Epoch: 667 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093971096989\n",
            "Epoch: 668 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.30939704419929\n",
            "Epoch: 669 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093970310993686\n",
            "Epoch: 670 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093970520592433\n",
            "Epoch: 671 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.30939704419929\n",
            "Epoch: 672 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093970398326498\n",
            "Epoch: 673 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093970529325714\n",
            "Epoch: 674 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309397028479384\n",
            "Epoch: 675 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309397035466009\n",
            "Epoch: 676 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093970267327277\n",
            "Epoch: 677 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309397035466009\n",
            "Epoch: 678 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093970660324934\n",
            "Epoch: 679 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093971140655407\n",
            "Epoch: 680 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093970267327277\n",
            "Epoch: 681 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093970179994465\n",
            "Epoch: 682 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093970070828447\n",
            "Epoch: 683 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093970088295013\n",
            "Epoch: 684 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093970131961417\n",
            "Epoch: 685 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093970201827667\n",
            "Epoch: 686 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093969957295792\n",
            "Epoch: 687 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093969931095946\n",
            "Epoch: 688 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396969529735\n",
            "Epoch: 689 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093970044628604\n",
            "Epoch: 690 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.30939702804272\n",
            "Epoch: 691 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309397062975845\n",
            "Epoch: 692 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093970044628604\n",
            "Epoch: 693 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093970393959857\n",
            "Epoch: 694 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093970083928372\n",
            "Epoch: 695 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093970175627825\n",
            "Epoch: 696 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396986996298\n",
            "Epoch: 697 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093970044628604\n",
            "Epoch: 698 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093969913629384\n",
            "Epoch: 699 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396989179618\n",
            "Epoch: 700 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396986996298\n",
            "Epoch: 701 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396988742954\n",
            "Epoch: 702 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093970083928372\n",
            "Epoch: 703 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093970000962196\n",
            "Epoch: 704 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093970062095166\n",
            "Epoch: 705 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093970131961417\n",
            "Epoch: 706 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093970070828447\n",
            "Epoch: 707 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396989179618\n",
            "Epoch: 708 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309397001842876\n",
            "Epoch: 709 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093969498798526\n",
            "Epoch: 710 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396976079696\n",
            "Epoch: 711 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093969498798526\n",
            "Epoch: 712 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396971713056\n",
            "Epoch: 713 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396961233118\n",
            "Epoch: 714 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396973459712\n",
            "Epoch: 715 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396962979774\n",
            "Epoch: 716 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396995292915\n",
            "Epoch: 717 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396989179618\n",
            "Epoch: 718 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396980446337\n",
            "Epoch: 719 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093969297933055\n",
            "Epoch: 720 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396973459712\n",
            "Epoch: 721 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396947259868\n",
            "Epoch: 722 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093969297933055\n",
            "Epoch: 723 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093969341599463\n",
            "Epoch: 724 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093969341599463\n",
            "Epoch: 725 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396955993149\n",
            "Epoch: 726 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396986559634\n",
            "Epoch: 727 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093969254266646\n",
            "Epoch: 728 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093969586131338\n",
            "Epoch: 729 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396955993149\n",
            "Epoch: 730 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093969254266646\n",
            "Epoch: 731 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396906213446\n",
            "Epoch: 732 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396906213446\n",
            "Epoch: 733 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396880013602\n",
            "Epoch: 734 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396941146571\n",
            "Epoch: 735 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093969105800864\n",
            "Epoch: 736 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093969018468052\n",
            "Epoch: 737 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396897480165\n",
            "Epoch: 738 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396965599759\n",
            "Epoch: 739 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396897480165\n",
            "Epoch: 740 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.30939690446679\n",
            "Epoch: 741 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.30939690446679\n",
            "Epoch: 742 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093969586131338\n",
            "Epoch: 743 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093968957335083\n",
            "Epoch: 744 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396906213446\n",
            "Epoch: 745 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093969149467273\n",
            "Epoch: 746 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093969105800864\n",
            "Epoch: 747 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093969385265867\n",
            "Epoch: 748 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396888746883\n",
            "Epoch: 749 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396888746883\n",
            "Epoch: 750 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093969236800085\n",
            "Epoch: 751 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396893113524\n",
            "Epoch: 752 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396893113524\n",
            "Epoch: 753 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396961233118\n",
            "Epoch: 754 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093969306666335\n",
            "Epoch: 755 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396913200071\n",
            "Epoch: 756 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396952499837\n",
            "Epoch: 757 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396888746883\n",
            "Epoch: 758 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396831980555\n",
            "Epoch: 759 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093968826335867\n",
            "Epoch: 760 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396913200071\n",
            "Epoch: 761 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093969437665556\n",
            "Epoch: 762 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093969393999147\n",
            "Epoch: 763 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093969306666335\n",
            "Epoch: 764 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093968215006173\n",
            "Epoch: 765 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396891366868\n",
            "Epoch: 766 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093968084006957\n",
            "Epoch: 767 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093969018468052\n",
            "Epoch: 768 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.30939690446679\n",
            "Epoch: 769 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093969035934614\n",
            "Epoch: 770 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093968642936957\n",
            "Epoch: 771 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093968555604145\n",
            "Epoch: 772 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093968669136804\n",
            "Epoch: 773 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396886126899\n",
            "Epoch: 774 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093967948641096\n",
            "Epoch: 775 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396871716985\n",
            "Epoch: 776 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396797484094\n",
            "Epoch: 777 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396823683938\n",
            "Epoch: 778 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093968673503444\n",
            "Epoch: 779 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093967931174535\n",
            "Epoch: 780 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093968542504224\n",
            "Epoch: 781 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093968542504224\n",
            "Epoch: 782 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093967931174535\n",
            "Epoch: 783 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396816697313\n",
            "Epoch: 784 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093968367838595\n",
            "Epoch: 785 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396825867258\n",
            "Epoch: 786 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093968367838595\n",
            "Epoch: 787 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093968346005393\n",
            "Epoch: 788 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093967931174535\n",
            "Epoch: 789 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093967931174535\n",
            "Epoch: 790 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396758184328\n",
            "Epoch: 791 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093967472677264\n",
            "Epoch: 792 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093967062213046\n",
            "Epoch: 793 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093967787075393\n",
            "Epoch: 794 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396731984484\n",
            "Epoch: 795 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093967145179217\n",
            "Epoch: 796 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093967538176874\n",
            "Epoch: 797 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396731984484\n",
            "Epoch: 798 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093967014179997\n",
            "Epoch: 799 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396710151281\n",
            "Epoch: 800 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396716701242\n",
            "Epoch: 801 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396734604469\n",
            "Epoch: 802 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396723251203\n",
            "Epoch: 803 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093966953047027\n",
            "Epoch: 804 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396721504547\n",
            "Epoch: 805 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093967127712656\n",
            "Epoch: 806 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093967018546637\n",
            "Epoch: 807 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093967018546637\n",
            "Epoch: 808 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396682641445\n",
            "Epoch: 809 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093966913747264\n",
            "Epoch: 810 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093967132079296\n",
            "Epoch: 811 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093967132079296\n",
            "Epoch: 812 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093966870080855\n",
            "Epoch: 813 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093967001080076\n",
            "Epoch: 814 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396708841289\n",
            "Epoch: 815 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093967263078516\n",
            "Epoch: 816 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396743774414\n",
            "Epoch: 817 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093967044746484\n",
            "Epoch: 818 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093966913747264\n",
            "Epoch: 819 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.30939671757457\n",
            "Epoch: 820 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093967001080076\n",
            "Epoch: 821 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093966961780312\n",
            "Epoch: 822 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093967044746484\n",
            "Epoch: 823 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093967114612735\n",
            "Epoch: 824 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093967044746484\n",
            "Epoch: 825 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093967066579686\n",
            "Epoch: 826 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093967267445157\n",
            "Epoch: 827 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396735477797\n",
            "Epoch: 828 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093967118979375\n",
            "Epoch: 829 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093966987980155\n",
            "Epoch: 830 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093966944313746\n",
            "Epoch: 831 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396741591094\n",
            "Epoch: 832 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396680458125\n",
            "Epoch: 833 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396666048211\n",
            "Epoch: 834 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093966835147732\n",
            "Epoch: 835 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396670414851\n",
            "Epoch: 836 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396674781492\n",
            "Epoch: 837 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093966878814136\n",
            "Epoch: 838 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093966529482888\n",
            "Epoch: 839 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396666048211\n",
            "Epoch: 840 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396659934914\n",
            "Epoch: 841 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093966878814136\n",
            "Epoch: 842 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396681768117\n",
            "Epoch: 843 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093966774014762\n",
            "Epoch: 844 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396670414851\n",
            "Epoch: 845 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396668668195\n",
            "Epoch: 846 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396613648523\n",
            "Epoch: 847 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396657751593\n",
            "Epoch: 848 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093966402850308\n",
            "Epoch: 849 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093966359183904\n",
            "Epoch: 850 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093966315517496\n",
            "Epoch: 851 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093966734715\n",
            "Epoch: 852 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093966647382183\n",
            "Epoch: 853 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396634171734\n",
            "Epoch: 854 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396660371578\n",
            "Epoch: 855 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396656004937\n",
            "Epoch: 856 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093966647382183\n",
            "Epoch: 857 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396575222085\n",
            "Epoch: 858 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396640721695\n",
            "Epoch: 859 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093965970552883\n",
            "Epoch: 860 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396627621773\n",
            "Epoch: 861 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396640721695\n",
            "Epoch: 862 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093965900686633\n",
            "Epoch: 863 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093966250017885\n",
            "Epoch: 864 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396614521851\n",
            "Epoch: 865 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093965909419913\n",
            "Epoch: 866 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.30939658220871\n",
            "Epoch: 867 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396586575351\n",
            "Epoch: 868 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093965909419913\n",
            "Epoch: 869 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396595308632\n",
            "Epoch: 870 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396593125312\n",
            "Epoch: 871 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396575658749\n",
            "Epoch: 872 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396566925468\n",
            "Epoch: 873 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093965839553667\n",
            "Epoch: 874 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093965843920308\n",
            "Epoch: 875 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093966044785774\n",
            "Epoch: 876 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396601858593\n",
            "Epoch: 877 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396591378656\n",
            "Epoch: 878 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396600111937\n",
            "Epoch: 879 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396589195335\n",
            "Epoch: 880 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396606661898\n",
            "Epoch: 881 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093966092818823\n",
            "Epoch: 882 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396593561976\n",
            "Epoch: 883 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396578715398\n",
            "Epoch: 884 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093966311150855\n",
            "Epoch: 885 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093966092818823\n",
            "Epoch: 886 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093966092818823\n",
            "Epoch: 887 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396578715398\n",
            "Epoch: 888 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093965568821946\n",
            "Epoch: 889 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093966049152415\n",
            "Epoch: 890 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093965638688196\n",
            "Epoch: 891 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093965507688976\n",
            "Epoch: 892 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396585702023\n",
            "Epoch: 893 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093965704187807\n",
            "Epoch: 894 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093966053519055\n",
            "Epoch: 895 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093965573188586\n",
            "Epoch: 896 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093965835187023\n",
            "Epoch: 897 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396581772046\n",
            "Epoch: 898 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093965769687417\n",
            "Epoch: 899 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093965577555227\n",
            "Epoch: 900 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396559502179\n",
            "Epoch: 901 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093965507688976\n",
            "Epoch: 902 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093965621221635\n",
            "Epoch: 903 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093965507688976\n",
            "Epoch: 904 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396566488804\n",
            "Epoch: 905 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396525442382\n",
            "Epoch: 906 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396514089116\n",
            "Epoch: 907 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093965385423036\n",
            "Epoch: 908 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093965385423036\n",
            "Epoch: 909 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093965450922647\n",
            "Epoch: 910 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093965581921867\n",
            "Epoch: 911 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093965407256243\n",
            "Epoch: 912 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093965712921087\n",
            "Epoch: 913 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093965625588275\n",
            "Epoch: 914 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093965276257022\n",
            "Epoch: 915 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.30939651015914\n",
            "Epoch: 916 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093965306823505\n",
            "Epoch: 917 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.30939651234246\n",
            "Epoch: 918 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093964739160224\n",
            "Epoch: 919 \tTraining Loss: 2.309396 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396513215788\n",
            "Epoch: 920 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396478282663\n",
            "Epoch: 921 \tTraining Loss: 2.309396 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396491382585\n",
            "Epoch: 922 \tTraining Loss: 2.309396 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093964804659834\n",
            "Epoch: 923 \tTraining Loss: 2.309396 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093964739160224\n",
            "Epoch: 924 \tTraining Loss: 2.309396 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396478719327\n",
            "Epoch: 925 \tTraining Loss: 2.309396 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093964743526865\n",
            "Epoch: 926 \tTraining Loss: 2.309396 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396504919171\n",
            "Epoch: 927 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093964874526085\n",
            "Epoch: 928 \tTraining Loss: 2.309396 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093965223857333\n",
            "Epoch: 929 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093964721693663\n",
            "Epoch: 930 \tTraining Loss: 2.309396 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396443786202\n",
            "Epoch: 931 \tTraining Loss: 2.309396 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.30939642850296\n",
            "Epoch: 932 \tTraining Loss: 2.309396 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396450336163\n",
            "Epoch: 933 \tTraining Loss: 2.309396 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093964547028034\n",
            "Epoch: 934 \tTraining Loss: 2.309396 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396435489585\n",
            "Epoch: 935 \tTraining Loss: 2.309396 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.30939642850296\n",
            "Epoch: 936 \tTraining Loss: 2.309396 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093964416028814\n",
            "Epoch: 937 \tTraining Loss: 2.309396 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396424136319\n",
            "Epoch: 938 \tTraining Loss: 2.309396 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.30939642850296\n",
            "Epoch: 939 \tTraining Loss: 2.309396 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396418023022\n",
            "Epoch: 940 \tTraining Loss: 2.309396 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396428939624\n",
            "Epoch: 941 \tTraining Loss: 2.309396 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093964071064206\n",
            "Epoch: 942 \tTraining Loss: 2.309396 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093964464061862\n",
            "Epoch: 943 \tTraining Loss: 2.309396 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396451646155\n",
            "Epoch: 944 \tTraining Loss: 2.309396 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396437672905\n",
            "Epoch: 945 \tTraining Loss: 2.309396 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396435926249\n",
            "Epoch: 946 \tTraining Loss: 2.309396 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093964494628345\n",
            "Epoch: 947 \tTraining Loss: 2.309396 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093964014297876\n",
            "Epoch: 948 \tTraining Loss: 2.309396 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093964712960378\n",
            "Epoch: 949 \tTraining Loss: 2.309396 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093964407295533\n",
            "Epoch: 950 \tTraining Loss: 2.309396 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093964712960378\n",
            "Epoch: 951 \tTraining Loss: 2.309396 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093964407295533\n",
            "Epoch: 952 \tTraining Loss: 2.309396 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093964739160224\n",
            "Epoch: 953 \tTraining Loss: 2.309396 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396436799577\n",
            "Epoch: 954 \tTraining Loss: 2.309396 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396432432936\n",
            "Epoch: 955 \tTraining Loss: 2.309396 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396449899499\n",
            "Epoch: 956 \tTraining Loss: 2.309396 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396432432936\n",
            "Epoch: 957 \tTraining Loss: 2.309396 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093964721693663\n",
            "Epoch: 958 \tTraining Loss: 2.309396 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093963996831315\n",
            "Epoch: 959 \tTraining Loss: 2.309396 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093964023031157\n",
            "Epoch: 960 \tTraining Loss: 2.309396 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093964197696786\n",
            "Epoch: 961 \tTraining Loss: 2.309396 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396437236241\n",
            "Epoch: 962 \tTraining Loss: 2.309396 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093964839592958\n",
            "Epoch: 963 \tTraining Loss: 2.309396 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.30939644465953\n",
            "Epoch: 964 \tTraining Loss: 2.309396 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093964132197176\n",
            "Epoch: 965 \tTraining Loss: 2.309396 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396418459686\n",
            "Epoch: 966 \tTraining Loss: 2.309396 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093963791599204\n",
            "Epoch: 967 \tTraining Loss: 2.309396 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.30939644465953\n",
            "Epoch: 968 \tTraining Loss: 2.309396 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093963730466234\n",
            "Epoch: 969 \tTraining Loss: 2.309396 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093964468428503\n",
            "Epoch: 970 \tTraining Loss: 2.309396 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093964337429282\n",
            "Epoch: 971 \tTraining Loss: 2.309396 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.30939644247621\n",
            "Epoch: 972 \tTraining Loss: 2.309396 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396436362913\n",
            "Epoch: 973 \tTraining Loss: 2.309396 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396403176444\n",
            "Epoch: 974 \tTraining Loss: 2.309396 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396397063147\n",
            "Epoch: 975 \tTraining Loss: 2.309396 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396429376288\n",
            "Epoch: 976 \tTraining Loss: 2.309396 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396397063147\n",
            "Epoch: 977 \tTraining Loss: 2.309396 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093963992464674\n",
            "Epoch: 978 \tTraining Loss: 2.309396 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093964062330925\n",
            "Epoch: 979 \tTraining Loss: 2.309396 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093964088530767\n",
            "Epoch: 980 \tTraining Loss: 2.309396 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396500115866\n",
            "Epoch: 981 \tTraining Loss: 2.309397 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093963822165686\n",
            "Epoch: 982 \tTraining Loss: 2.309396 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093963865832094\n",
            "Epoch: 983 \tTraining Loss: 2.309396 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093963822165686\n",
            "Epoch: 984 \tTraining Loss: 2.309396 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093963385501626\n",
            "Epoch: 985 \tTraining Loss: 2.309396 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093963940064985\n",
            "Epoch: 986 \tTraining Loss: 2.309396 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396411473061\n",
            "Epoch: 987 \tTraining Loss: 2.309396 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093964027397798\n",
            "Epoch: 988 \tTraining Loss: 2.309396 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093963721732953\n",
            "Epoch: 989 \tTraining Loss: 2.309396 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093964202063426\n",
            "Epoch: 990 \tTraining Loss: 2.309396 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093963721732953\n",
            "Epoch: 991 \tTraining Loss: 2.309396 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396411473061\n",
            "Epoch: 992 \tTraining Loss: 2.309396 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396418023022\n",
            "Epoch: 993 \tTraining Loss: 2.309396 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093963961898187\n",
            "Epoch: 994 \tTraining Loss: 2.309396 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396438109569\n",
            "Epoch: 995 \tTraining Loss: 2.309396 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.309396355143397\n",
            "Epoch: 996 \tTraining Loss: 2.309396 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093964009931236\n",
            "Epoch: 997 \tTraining Loss: 2.309396 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093963206469357\n",
            "Epoch: 998 \tTraining Loss: 2.309396 \tValidation Loss: 0.060605\n",
            "train_loss:\n",
            "2.3093963468467797\n",
            "Epoch: 999 \tTraining Loss: 2.309396 \tValidation Loss: 0.060605\n",
            "---------------------------------------------\n",
            "Test Loss: 3.757222\n",
            "\n",
            "---------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "De igual manera se aproximan los valores para graficar"
      ],
      "metadata": {
        "id": "JAjbl4blBiG2"
      },
      "id": "JAjbl4blBiG2"
    },
    {
      "cell_type": "code",
      "source": [
        "lt = []\n",
        "for i in loss_test:\n",
        "  redon= round(float(i),3)\n",
        "  lt.append(redon)\n",
        "plt.plot(lr,lt)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "0vyZJUn922Bn",
        "outputId": "4d13d335-252d-4d7b-dc76-79c27b3b36f1"
      },
      "id": "0vyZJUn922Bn",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "De ambas gráficas se puede concluir que se va a tener un menor test loss con un learning rate pequeño y con 1000 epochs. Ahora entrenemos la red de esa manera para ver cuanto cambia el accuracy."
      ],
      "metadata": {
        "id": "UU50YCL8litg"
      },
      "id": "UU50YCL8litg"
    },
    {
      "cell_type": "code",
      "source": [
        "model=Net()\n",
        "optimizer=torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "train_model(model,optimizer,criterion,train_data,val_data, 1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49NvFKnbl0BB",
        "outputId": "0578598f-3fae-4ae4-afc4-83506c337dcc"
      },
      "id": "49NvFKnbl0BB",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_loss:\n",
            "1.124303156202966\n",
            "Epoch: 0 \tTraining Loss: 1.124303 \tValidation Loss: 0.024352\n",
            "Validation loss decreased (inf --> 0.024352).  Saving model ...\n",
            "train_loss:\n",
            "1.1136695654182645\n",
            "Epoch: 1 \tTraining Loss: 1.113670 \tValidation Loss: 0.024317\n",
            "Validation loss decreased (0.024352 --> 0.024317).  Saving model ...\n",
            "train_loss:\n",
            "1.1088750070922977\n",
            "Epoch: 2 \tTraining Loss: 1.108875 \tValidation Loss: 0.024301\n",
            "Validation loss decreased (0.024317 --> 0.024301).  Saving model ...\n",
            "train_loss:\n",
            "1.1046901109454397\n",
            "Epoch: 3 \tTraining Loss: 1.104690 \tValidation Loss: 0.024296\n",
            "Validation loss decreased (0.024301 --> 0.024296).  Saving model ...\n",
            "train_loss:\n",
            "1.101008448626969\n",
            "Epoch: 4 \tTraining Loss: 1.101008 \tValidation Loss: 0.024304\n",
            "train_loss:\n",
            "1.097786953488549\n",
            "Epoch: 5 \tTraining Loss: 1.097787 \tValidation Loss: 0.024321\n",
            "train_loss:\n",
            "1.0950001016422943\n",
            "Epoch: 6 \tTraining Loss: 1.095000 \tValidation Loss: 0.024345\n",
            "train_loss:\n",
            "1.0926191007697976\n",
            "Epoch: 7 \tTraining Loss: 1.092619 \tValidation Loss: 0.024374\n",
            "train_loss:\n",
            "1.0906054279306432\n",
            "Epoch: 8 \tTraining Loss: 1.090605 \tValidation Loss: 0.024403\n",
            "train_loss:\n",
            "1.0889123602882846\n",
            "Epoch: 9 \tTraining Loss: 1.088912 \tValidation Loss: 0.024431\n",
            "train_loss:\n",
            "1.0874899003531906\n",
            "Epoch: 10 \tTraining Loss: 1.087490 \tValidation Loss: 0.024457\n",
            "train_loss:\n",
            "1.0862896694586828\n",
            "Epoch: 11 \tTraining Loss: 1.086290 \tValidation Loss: 0.024481\n",
            "train_loss:\n",
            "1.0852682875407922\n",
            "Epoch: 12 \tTraining Loss: 1.085268 \tValidation Loss: 0.024501\n",
            "train_loss:\n",
            "1.0843888373820336\n",
            "Epoch: 13 \tTraining Loss: 1.084389 \tValidation Loss: 0.024518\n",
            "train_loss:\n",
            "1.0836209743232517\n",
            "Epoch: 14 \tTraining Loss: 1.083621 \tValidation Loss: 0.024532\n",
            "train_loss:\n",
            "1.0829403053273212\n",
            "Epoch: 15 \tTraining Loss: 1.082940 \tValidation Loss: 0.024544\n",
            "train_loss:\n",
            "1.0823274716571136\n",
            "Epoch: 16 \tTraining Loss: 1.082327 \tValidation Loss: 0.024553\n",
            "train_loss:\n",
            "1.0817671971661704\n",
            "Epoch: 17 \tTraining Loss: 1.081767 \tValidation Loss: 0.024561\n",
            "train_loss:\n",
            "1.0812475013208913\n",
            "Epoch: 18 \tTraining Loss: 1.081248 \tValidation Loss: 0.024567\n",
            "train_loss:\n",
            "1.0807589705173786\n",
            "Epoch: 19 \tTraining Loss: 1.080759 \tValidation Loss: 0.024571\n",
            "train_loss:\n",
            "1.0802943254565145\n",
            "Epoch: 20 \tTraining Loss: 1.080294 \tValidation Loss: 0.024575\n",
            "train_loss:\n",
            "1.079848003911448\n",
            "Epoch: 21 \tTraining Loss: 1.079848 \tValidation Loss: 0.024577\n",
            "train_loss:\n",
            "1.0794160041835281\n",
            "Epoch: 22 \tTraining Loss: 1.079416 \tValidation Loss: 0.024579\n",
            "train_loss:\n",
            "1.0789956398062654\n",
            "Epoch: 23 \tTraining Loss: 1.078996 \tValidation Loss: 0.024581\n",
            "train_loss:\n",
            "1.078585493695605\n",
            "Epoch: 24 \tTraining Loss: 1.078585 \tValidation Loss: 0.024581\n",
            "train_loss:\n",
            "1.07818521968611\n",
            "Epoch: 25 \tTraining Loss: 1.078185 \tValidation Loss: 0.024580\n",
            "train_loss:\n",
            "1.0777952864274873\n",
            "Epoch: 26 \tTraining Loss: 1.077795 \tValidation Loss: 0.024579\n",
            "train_loss:\n",
            "1.0774166557815048\n",
            "Epoch: 27 \tTraining Loss: 1.077417 \tValidation Loss: 0.024576\n",
            "train_loss:\n",
            "1.0770503718119402\n",
            "Epoch: 28 \tTraining Loss: 1.077050 \tValidation Loss: 0.024571\n",
            "train_loss:\n",
            "1.0766972555564\n",
            "Epoch: 29 \tTraining Loss: 1.076697 \tValidation Loss: 0.024566\n",
            "train_loss:\n",
            "1.0763576747951926\n",
            "Epoch: 30 \tTraining Loss: 1.076358 \tValidation Loss: 0.024559\n",
            "train_loss:\n",
            "1.0760315709061674\n",
            "Epoch: 31 \tTraining Loss: 1.076032 \tValidation Loss: 0.024552\n",
            "train_loss:\n",
            "1.075718527311807\n",
            "Epoch: 32 \tTraining Loss: 1.075719 \tValidation Loss: 0.024543\n",
            "train_loss:\n",
            "1.0754179103033883\n",
            "Epoch: 33 \tTraining Loss: 1.075418 \tValidation Loss: 0.024534\n",
            "train_loss:\n",
            "1.0751289938177382\n",
            "Epoch: 34 \tTraining Loss: 1.075129 \tValidation Loss: 0.024525\n",
            "train_loss:\n",
            "1.0748510494991974\n",
            "Epoch: 35 \tTraining Loss: 1.074851 \tValidation Loss: 0.024516\n",
            "train_loss:\n",
            "1.0745833892743666\n",
            "Epoch: 36 \tTraining Loss: 1.074583 \tValidation Loss: 0.024506\n",
            "train_loss:\n",
            "1.0743253958094252\n",
            "Epoch: 37 \tTraining Loss: 1.074325 \tValidation Loss: 0.024496\n",
            "train_loss:\n",
            "1.0740764697829446\n",
            "Epoch: 38 \tTraining Loss: 1.074076 \tValidation Loss: 0.024487\n",
            "train_loss:\n",
            "1.073836088180542\n",
            "Epoch: 39 \tTraining Loss: 1.073836 \tValidation Loss: 0.024477\n",
            "train_loss:\n",
            "1.0736037103029399\n",
            "Epoch: 40 \tTraining Loss: 1.073604 \tValidation Loss: 0.024468\n",
            "train_loss:\n",
            "1.0733788219782023\n",
            "Epoch: 41 \tTraining Loss: 1.073379 \tValidation Loss: 0.024459\n",
            "train_loss:\n",
            "1.0731609355617355\n",
            "Epoch: 42 \tTraining Loss: 1.073161 \tValidation Loss: 0.024451\n",
            "train_loss:\n",
            "1.0729495486715337\n",
            "Epoch: 43 \tTraining Loss: 1.072950 \tValidation Loss: 0.024443\n",
            "train_loss:\n",
            "1.07274418086796\n",
            "Epoch: 44 \tTraining Loss: 1.072744 \tValidation Loss: 0.024435\n",
            "train_loss:\n",
            "1.0725444109885247\n",
            "Epoch: 45 \tTraining Loss: 1.072544 \tValidation Loss: 0.024428\n",
            "train_loss:\n",
            "1.072349801495835\n",
            "Epoch: 46 \tTraining Loss: 1.072350 \tValidation Loss: 0.024422\n",
            "train_loss:\n",
            "1.07216001866938\n",
            "Epoch: 47 \tTraining Loss: 1.072160 \tValidation Loss: 0.024416\n",
            "train_loss:\n",
            "1.0719747648134337\n",
            "Epoch: 48 \tTraining Loss: 1.071975 \tValidation Loss: 0.024410\n",
            "train_loss:\n",
            "1.0717937821870323\n",
            "Epoch: 49 \tTraining Loss: 1.071794 \tValidation Loss: 0.024406\n",
            "train_loss:\n",
            "1.0716169397909563\n",
            "Epoch: 50 \tTraining Loss: 1.071617 \tValidation Loss: 0.024401\n",
            "train_loss:\n",
            "1.071444116450928\n",
            "Epoch: 51 \tTraining Loss: 1.071444 \tValidation Loss: 0.024397\n",
            "train_loss:\n",
            "1.0712752548547892\n",
            "Epoch: 52 \tTraining Loss: 1.071275 \tValidation Loss: 0.024394\n",
            "train_loss:\n",
            "1.071110344195104\n",
            "Epoch: 53 \tTraining Loss: 1.071110 \tValidation Loss: 0.024390\n",
            "train_loss:\n",
            "1.0709493566345383\n",
            "Epoch: 54 \tTraining Loss: 1.070949 \tValidation Loss: 0.024387\n",
            "train_loss:\n",
            "1.0707923059280102\n",
            "Epoch: 55 \tTraining Loss: 1.070792 \tValidation Loss: 0.024384\n",
            "train_loss:\n",
            "1.0706391694781545\n",
            "Epoch: 56 \tTraining Loss: 1.070639 \tValidation Loss: 0.024381\n",
            "train_loss:\n",
            "1.070489892920295\n",
            "Epoch: 57 \tTraining Loss: 1.070490 \tValidation Loss: 0.024378\n",
            "train_loss:\n",
            "1.0703444199247674\n",
            "Epoch: 58 \tTraining Loss: 1.070344 \tValidation Loss: 0.024375\n",
            "train_loss:\n",
            "1.0702026129423916\n",
            "Epoch: 59 \tTraining Loss: 1.070203 \tValidation Loss: 0.024372\n",
            "train_loss:\n",
            "1.070064345231423\n",
            "Epoch: 60 \tTraining Loss: 1.070064 \tValidation Loss: 0.024368\n",
            "train_loss:\n",
            "1.0699293954031808\n",
            "Epoch: 61 \tTraining Loss: 1.069929 \tValidation Loss: 0.024364\n",
            "train_loss:\n",
            "1.069797573181299\n",
            "Epoch: 62 \tTraining Loss: 1.069798 \tValidation Loss: 0.024361\n",
            "train_loss:\n",
            "1.0696686565221012\n",
            "Epoch: 63 \tTraining Loss: 1.069669 \tValidation Loss: 0.024357\n",
            "train_loss:\n",
            "1.0695423971820663\n",
            "Epoch: 64 \tTraining Loss: 1.069542 \tValidation Loss: 0.024353\n",
            "train_loss:\n",
            "1.0694185518301451\n",
            "Epoch: 65 \tTraining Loss: 1.069419 \tValidation Loss: 0.024348\n",
            "train_loss:\n",
            "1.0692968715678204\n",
            "Epoch: 66 \tTraining Loss: 1.069297 \tValidation Loss: 0.024344\n",
            "train_loss:\n",
            "1.0691771251814706\n",
            "Epoch: 67 \tTraining Loss: 1.069177 \tValidation Loss: 0.024339\n",
            "train_loss:\n",
            "1.069059091937411\n",
            "Epoch: 68 \tTraining Loss: 1.069059 \tValidation Loss: 0.024335\n",
            "train_loss:\n",
            "1.0689425553594316\n",
            "Epoch: 69 \tTraining Loss: 1.068943 \tValidation Loss: 0.024330\n",
            "train_loss:\n",
            "1.0688273291011432\n",
            "Epoch: 70 \tTraining Loss: 1.068827 \tValidation Loss: 0.024325\n",
            "train_loss:\n",
            "1.068713217973709\n",
            "Epoch: 71 \tTraining Loss: 1.068713 \tValidation Loss: 0.024321\n",
            "train_loss:\n",
            "1.0686000706730308\n",
            "Epoch: 72 \tTraining Loss: 1.068600 \tValidation Loss: 0.024316\n",
            "train_loss:\n",
            "1.0684877539074027\n",
            "Epoch: 73 \tTraining Loss: 1.068488 \tValidation Loss: 0.024311\n",
            "train_loss:\n",
            "1.0683761301276449\n",
            "Epoch: 74 \tTraining Loss: 1.068376 \tValidation Loss: 0.024307\n",
            "train_loss:\n",
            "1.0682650968268677\n",
            "Epoch: 75 \tTraining Loss: 1.068265 \tValidation Loss: 0.024302\n",
            "train_loss:\n",
            "1.068154567873085\n",
            "Epoch: 76 \tTraining Loss: 1.068155 \tValidation Loss: 0.024298\n",
            "train_loss:\n",
            "1.068044480059173\n",
            "Epoch: 77 \tTraining Loss: 1.068044 \tValidation Loss: 0.024293\n",
            "Validation loss decreased (0.024296 --> 0.024293).  Saving model ...\n",
            "train_loss:\n",
            "1.0679347750904795\n",
            "Epoch: 78 \tTraining Loss: 1.067935 \tValidation Loss: 0.024289\n",
            "Validation loss decreased (0.024293 --> 0.024289).  Saving model ...\n",
            "train_loss:\n",
            "1.0678253995848226\n",
            "Epoch: 79 \tTraining Loss: 1.067825 \tValidation Loss: 0.024285\n",
            "Validation loss decreased (0.024289 --> 0.024285).  Saving model ...\n",
            "train_loss:\n",
            "1.0677163509222178\n",
            "Epoch: 80 \tTraining Loss: 1.067716 \tValidation Loss: 0.024281\n",
            "Validation loss decreased (0.024285 --> 0.024281).  Saving model ...\n",
            "train_loss:\n",
            "1.0676076002828367\n",
            "Epoch: 81 \tTraining Loss: 1.067608 \tValidation Loss: 0.024276\n",
            "Validation loss decreased (0.024281 --> 0.024276).  Saving model ...\n",
            "train_loss:\n",
            "1.0674991781239982\n",
            "Epoch: 82 \tTraining Loss: 1.067499 \tValidation Loss: 0.024272\n",
            "Validation loss decreased (0.024276 --> 0.024272).  Saving model ...\n",
            "train_loss:\n",
            "1.0673910837907057\n",
            "Epoch: 83 \tTraining Loss: 1.067391 \tValidation Loss: 0.024268\n",
            "Validation loss decreased (0.024272 --> 0.024268).  Saving model ...\n",
            "train_loss:\n",
            "1.0672833546177372\n",
            "Epoch: 84 \tTraining Loss: 1.067283 \tValidation Loss: 0.024264\n",
            "Validation loss decreased (0.024268 --> 0.024264).  Saving model ...\n",
            "train_loss:\n",
            "1.0671760331798386\n",
            "Epoch: 85 \tTraining Loss: 1.067176 \tValidation Loss: 0.024260\n",
            "Validation loss decreased (0.024264 --> 0.024260).  Saving model ...\n",
            "train_loss:\n",
            "1.0670691604142661\n",
            "Epoch: 86 \tTraining Loss: 1.067069 \tValidation Loss: 0.024257\n",
            "Validation loss decreased (0.024260 --> 0.024257).  Saving model ...\n",
            "train_loss:\n",
            "1.0669628018206292\n",
            "Epoch: 87 \tTraining Loss: 1.066963 \tValidation Loss: 0.024253\n",
            "Validation loss decreased (0.024257 --> 0.024253).  Saving model ...\n",
            "train_loss:\n",
            "1.0668570274835105\n",
            "Epoch: 88 \tTraining Loss: 1.066857 \tValidation Loss: 0.024249\n",
            "Validation loss decreased (0.024253 --> 0.024249).  Saving model ...\n",
            "train_loss:\n",
            "1.0667519042125115\n",
            "Epoch: 89 \tTraining Loss: 1.066752 \tValidation Loss: 0.024245\n",
            "Validation loss decreased (0.024249 --> 0.024245).  Saving model ...\n",
            "train_loss:\n",
            "1.0666475148646386\n",
            "Epoch: 90 \tTraining Loss: 1.066648 \tValidation Loss: 0.024241\n",
            "Validation loss decreased (0.024245 --> 0.024241).  Saving model ...\n",
            "train_loss:\n",
            "1.0665439472093687\n",
            "Epoch: 91 \tTraining Loss: 1.066544 \tValidation Loss: 0.024238\n",
            "Validation loss decreased (0.024241 --> 0.024238).  Saving model ...\n",
            "train_loss:\n",
            "1.0664412834487118\n",
            "Epoch: 92 \tTraining Loss: 1.066441 \tValidation Loss: 0.024234\n",
            "Validation loss decreased (0.024238 --> 0.024234).  Saving model ...\n",
            "train_loss:\n",
            "1.0663396188846002\n",
            "Epoch: 93 \tTraining Loss: 1.066340 \tValidation Loss: 0.024230\n",
            "Validation loss decreased (0.024234 --> 0.024230).  Saving model ...\n",
            "train_loss:\n",
            "1.0662390435789968\n",
            "Epoch: 94 \tTraining Loss: 1.066239 \tValidation Loss: 0.024226\n",
            "Validation loss decreased (0.024230 --> 0.024226).  Saving model ...\n",
            "train_loss:\n",
            "1.0661396633137714\n",
            "Epoch: 95 \tTraining Loss: 1.066140 \tValidation Loss: 0.024222\n",
            "Validation loss decreased (0.024226 --> 0.024222).  Saving model ...\n",
            "train_loss:\n",
            "1.066041560618432\n",
            "Epoch: 96 \tTraining Loss: 1.066042 \tValidation Loss: 0.024217\n",
            "Validation loss decreased (0.024222 --> 0.024217).  Saving model ...\n",
            "train_loss:\n",
            "1.0659448301399148\n",
            "Epoch: 97 \tTraining Loss: 1.065945 \tValidation Loss: 0.024213\n",
            "Validation loss decreased (0.024217 --> 0.024213).  Saving model ...\n",
            "train_loss:\n",
            "1.065849549822755\n",
            "Epoch: 98 \tTraining Loss: 1.065850 \tValidation Loss: 0.024209\n",
            "Validation loss decreased (0.024213 --> 0.024209).  Saving model ...\n",
            "train_loss:\n",
            "1.0657558205363515\n",
            "Epoch: 99 \tTraining Loss: 1.065756 \tValidation Loss: 0.024204\n",
            "Validation loss decreased (0.024209 --> 0.024204).  Saving model ...\n",
            "train_loss:\n",
            "1.0656636930429018\n",
            "Epoch: 100 \tTraining Loss: 1.065664 \tValidation Loss: 0.024199\n",
            "Validation loss decreased (0.024204 --> 0.024199).  Saving model ...\n",
            "train_loss:\n",
            "1.0655732328420158\n",
            "Epoch: 101 \tTraining Loss: 1.065573 \tValidation Loss: 0.024194\n",
            "Validation loss decreased (0.024199 --> 0.024194).  Saving model ...\n",
            "train_loss:\n",
            "1.065484487748408\n",
            "Epoch: 102 \tTraining Loss: 1.065484 \tValidation Loss: 0.024189\n",
            "Validation loss decreased (0.024194 --> 0.024189).  Saving model ...\n",
            "train_loss:\n",
            "1.0653974885468955\n",
            "Epoch: 103 \tTraining Loss: 1.065397 \tValidation Loss: 0.024183\n",
            "Validation loss decreased (0.024189 --> 0.024183).  Saving model ...\n",
            "train_loss:\n",
            "1.0653122542323648\n",
            "Epoch: 104 \tTraining Loss: 1.065312 \tValidation Loss: 0.024177\n",
            "Validation loss decreased (0.024183 --> 0.024177).  Saving model ...\n",
            "train_loss:\n",
            "1.0652287969222436\n",
            "Epoch: 105 \tTraining Loss: 1.065229 \tValidation Loss: 0.024171\n",
            "Validation loss decreased (0.024177 --> 0.024171).  Saving model ...\n",
            "train_loss:\n",
            "1.0651470933641707\n",
            "Epoch: 106 \tTraining Loss: 1.065147 \tValidation Loss: 0.024164\n",
            "Validation loss decreased (0.024171 --> 0.024164).  Saving model ...\n",
            "train_loss:\n",
            "1.065067124235761\n",
            "Epoch: 107 \tTraining Loss: 1.065067 \tValidation Loss: 0.024158\n",
            "Validation loss decreased (0.024164 --> 0.024158).  Saving model ...\n",
            "train_loss:\n",
            "1.0649888446697822\n",
            "Epoch: 108 \tTraining Loss: 1.064989 \tValidation Loss: 0.024150\n",
            "Validation loss decreased (0.024158 --> 0.024150).  Saving model ...\n",
            "train_loss:\n",
            "1.064912192441605\n",
            "Epoch: 109 \tTraining Loss: 1.064912 \tValidation Loss: 0.024143\n",
            "Validation loss decreased (0.024150 --> 0.024143).  Saving model ...\n",
            "train_loss:\n",
            "1.064837125303981\n",
            "Epoch: 110 \tTraining Loss: 1.064837 \tValidation Loss: 0.024135\n",
            "Validation loss decreased (0.024143 --> 0.024135).  Saving model ...\n",
            "train_loss:\n",
            "1.0647635188076523\n",
            "Epoch: 111 \tTraining Loss: 1.064764 \tValidation Loss: 0.024127\n",
            "Validation loss decreased (0.024135 --> 0.024127).  Saving model ...\n",
            "train_loss:\n",
            "1.0646913290678799\n",
            "Epoch: 112 \tTraining Loss: 1.064691 \tValidation Loss: 0.024119\n",
            "Validation loss decreased (0.024127 --> 0.024119).  Saving model ...\n",
            "train_loss:\n",
            "1.0646204401503552\n",
            "Epoch: 113 \tTraining Loss: 1.064620 \tValidation Loss: 0.024110\n",
            "Validation loss decreased (0.024119 --> 0.024110).  Saving model ...\n",
            "train_loss:\n",
            "1.0645507593731305\n",
            "Epoch: 114 \tTraining Loss: 1.064551 \tValidation Loss: 0.024101\n",
            "Validation loss decreased (0.024110 --> 0.024101).  Saving model ...\n",
            "train_loss:\n",
            "1.064482188486791\n",
            "Epoch: 115 \tTraining Loss: 1.064482 \tValidation Loss: 0.024092\n",
            "Validation loss decreased (0.024101 --> 0.024092).  Saving model ...\n",
            "train_loss:\n",
            "1.064414613522016\n",
            "Epoch: 116 \tTraining Loss: 1.064415 \tValidation Loss: 0.024083\n",
            "Validation loss decreased (0.024092 --> 0.024083).  Saving model ...\n",
            "train_loss:\n",
            "1.064347919526991\n",
            "Epoch: 117 \tTraining Loss: 1.064348 \tValidation Loss: 0.024073\n",
            "Validation loss decreased (0.024083 --> 0.024073).  Saving model ...\n",
            "train_loss:\n",
            "1.064282014802262\n",
            "Epoch: 118 \tTraining Loss: 1.064282 \tValidation Loss: 0.024063\n",
            "Validation loss decreased (0.024073 --> 0.024063).  Saving model ...\n",
            "train_loss:\n",
            "1.0642167807935359\n",
            "Epoch: 119 \tTraining Loss: 1.064217 \tValidation Loss: 0.024053\n",
            "Validation loss decreased (0.024063 --> 0.024053).  Saving model ...\n",
            "train_loss:\n",
            "1.06415213398881\n",
            "Epoch: 120 \tTraining Loss: 1.064152 \tValidation Loss: 0.024042\n",
            "Validation loss decreased (0.024053 --> 0.024042).  Saving model ...\n",
            "train_loss:\n",
            "1.0640879499388265\n",
            "Epoch: 121 \tTraining Loss: 1.064088 \tValidation Loss: 0.024032\n",
            "Validation loss decreased (0.024042 --> 0.024032).  Saving model ...\n",
            "train_loss:\n",
            "1.064024146441575\n",
            "Epoch: 122 \tTraining Loss: 1.064024 \tValidation Loss: 0.024021\n",
            "Validation loss decreased (0.024032 --> 0.024021).  Saving model ...\n",
            "train_loss:\n",
            "1.0639606167326916\n",
            "Epoch: 123 \tTraining Loss: 1.063961 \tValidation Loss: 0.024010\n",
            "Validation loss decreased (0.024021 --> 0.024010).  Saving model ...\n",
            "train_loss:\n",
            "1.0638972763176804\n",
            "Epoch: 124 \tTraining Loss: 1.063897 \tValidation Loss: 0.023998\n",
            "Validation loss decreased (0.024010 --> 0.023998).  Saving model ...\n",
            "train_loss:\n",
            "1.0638340495444916\n",
            "Epoch: 125 \tTraining Loss: 1.063834 \tValidation Loss: 0.023987\n",
            "Validation loss decreased (0.023998 --> 0.023987).  Saving model ...\n",
            "train_loss:\n",
            "1.063770849953641\n",
            "Epoch: 126 \tTraining Loss: 1.063771 \tValidation Loss: 0.023975\n",
            "Validation loss decreased (0.023987 --> 0.023975).  Saving model ...\n",
            "train_loss:\n",
            "1.0637076038580675\n",
            "Epoch: 127 \tTraining Loss: 1.063708 \tValidation Loss: 0.023963\n",
            "Validation loss decreased (0.023975 --> 0.023963).  Saving model ...\n",
            "train_loss:\n",
            "1.0636442421556829\n",
            "Epoch: 128 \tTraining Loss: 1.063644 \tValidation Loss: 0.023951\n",
            "Validation loss decreased (0.023963 --> 0.023951).  Saving model ...\n",
            "train_loss:\n",
            "1.0635806839544695\n",
            "Epoch: 129 \tTraining Loss: 1.063581 \tValidation Loss: 0.023939\n",
            "Validation loss decreased (0.023951 --> 0.023939).  Saving model ...\n",
            "train_loss:\n",
            "1.063516874889751\n",
            "Epoch: 130 \tTraining Loss: 1.063517 \tValidation Loss: 0.023927\n",
            "Validation loss decreased (0.023939 --> 0.023927).  Saving model ...\n",
            "train_loss:\n",
            "1.0634527818842248\n",
            "Epoch: 131 \tTraining Loss: 1.063453 \tValidation Loss: 0.023915\n",
            "Validation loss decreased (0.023927 --> 0.023915).  Saving model ...\n",
            "train_loss:\n",
            "1.0633883158584216\n",
            "Epoch: 132 \tTraining Loss: 1.063388 \tValidation Loss: 0.023902\n",
            "Validation loss decreased (0.023915 --> 0.023902).  Saving model ...\n",
            "train_loss:\n",
            "1.0633234493025057\n",
            "Epoch: 133 \tTraining Loss: 1.063323 \tValidation Loss: 0.023890\n",
            "Validation loss decreased (0.023902 --> 0.023890).  Saving model ...\n",
            "train_loss:\n",
            "1.0632581357117539\n",
            "Epoch: 134 \tTraining Loss: 1.063258 \tValidation Loss: 0.023877\n",
            "Validation loss decreased (0.023890 --> 0.023877).  Saving model ...\n",
            "train_loss:\n",
            "1.0631923416813651\n",
            "Epoch: 135 \tTraining Loss: 1.063192 \tValidation Loss: 0.023864\n",
            "Validation loss decreased (0.023877 --> 0.023864).  Saving model ...\n",
            "train_loss:\n",
            "1.0631260062967027\n",
            "Epoch: 136 \tTraining Loss: 1.063126 \tValidation Loss: 0.023851\n",
            "Validation loss decreased (0.023864 --> 0.023851).  Saving model ...\n",
            "train_loss:\n",
            "1.063059124972794\n",
            "Epoch: 137 \tTraining Loss: 1.063059 \tValidation Loss: 0.023838\n",
            "Validation loss decreased (0.023851 --> 0.023838).  Saving model ...\n",
            "train_loss:\n",
            "1.0629916358125078\n",
            "Epoch: 138 \tTraining Loss: 1.062992 \tValidation Loss: 0.023825\n",
            "Validation loss decreased (0.023838 --> 0.023825).  Saving model ...\n",
            "train_loss:\n",
            "1.0629235561732406\n",
            "Epoch: 139 \tTraining Loss: 1.062924 \tValidation Loss: 0.023812\n",
            "Validation loss decreased (0.023825 --> 0.023812).  Saving model ...\n",
            "train_loss:\n",
            "1.0628548316903166\n",
            "Epoch: 140 \tTraining Loss: 1.062855 \tValidation Loss: 0.023799\n",
            "Validation loss decreased (0.023812 --> 0.023799).  Saving model ...\n",
            "train_loss:\n",
            "1.062785422736472\n",
            "Epoch: 141 \tTraining Loss: 1.062785 \tValidation Loss: 0.023785\n",
            "Validation loss decreased (0.023799 --> 0.023785).  Saving model ...\n",
            "train_loss:\n",
            "1.0627153568215422\n",
            "Epoch: 142 \tTraining Loss: 1.062715 \tValidation Loss: 0.023772\n",
            "Validation loss decreased (0.023785 --> 0.023772).  Saving model ...\n",
            "train_loss:\n",
            "1.062644599885731\n",
            "Epoch: 143 \tTraining Loss: 1.062645 \tValidation Loss: 0.023758\n",
            "Validation loss decreased (0.023772 --> 0.023758).  Saving model ...\n",
            "train_loss:\n",
            "1.0625731178692408\n",
            "Epoch: 144 \tTraining Loss: 1.062573 \tValidation Loss: 0.023745\n",
            "Validation loss decreased (0.023758 --> 0.023745).  Saving model ...\n",
            "train_loss:\n",
            "1.0625009330419393\n",
            "Epoch: 145 \tTraining Loss: 1.062501 \tValidation Loss: 0.023731\n",
            "Validation loss decreased (0.023745 --> 0.023731).  Saving model ...\n",
            "train_loss:\n",
            "1.0624280106890334\n",
            "Epoch: 146 \tTraining Loss: 1.062428 \tValidation Loss: 0.023718\n",
            "Validation loss decreased (0.023731 --> 0.023718).  Saving model ...\n",
            "train_loss:\n",
            "1.0623543576879815\n",
            "Epoch: 147 \tTraining Loss: 1.062354 \tValidation Loss: 0.023704\n",
            "Validation loss decreased (0.023718 --> 0.023704).  Saving model ...\n",
            "train_loss:\n",
            "1.062279964213843\n",
            "Epoch: 148 \tTraining Loss: 1.062280 \tValidation Loss: 0.023691\n",
            "Validation loss decreased (0.023704 --> 0.023691).  Saving model ...\n",
            "train_loss:\n",
            "1.062204829611621\n",
            "Epoch: 149 \tTraining Loss: 1.062205 \tValidation Loss: 0.023677\n",
            "Validation loss decreased (0.023691 --> 0.023677).  Saving model ...\n",
            "train_loss:\n",
            "1.062128938816406\n",
            "Epoch: 150 \tTraining Loss: 1.062129 \tValidation Loss: 0.023663\n",
            "Validation loss decreased (0.023677 --> 0.023663).  Saving model ...\n",
            "train_loss:\n",
            "1.0620522842957423\n",
            "Epoch: 151 \tTraining Loss: 1.062052 \tValidation Loss: 0.023649\n",
            "Validation loss decreased (0.023663 --> 0.023649).  Saving model ...\n",
            "train_loss:\n",
            "1.0619749063318902\n",
            "Epoch: 152 \tTraining Loss: 1.061975 \tValidation Loss: 0.023635\n",
            "Validation loss decreased (0.023649 --> 0.023635).  Saving model ...\n",
            "train_loss:\n",
            "1.06189676628008\n",
            "Epoch: 153 \tTraining Loss: 1.061897 \tValidation Loss: 0.023621\n",
            "Validation loss decreased (0.023635 --> 0.023621).  Saving model ...\n",
            "train_loss:\n",
            "1.0618178854276845\n",
            "Epoch: 154 \tTraining Loss: 1.061818 \tValidation Loss: 0.023607\n",
            "Validation loss decreased (0.023621 --> 0.023607).  Saving model ...\n",
            "train_loss:\n",
            "1.0617382572247431\n",
            "Epoch: 155 \tTraining Loss: 1.061738 \tValidation Loss: 0.023593\n",
            "Validation loss decreased (0.023607 --> 0.023593).  Saving model ...\n",
            "train_loss:\n",
            "1.0616578803612635\n",
            "Epoch: 156 \tTraining Loss: 1.061658 \tValidation Loss: 0.023579\n",
            "Validation loss decreased (0.023593 --> 0.023579).  Saving model ...\n",
            "train_loss:\n",
            "1.0615767780896073\n",
            "Epoch: 157 \tTraining Loss: 1.061577 \tValidation Loss: 0.023565\n",
            "Validation loss decreased (0.023579 --> 0.023565).  Saving model ...\n",
            "train_loss:\n",
            "1.0614949317423852\n",
            "Epoch: 158 \tTraining Loss: 1.061495 \tValidation Loss: 0.023551\n",
            "Validation loss decreased (0.023565 --> 0.023551).  Saving model ...\n",
            "train_loss:\n",
            "1.0614123468870644\n",
            "Epoch: 159 \tTraining Loss: 1.061412 \tValidation Loss: 0.023537\n",
            "Validation loss decreased (0.023551 --> 0.023537).  Saving model ...\n",
            "train_loss:\n",
            "1.061329036623567\n",
            "Epoch: 160 \tTraining Loss: 1.061329 \tValidation Loss: 0.023522\n",
            "Validation loss decreased (0.023537 --> 0.023522).  Saving model ...\n",
            "train_loss:\n",
            "1.061244989489461\n",
            "Epoch: 161 \tTraining Loss: 1.061245 \tValidation Loss: 0.023508\n",
            "Validation loss decreased (0.023522 --> 0.023508).  Saving model ...\n",
            "train_loss:\n",
            "1.0611602251346295\n",
            "Epoch: 162 \tTraining Loss: 1.061160 \tValidation Loss: 0.023494\n",
            "Validation loss decreased (0.023508 --> 0.023494).  Saving model ...\n",
            "train_loss:\n",
            "1.0610747373366094\n",
            "Epoch: 163 \tTraining Loss: 1.061075 \tValidation Loss: 0.023479\n",
            "Validation loss decreased (0.023494 --> 0.023479).  Saving model ...\n",
            "train_loss:\n",
            "1.0609885175804517\n",
            "Epoch: 164 \tTraining Loss: 1.060989 \tValidation Loss: 0.023465\n",
            "Validation loss decreased (0.023479 --> 0.023465).  Saving model ...\n",
            "train_loss:\n",
            "1.0609015779835838\n",
            "Epoch: 165 \tTraining Loss: 1.060902 \tValidation Loss: 0.023451\n",
            "Validation loss decreased (0.023465 --> 0.023451).  Saving model ...\n",
            "train_loss:\n",
            "1.060813933283418\n",
            "Epoch: 166 \tTraining Loss: 1.060814 \tValidation Loss: 0.023436\n",
            "Validation loss decreased (0.023451 --> 0.023436).  Saving model ...\n",
            "train_loss:\n",
            "1.0607255687425425\n",
            "Epoch: 167 \tTraining Loss: 1.060726 \tValidation Loss: 0.023422\n",
            "Validation loss decreased (0.023436 --> 0.023422).  Saving model ...\n",
            "train_loss:\n",
            "1.0606364683135525\n",
            "Epoch: 168 \tTraining Loss: 1.060636 \tValidation Loss: 0.023407\n",
            "Validation loss decreased (0.023422 --> 0.023407).  Saving model ...\n",
            "train_loss:\n",
            "1.0605466581962921\n",
            "Epoch: 169 \tTraining Loss: 1.060547 \tValidation Loss: 0.023392\n",
            "Validation loss decreased (0.023407 --> 0.023392).  Saving model ...\n",
            "train_loss:\n",
            "1.0604561007284856\n",
            "Epoch: 170 \tTraining Loss: 1.060456 \tValidation Loss: 0.023378\n",
            "Validation loss decreased (0.023392 --> 0.023378).  Saving model ...\n",
            "train_loss:\n",
            "1.0603648404498676\n",
            "Epoch: 171 \tTraining Loss: 1.060365 \tValidation Loss: 0.023363\n",
            "Validation loss decreased (0.023378 --> 0.023363).  Saving model ...\n",
            "train_loss:\n",
            "1.0602728446106335\n",
            "Epoch: 172 \tTraining Loss: 1.060273 \tValidation Loss: 0.023348\n",
            "Validation loss decreased (0.023363 --> 0.023348).  Saving model ...\n",
            "train_loss:\n",
            "1.0601801125557868\n",
            "Epoch: 173 \tTraining Loss: 1.060180 \tValidation Loss: 0.023334\n",
            "Validation loss decreased (0.023348 --> 0.023334).  Saving model ...\n",
            "train_loss:\n",
            "1.0600866279104253\n",
            "Epoch: 174 \tTraining Loss: 1.060087 \tValidation Loss: 0.023319\n",
            "Validation loss decreased (0.023334 --> 0.023319).  Saving model ...\n",
            "train_loss:\n",
            "1.0599924119619222\n",
            "Epoch: 175 \tTraining Loss: 1.059992 \tValidation Loss: 0.023304\n",
            "Validation loss decreased (0.023319 --> 0.023304).  Saving model ...\n",
            "train_loss:\n",
            "1.0598974322879708\n",
            "Epoch: 176 \tTraining Loss: 1.059897 \tValidation Loss: 0.023290\n",
            "Validation loss decreased (0.023304 --> 0.023290).  Saving model ...\n",
            "train_loss:\n",
            "1.0598016924910494\n",
            "Epoch: 177 \tTraining Loss: 1.059802 \tValidation Loss: 0.023275\n",
            "Validation loss decreased (0.023290 --> 0.023275).  Saving model ...\n",
            "train_loss:\n",
            "1.0597051834012126\n",
            "Epoch: 178 \tTraining Loss: 1.059705 \tValidation Loss: 0.023260\n",
            "Validation loss decreased (0.023275 --> 0.023260).  Saving model ...\n",
            "train_loss:\n",
            "1.059607901088484\n",
            "Epoch: 179 \tTraining Loss: 1.059608 \tValidation Loss: 0.023245\n",
            "Validation loss decreased (0.023260 --> 0.023245).  Saving model ...\n",
            "train_loss:\n",
            "1.059509827540471\n",
            "Epoch: 180 \tTraining Loss: 1.059510 \tValidation Loss: 0.023230\n",
            "Validation loss decreased (0.023245 --> 0.023230).  Saving model ...\n",
            "train_loss:\n",
            "1.0594109683246402\n",
            "Epoch: 181 \tTraining Loss: 1.059411 \tValidation Loss: 0.023216\n",
            "Validation loss decreased (0.023230 --> 0.023216).  Saving model ...\n",
            "train_loss:\n",
            "1.0593113080485836\n",
            "Epoch: 182 \tTraining Loss: 1.059311 \tValidation Loss: 0.023201\n",
            "Validation loss decreased (0.023216 --> 0.023201).  Saving model ...\n",
            "train_loss:\n",
            "1.0592108375423557\n",
            "Epoch: 183 \tTraining Loss: 1.059211 \tValidation Loss: 0.023186\n",
            "Validation loss decreased (0.023201 --> 0.023186).  Saving model ...\n",
            "train_loss:\n",
            "1.0591095744908512\n",
            "Epoch: 184 \tTraining Loss: 1.059110 \tValidation Loss: 0.023171\n",
            "Validation loss decreased (0.023186 --> 0.023171).  Saving model ...\n",
            "train_loss:\n",
            "1.059007466494382\n",
            "Epoch: 185 \tTraining Loss: 1.059007 \tValidation Loss: 0.023156\n",
            "Validation loss decreased (0.023171 --> 0.023156).  Saving model ...\n",
            "train_loss:\n",
            "1.0589045266528705\n",
            "Epoch: 186 \tTraining Loss: 1.058905 \tValidation Loss: 0.023142\n",
            "Validation loss decreased (0.023156 --> 0.023142).  Saving model ...\n",
            "train_loss:\n",
            "1.0588007605337835\n",
            "Epoch: 187 \tTraining Loss: 1.058801 \tValidation Loss: 0.023127\n",
            "Validation loss decreased (0.023142 --> 0.023127).  Saving model ...\n",
            "train_loss:\n",
            "1.0586961527447125\n",
            "Epoch: 188 \tTraining Loss: 1.058696 \tValidation Loss: 0.023112\n",
            "Validation loss decreased (0.023127 --> 0.023112).  Saving model ...\n",
            "train_loss:\n",
            "1.0585907167130775\n",
            "Epoch: 189 \tTraining Loss: 1.058591 \tValidation Loss: 0.023097\n",
            "Validation loss decreased (0.023112 --> 0.023097).  Saving model ...\n",
            "train_loss:\n",
            "1.058484441303945\n",
            "Epoch: 190 \tTraining Loss: 1.058484 \tValidation Loss: 0.023083\n",
            "Validation loss decreased (0.023097 --> 0.023083).  Saving model ...\n",
            "train_loss:\n",
            "1.058377328154805\n",
            "Epoch: 191 \tTraining Loss: 1.058377 \tValidation Loss: 0.023068\n",
            "Validation loss decreased (0.023083 --> 0.023068).  Saving model ...\n",
            "train_loss:\n",
            "1.0582693811956343\n",
            "Epoch: 192 \tTraining Loss: 1.058269 \tValidation Loss: 0.023053\n",
            "Validation loss decreased (0.023068 --> 0.023053).  Saving model ...\n",
            "train_loss:\n",
            "1.0581606073038918\n",
            "Epoch: 193 \tTraining Loss: 1.058161 \tValidation Loss: 0.023039\n",
            "Validation loss decreased (0.023053 --> 0.023039).  Saving model ...\n",
            "train_loss:\n",
            "1.0580510277669508\n",
            "Epoch: 194 \tTraining Loss: 1.058051 \tValidation Loss: 0.023024\n",
            "Validation loss decreased (0.023039 --> 0.023024).  Saving model ...\n",
            "train_loss:\n",
            "1.057940641274819\n",
            "Epoch: 195 \tTraining Loss: 1.057941 \tValidation Loss: 0.023009\n",
            "Validation loss decreased (0.023024 --> 0.023009).  Saving model ...\n",
            "train_loss:\n",
            "1.0578294635474026\n",
            "Epoch: 196 \tTraining Loss: 1.057829 \tValidation Loss: 0.022994\n",
            "Validation loss decreased (0.023009 --> 0.022994).  Saving model ...\n",
            "train_loss:\n",
            "1.0577175286444989\n",
            "Epoch: 197 \tTraining Loss: 1.057718 \tValidation Loss: 0.022980\n",
            "Validation loss decreased (0.022994 --> 0.022980).  Saving model ...\n",
            "train_loss:\n",
            "1.0576048290336526\n",
            "Epoch: 198 \tTraining Loss: 1.057605 \tValidation Loss: 0.022965\n",
            "Validation loss decreased (0.022980 --> 0.022965).  Saving model ...\n",
            "train_loss:\n",
            "1.0574914239920103\n",
            "Epoch: 199 \tTraining Loss: 1.057491 \tValidation Loss: 0.022950\n",
            "Validation loss decreased (0.022965 --> 0.022950).  Saving model ...\n",
            "train_loss:\n",
            "1.0573773279294862\n",
            "Epoch: 200 \tTraining Loss: 1.057377 \tValidation Loss: 0.022936\n",
            "Validation loss decreased (0.022950 --> 0.022936).  Saving model ...\n",
            "train_loss:\n",
            "1.0572625539460025\n",
            "Epoch: 201 \tTraining Loss: 1.057263 \tValidation Loss: 0.022921\n",
            "Validation loss decreased (0.022936 --> 0.022921).  Saving model ...\n",
            "train_loss:\n",
            "1.0571471475637877\n",
            "Epoch: 202 \tTraining Loss: 1.057147 \tValidation Loss: 0.022906\n",
            "Validation loss decreased (0.022921 --> 0.022906).  Saving model ...\n",
            "train_loss:\n",
            "1.0570311434976347\n",
            "Epoch: 203 \tTraining Loss: 1.057031 \tValidation Loss: 0.022891\n",
            "Validation loss decreased (0.022906 --> 0.022891).  Saving model ...\n",
            "train_loss:\n",
            "1.0569145803923135\n",
            "Epoch: 204 \tTraining Loss: 1.056915 \tValidation Loss: 0.022877\n",
            "Validation loss decreased (0.022891 --> 0.022877).  Saving model ...\n",
            "train_loss:\n",
            "1.056797471020248\n",
            "Epoch: 205 \tTraining Loss: 1.056797 \tValidation Loss: 0.022862\n",
            "Validation loss decreased (0.022877 --> 0.022862).  Saving model ...\n",
            "train_loss:\n",
            "1.0566798903784909\n",
            "Epoch: 206 \tTraining Loss: 1.056680 \tValidation Loss: 0.022847\n",
            "Validation loss decreased (0.022862 --> 0.022847).  Saving model ...\n",
            "train_loss:\n",
            "1.0565618260221168\n",
            "Epoch: 207 \tTraining Loss: 1.056562 \tValidation Loss: 0.022832\n",
            "Validation loss decreased (0.022847 --> 0.022832).  Saving model ...\n",
            "train_loss:\n",
            "1.0564433477082096\n",
            "Epoch: 208 \tTraining Loss: 1.056443 \tValidation Loss: 0.022816\n",
            "Validation loss decreased (0.022832 --> 0.022816).  Saving model ...\n",
            "train_loss:\n",
            "1.0563244721391698\n",
            "Epoch: 209 \tTraining Loss: 1.056324 \tValidation Loss: 0.022801\n",
            "Validation loss decreased (0.022816 --> 0.022801).  Saving model ...\n",
            "train_loss:\n",
            "1.056205221257367\n",
            "Epoch: 210 \tTraining Loss: 1.056205 \tValidation Loss: 0.022786\n",
            "Validation loss decreased (0.022801 --> 0.022786).  Saving model ...\n",
            "train_loss:\n",
            "1.056085644187508\n",
            "Epoch: 211 \tTraining Loss: 1.056086 \tValidation Loss: 0.022770\n",
            "Validation loss decreased (0.022786 --> 0.022770).  Saving model ...\n",
            "train_loss:\n",
            "1.0559657507545346\n",
            "Epoch: 212 \tTraining Loss: 1.055966 \tValidation Loss: 0.022755\n",
            "Validation loss decreased (0.022770 --> 0.022755).  Saving model ...\n",
            "train_loss:\n",
            "1.0558455707607688\n",
            "Epoch: 213 \tTraining Loss: 1.055846 \tValidation Loss: 0.022739\n",
            "Validation loss decreased (0.022755 --> 0.022739).  Saving model ...\n",
            "train_loss:\n",
            "1.0557251061711992\n",
            "Epoch: 214 \tTraining Loss: 1.055725 \tValidation Loss: 0.022723\n",
            "Validation loss decreased (0.022739 --> 0.022723).  Saving model ...\n",
            "train_loss:\n",
            "1.0556043867881482\n",
            "Epoch: 215 \tTraining Loss: 1.055604 \tValidation Loss: 0.022707\n",
            "Validation loss decreased (0.022723 --> 0.022707).  Saving model ...\n",
            "train_loss:\n",
            "1.0554834191615765\n",
            "Epoch: 216 \tTraining Loss: 1.055483 \tValidation Loss: 0.022691\n",
            "Validation loss decreased (0.022707 --> 0.022691).  Saving model ...\n",
            "train_loss:\n",
            "1.0553622170464023\n",
            "Epoch: 217 \tTraining Loss: 1.055362 \tValidation Loss: 0.022675\n",
            "Validation loss decreased (0.022691 --> 0.022675).  Saving model ...\n",
            "train_loss:\n",
            "1.0552407683251979\n",
            "Epoch: 218 \tTraining Loss: 1.055241 \tValidation Loss: 0.022659\n",
            "Validation loss decreased (0.022675 --> 0.022659).  Saving model ...\n",
            "train_loss:\n",
            "1.0551191247426546\n",
            "Epoch: 219 \tTraining Loss: 1.055119 \tValidation Loss: 0.022642\n",
            "Validation loss decreased (0.022659 --> 0.022642).  Saving model ...\n",
            "train_loss:\n",
            "1.0549972171966846\n",
            "Epoch: 220 \tTraining Loss: 1.054997 \tValidation Loss: 0.022626\n",
            "Validation loss decreased (0.022642 --> 0.022626).  Saving model ...\n",
            "train_loss:\n",
            "1.0548751124968896\n",
            "Epoch: 221 \tTraining Loss: 1.054875 \tValidation Loss: 0.022609\n",
            "Validation loss decreased (0.022626 --> 0.022609).  Saving model ...\n",
            "train_loss:\n",
            "1.0547527713435036\n",
            "Epoch: 222 \tTraining Loss: 1.054753 \tValidation Loss: 0.022592\n",
            "Validation loss decreased (0.022609 --> 0.022592).  Saving model ...\n",
            "train_loss:\n",
            "1.054630193736527\n",
            "Epoch: 223 \tTraining Loss: 1.054630 \tValidation Loss: 0.022575\n",
            "Validation loss decreased (0.022592 --> 0.022575).  Saving model ...\n",
            "train_loss:\n",
            "1.054507396050862\n",
            "Epoch: 224 \tTraining Loss: 1.054507 \tValidation Loss: 0.022558\n",
            "Validation loss decreased (0.022575 --> 0.022558).  Saving model ...\n",
            "train_loss:\n",
            "1.0543843353842641\n",
            "Epoch: 225 \tTraining Loss: 1.054384 \tValidation Loss: 0.022541\n",
            "Validation loss decreased (0.022558 --> 0.022541).  Saving model ...\n",
            "train_loss:\n",
            "1.0542610340066008\n",
            "Epoch: 226 \tTraining Loss: 1.054261 \tValidation Loss: 0.022523\n",
            "Validation loss decreased (0.022541 --> 0.022523).  Saving model ...\n",
            "train_loss:\n",
            "1.0541374653905302\n",
            "Epoch: 227 \tTraining Loss: 1.054137 \tValidation Loss: 0.022506\n",
            "Validation loss decreased (0.022523 --> 0.022506).  Saving model ...\n",
            "train_loss:\n",
            "1.054013620693605\n",
            "Epoch: 228 \tTraining Loss: 1.054014 \tValidation Loss: 0.022489\n",
            "Validation loss decreased (0.022506 --> 0.022489).  Saving model ...\n",
            "train_loss:\n",
            "1.05388950581079\n",
            "Epoch: 229 \tTraining Loss: 1.053890 \tValidation Loss: 0.022471\n",
            "Validation loss decreased (0.022489 --> 0.022471).  Saving model ...\n",
            "train_loss:\n",
            "1.0537651145196223\n",
            "Epoch: 230 \tTraining Loss: 1.053765 \tValidation Loss: 0.022453\n",
            "Validation loss decreased (0.022471 --> 0.022453).  Saving model ...\n",
            "train_loss:\n",
            "1.0536404042453555\n",
            "Epoch: 231 \tTraining Loss: 1.053640 \tValidation Loss: 0.022436\n",
            "Validation loss decreased (0.022453 --> 0.022436).  Saving model ...\n",
            "train_loss:\n",
            "1.0535153897254022\n",
            "Epoch: 232 \tTraining Loss: 1.053515 \tValidation Loss: 0.022418\n",
            "Validation loss decreased (0.022436 --> 0.022418).  Saving model ...\n",
            "train_loss:\n",
            "1.0533900594973302\n",
            "Epoch: 233 \tTraining Loss: 1.053390 \tValidation Loss: 0.022400\n",
            "Validation loss decreased (0.022418 --> 0.022400).  Saving model ...\n",
            "train_loss:\n",
            "1.0532644197836027\n",
            "Epoch: 234 \tTraining Loss: 1.053264 \tValidation Loss: 0.022382\n",
            "Validation loss decreased (0.022400 --> 0.022382).  Saving model ...\n",
            "train_loss:\n",
            "1.0531384227695046\n",
            "Epoch: 235 \tTraining Loss: 1.053138 \tValidation Loss: 0.022364\n",
            "Validation loss decreased (0.022382 --> 0.022364).  Saving model ...\n",
            "train_loss:\n",
            "1.053012076314989\n",
            "Epoch: 236 \tTraining Loss: 1.053012 \tValidation Loss: 0.022346\n",
            "Validation loss decreased (0.022364 --> 0.022346).  Saving model ...\n",
            "train_loss:\n",
            "1.0528853908999936\n",
            "Epoch: 237 \tTraining Loss: 1.052885 \tValidation Loss: 0.022328\n",
            "Validation loss decreased (0.022346 --> 0.022328).  Saving model ...\n",
            "train_loss:\n",
            "1.0527583363946977\n",
            "Epoch: 238 \tTraining Loss: 1.052758 \tValidation Loss: 0.022310\n",
            "Validation loss decreased (0.022328 --> 0.022310).  Saving model ...\n",
            "train_loss:\n",
            "1.052630907886631\n",
            "Epoch: 239 \tTraining Loss: 1.052631 \tValidation Loss: 0.022292\n",
            "Validation loss decreased (0.022310 --> 0.022292).  Saving model ...\n",
            "train_loss:\n",
            "1.0525031083232754\n",
            "Epoch: 240 \tTraining Loss: 1.052503 \tValidation Loss: 0.022274\n",
            "Validation loss decreased (0.022292 --> 0.022274).  Saving model ...\n",
            "train_loss:\n",
            "1.0523749049548263\n",
            "Epoch: 241 \tTraining Loss: 1.052375 \tValidation Loss: 0.022256\n",
            "Validation loss decreased (0.022274 --> 0.022256).  Saving model ...\n",
            "train_loss:\n",
            "1.052246318741159\n",
            "Epoch: 242 \tTraining Loss: 1.052246 \tValidation Loss: 0.022238\n",
            "Validation loss decreased (0.022256 --> 0.022238).  Saving model ...\n",
            "train_loss:\n",
            "1.0521173297048925\n",
            "Epoch: 243 \tTraining Loss: 1.052117 \tValidation Loss: 0.022220\n",
            "Validation loss decreased (0.022238 --> 0.022220).  Saving model ...\n",
            "train_loss:\n",
            "1.0519879414485052\n",
            "Epoch: 244 \tTraining Loss: 1.051988 \tValidation Loss: 0.022202\n",
            "Validation loss decreased (0.022220 --> 0.022202).  Saving model ...\n",
            "train_loss:\n",
            "1.0518581143447332\n",
            "Epoch: 245 \tTraining Loss: 1.051858 \tValidation Loss: 0.022184\n",
            "Validation loss decreased (0.022202 --> 0.022184).  Saving model ...\n",
            "train_loss:\n",
            "1.0517278876933422\n",
            "Epoch: 246 \tTraining Loss: 1.051728 \tValidation Loss: 0.022166\n",
            "Validation loss decreased (0.022184 --> 0.022166).  Saving model ...\n",
            "train_loss:\n",
            "1.0515972470844186\n",
            "Epoch: 247 \tTraining Loss: 1.051597 \tValidation Loss: 0.022149\n",
            "Validation loss decreased (0.022166 --> 0.022149).  Saving model ...\n",
            "train_loss:\n",
            "1.0514661709030906\n",
            "Epoch: 248 \tTraining Loss: 1.051466 \tValidation Loss: 0.022131\n",
            "Validation loss decreased (0.022149 --> 0.022131).  Saving model ...\n",
            "train_loss:\n",
            "1.0513346725767785\n",
            "Epoch: 249 \tTraining Loss: 1.051335 \tValidation Loss: 0.022113\n",
            "Validation loss decreased (0.022131 --> 0.022113).  Saving model ...\n",
            "train_loss:\n",
            "1.0512027396605566\n",
            "Epoch: 250 \tTraining Loss: 1.051203 \tValidation Loss: 0.022095\n",
            "Validation loss decreased (0.022113 --> 0.022095).  Saving model ...\n",
            "train_loss:\n",
            "1.0510703574170124\n",
            "Epoch: 251 \tTraining Loss: 1.051070 \tValidation Loss: 0.022078\n",
            "Validation loss decreased (0.022095 --> 0.022078).  Saving model ...\n",
            "train_loss:\n",
            "1.0509375569584605\n",
            "Epoch: 252 \tTraining Loss: 1.050938 \tValidation Loss: 0.022060\n",
            "Validation loss decreased (0.022078 --> 0.022060).  Saving model ...\n",
            "train_loss:\n",
            "1.0508043071725865\n",
            "Epoch: 253 \tTraining Loss: 1.050804 \tValidation Loss: 0.022043\n",
            "Validation loss decreased (0.022060 --> 0.022043).  Saving model ...\n",
            "train_loss:\n",
            "1.050670624106795\n",
            "Epoch: 254 \tTraining Loss: 1.050671 \tValidation Loss: 0.022025\n",
            "Validation loss decreased (0.022043 --> 0.022025).  Saving model ...\n",
            "train_loss:\n",
            "1.0505365054685991\n",
            "Epoch: 255 \tTraining Loss: 1.050537 \tValidation Loss: 0.022008\n",
            "Validation loss decreased (0.022025 --> 0.022008).  Saving model ...\n",
            "train_loss:\n",
            "1.0504019512579992\n",
            "Epoch: 256 \tTraining Loss: 1.050402 \tValidation Loss: 0.021991\n",
            "Validation loss decreased (0.022008 --> 0.021991).  Saving model ...\n",
            "train_loss:\n",
            "1.0502669709724384\n",
            "Epoch: 257 \tTraining Loss: 1.050267 \tValidation Loss: 0.021973\n",
            "Validation loss decreased (0.021991 --> 0.021973).  Saving model ...\n",
            "train_loss:\n",
            "1.0501315606819404\n",
            "Epoch: 258 \tTraining Loss: 1.050132 \tValidation Loss: 0.021956\n",
            "Validation loss decreased (0.021973 --> 0.021956).  Saving model ...\n",
            "train_loss:\n",
            "1.049995739381392\n",
            "Epoch: 259 \tTraining Loss: 1.049996 \tValidation Loss: 0.021939\n",
            "Validation loss decreased (0.021956 --> 0.021939).  Saving model ...\n",
            "train_loss:\n",
            "1.049859493970871\n",
            "Epoch: 260 \tTraining Loss: 1.049859 \tValidation Loss: 0.021922\n",
            "Validation loss decreased (0.021939 --> 0.021922).  Saving model ...\n",
            "train_loss:\n",
            "1.0497228509777194\n",
            "Epoch: 261 \tTraining Loss: 1.049723 \tValidation Loss: 0.021905\n",
            "Validation loss decreased (0.021922 --> 0.021905).  Saving model ...\n",
            "train_loss:\n",
            "1.0495858097469413\n",
            "Epoch: 262 \tTraining Loss: 1.049586 \tValidation Loss: 0.021888\n",
            "Validation loss decreased (0.021905 --> 0.021888).  Saving model ...\n",
            "train_loss:\n",
            "1.0494483909109136\n",
            "Epoch: 263 \tTraining Loss: 1.049448 \tValidation Loss: 0.021872\n",
            "Validation loss decreased (0.021888 --> 0.021872).  Saving model ...\n",
            "train_loss:\n",
            "1.0493106092070486\n",
            "Epoch: 264 \tTraining Loss: 1.049311 \tValidation Loss: 0.021855\n",
            "Validation loss decreased (0.021872 --> 0.021855).  Saving model ...\n",
            "train_loss:\n",
            "1.0491724688928206\n",
            "Epoch: 265 \tTraining Loss: 1.049172 \tValidation Loss: 0.021838\n",
            "Validation loss decreased (0.021855 --> 0.021838).  Saving model ...\n",
            "train_loss:\n",
            "1.0490339716057202\n",
            "Epoch: 266 \tTraining Loss: 1.049034 \tValidation Loss: 0.021822\n",
            "Validation loss decreased (0.021838 --> 0.021822).  Saving model ...\n",
            "train_loss:\n",
            "1.0488951661429562\n",
            "Epoch: 267 \tTraining Loss: 1.048895 \tValidation Loss: 0.021806\n",
            "Validation loss decreased (0.021822 --> 0.021806).  Saving model ...\n",
            "train_loss:\n",
            "1.048756066259447\n",
            "Epoch: 268 \tTraining Loss: 1.048756 \tValidation Loss: 0.021789\n",
            "Validation loss decreased (0.021806 --> 0.021789).  Saving model ...\n",
            "train_loss:\n",
            "1.048616660820259\n",
            "Epoch: 269 \tTraining Loss: 1.048617 \tValidation Loss: 0.021773\n",
            "Validation loss decreased (0.021789 --> 0.021773).  Saving model ...\n",
            "train_loss:\n",
            "1.0484770012425852\n",
            "Epoch: 270 \tTraining Loss: 1.048477 \tValidation Loss: 0.021757\n",
            "Validation loss decreased (0.021773 --> 0.021757).  Saving model ...\n",
            "train_loss:\n",
            "1.048337120276231\n",
            "Epoch: 271 \tTraining Loss: 1.048337 \tValidation Loss: 0.021741\n",
            "Validation loss decreased (0.021757 --> 0.021741).  Saving model ...\n",
            "train_loss:\n",
            "1.048197015956208\n",
            "Epoch: 272 \tTraining Loss: 1.048197 \tValidation Loss: 0.021725\n",
            "Validation loss decreased (0.021741 --> 0.021725).  Saving model ...\n",
            "train_loss:\n",
            "1.0480567197223285\n",
            "Epoch: 273 \tTraining Loss: 1.048057 \tValidation Loss: 0.021709\n",
            "Validation loss decreased (0.021725 --> 0.021709).  Saving model ...\n",
            "train_loss:\n",
            "1.0479162551544525\n",
            "Epoch: 274 \tTraining Loss: 1.047916 \tValidation Loss: 0.021693\n",
            "Validation loss decreased (0.021709 --> 0.021693).  Saving model ...\n",
            "train_loss:\n",
            "1.0477756428849565\n",
            "Epoch: 275 \tTraining Loss: 1.047776 \tValidation Loss: 0.021678\n",
            "Validation loss decreased (0.021693 --> 0.021678).  Saving model ...\n",
            "train_loss:\n",
            "1.0476349055112064\n",
            "Epoch: 276 \tTraining Loss: 1.047635 \tValidation Loss: 0.021662\n",
            "Validation loss decreased (0.021678 --> 0.021662).  Saving model ...\n",
            "train_loss:\n",
            "1.0474940780754929\n",
            "Epoch: 277 \tTraining Loss: 1.047494 \tValidation Loss: 0.021646\n",
            "Validation loss decreased (0.021662 --> 0.021646).  Saving model ...\n",
            "train_loss:\n",
            "1.0473531723677456\n",
            "Epoch: 278 \tTraining Loss: 1.047353 \tValidation Loss: 0.021631\n",
            "Validation loss decreased (0.021646 --> 0.021631).  Saving model ...\n",
            "train_loss:\n",
            "1.0472122001778947\n",
            "Epoch: 279 \tTraining Loss: 1.047212 \tValidation Loss: 0.021615\n",
            "Validation loss decreased (0.021631 --> 0.021615).  Saving model ...\n",
            "train_loss:\n",
            "1.0470711716583796\n",
            "Epoch: 280 \tTraining Loss: 1.047071 \tValidation Loss: 0.021600\n",
            "Validation loss decreased (0.021615 --> 0.021600).  Saving model ...\n",
            "train_loss:\n",
            "1.0469301336414212\n",
            "Epoch: 281 \tTraining Loss: 1.046930 \tValidation Loss: 0.021585\n",
            "Validation loss decreased (0.021600 --> 0.021585).  Saving model ...\n",
            "train_loss:\n",
            "1.0467890779395679\n",
            "Epoch: 282 \tTraining Loss: 1.046789 \tValidation Loss: 0.021569\n",
            "Validation loss decreased (0.021585 --> 0.021569).  Saving model ...\n",
            "train_loss:\n",
            "1.046648043852586\n",
            "Epoch: 283 \tTraining Loss: 1.046648 \tValidation Loss: 0.021554\n",
            "Validation loss decreased (0.021569 --> 0.021554).  Saving model ...\n",
            "train_loss:\n",
            "1.0465070035431412\n",
            "Epoch: 284 \tTraining Loss: 1.046507 \tValidation Loss: 0.021539\n",
            "Validation loss decreased (0.021554 --> 0.021539).  Saving model ...\n",
            "train_loss:\n",
            "1.0463660002409756\n",
            "Epoch: 285 \tTraining Loss: 1.046366 \tValidation Loss: 0.021524\n",
            "Validation loss decreased (0.021539 --> 0.021524).  Saving model ...\n",
            "train_loss:\n",
            "1.046225014951203\n",
            "Epoch: 286 \tTraining Loss: 1.046225 \tValidation Loss: 0.021509\n",
            "Validation loss decreased (0.021524 --> 0.021509).  Saving model ...\n",
            "train_loss:\n",
            "1.0460840640487252\n",
            "Epoch: 287 \tTraining Loss: 1.046084 \tValidation Loss: 0.021493\n",
            "Validation loss decreased (0.021509 --> 0.021493).  Saving model ...\n",
            "train_loss:\n",
            "1.0459431567034878\n",
            "Epoch: 288 \tTraining Loss: 1.045943 \tValidation Loss: 0.021478\n",
            "Validation loss decreased (0.021493 --> 0.021478).  Saving model ...\n",
            "train_loss:\n",
            "1.0458022752305964\n",
            "Epoch: 289 \tTraining Loss: 1.045802 \tValidation Loss: 0.021463\n",
            "Validation loss decreased (0.021478 --> 0.021463).  Saving model ...\n",
            "train_loss:\n",
            "1.045661448449879\n",
            "Epoch: 290 \tTraining Loss: 1.045661 \tValidation Loss: 0.021448\n",
            "Validation loss decreased (0.021463 --> 0.021448).  Saving model ...\n",
            "train_loss:\n",
            "1.0455206377165658\n",
            "Epoch: 291 \tTraining Loss: 1.045521 \tValidation Loss: 0.021433\n",
            "Validation loss decreased (0.021448 --> 0.021433).  Saving model ...\n",
            "train_loss:\n",
            "1.0453798721779834\n",
            "Epoch: 292 \tTraining Loss: 1.045380 \tValidation Loss: 0.021418\n",
            "Validation loss decreased (0.021433 --> 0.021418).  Saving model ...\n",
            "train_loss:\n",
            "1.0452391361142253\n",
            "Epoch: 293 \tTraining Loss: 1.045239 \tValidation Loss: 0.021404\n",
            "Validation loss decreased (0.021418 --> 0.021404).  Saving model ...\n",
            "train_loss:\n",
            "1.045098405617934\n",
            "Epoch: 294 \tTraining Loss: 1.045098 \tValidation Loss: 0.021389\n",
            "Validation loss decreased (0.021404 --> 0.021389).  Saving model ...\n",
            "train_loss:\n",
            "1.0449577029589767\n",
            "Epoch: 295 \tTraining Loss: 1.044958 \tValidation Loss: 0.021374\n",
            "Validation loss decreased (0.021389 --> 0.021374).  Saving model ...\n",
            "train_loss:\n",
            "1.0448170143824358\n",
            "Epoch: 296 \tTraining Loss: 1.044817 \tValidation Loss: 0.021359\n",
            "Validation loss decreased (0.021374 --> 0.021359).  Saving model ...\n",
            "train_loss:\n",
            "1.0446763028810313\n",
            "Epoch: 297 \tTraining Loss: 1.044676 \tValidation Loss: 0.021344\n",
            "Validation loss decreased (0.021359 --> 0.021344).  Saving model ...\n",
            "train_loss:\n",
            "1.0445356090645215\n",
            "Epoch: 298 \tTraining Loss: 1.044536 \tValidation Loss: 0.021330\n",
            "Validation loss decreased (0.021344 --> 0.021330).  Saving model ...\n",
            "train_loss:\n",
            "1.0443948942881365\n",
            "Epoch: 299 \tTraining Loss: 1.044395 \tValidation Loss: 0.021315\n",
            "Validation loss decreased (0.021330 --> 0.021315).  Saving model ...\n",
            "train_loss:\n",
            "1.0442541565868881\n",
            "Epoch: 300 \tTraining Loss: 1.044254 \tValidation Loss: 0.021300\n",
            "Validation loss decreased (0.021315 --> 0.021300).  Saving model ...\n",
            "train_loss:\n",
            "1.0441133910483056\n",
            "Epoch: 301 \tTraining Loss: 1.044113 \tValidation Loss: 0.021286\n",
            "Validation loss decreased (0.021300 --> 0.021286).  Saving model ...\n",
            "train_loss:\n",
            "1.0439725803149926\n",
            "Epoch: 302 \tTraining Loss: 1.043973 \tValidation Loss: 0.021271\n",
            "Validation loss decreased (0.021286 --> 0.021271).  Saving model ...\n",
            "train_loss:\n",
            "1.043831733229396\n",
            "Epoch: 303 \tTraining Loss: 1.043832 \tValidation Loss: 0.021257\n",
            "Validation loss decreased (0.021271 --> 0.021257).  Saving model ...\n",
            "train_loss:\n",
            "1.0436908380015866\n",
            "Epoch: 304 \tTraining Loss: 1.043691 \tValidation Loss: 0.021242\n",
            "Validation loss decreased (0.021257 --> 0.021242).  Saving model ...\n",
            "train_loss:\n",
            "1.0435498622092572\n",
            "Epoch: 305 \tTraining Loss: 1.043550 \tValidation Loss: 0.021228\n",
            "Validation loss decreased (0.021242 --> 0.021228).  Saving model ...\n",
            "train_loss:\n",
            "1.0434088320522519\n",
            "Epoch: 306 \tTraining Loss: 1.043409 \tValidation Loss: 0.021213\n",
            "Validation loss decreased (0.021228 --> 0.021213).  Saving model ...\n",
            "train_loss:\n",
            "1.0432677259156993\n",
            "Epoch: 307 \tTraining Loss: 1.043268 \tValidation Loss: 0.021199\n",
            "Validation loss decreased (0.021213 --> 0.021199).  Saving model ...\n",
            "train_loss:\n",
            "1.0431265457645877\n",
            "Epoch: 308 \tTraining Loss: 1.043127 \tValidation Loss: 0.021185\n",
            "Validation loss decreased (0.021199 --> 0.021185).  Saving model ...\n",
            "train_loss:\n",
            "1.042985276861505\n",
            "Epoch: 309 \tTraining Loss: 1.042985 \tValidation Loss: 0.021170\n",
            "Validation loss decreased (0.021185 --> 0.021170).  Saving model ...\n",
            "train_loss:\n",
            "1.0428439028315493\n",
            "Epoch: 310 \tTraining Loss: 1.042844 \tValidation Loss: 0.021156\n",
            "Validation loss decreased (0.021170 --> 0.021156).  Saving model ...\n",
            "train_loss:\n",
            "1.0427024544595362\n",
            "Epoch: 311 \tTraining Loss: 1.042702 \tValidation Loss: 0.021142\n",
            "Validation loss decreased (0.021156 --> 0.021142).  Saving model ...\n",
            "train_loss:\n",
            "1.0425608803282727\n",
            "Epoch: 312 \tTraining Loss: 1.042561 \tValidation Loss: 0.021128\n",
            "Validation loss decreased (0.021142 --> 0.021128).  Saving model ...\n",
            "train_loss:\n",
            "1.0424192203925207\n",
            "Epoch: 313 \tTraining Loss: 1.042419 \tValidation Loss: 0.021114\n",
            "Validation loss decreased (0.021128 --> 0.021114).  Saving model ...\n",
            "train_loss:\n",
            "1.0422774379724984\n",
            "Epoch: 314 \tTraining Loss: 1.042277 \tValidation Loss: 0.021100\n",
            "Validation loss decreased (0.021114 --> 0.021100).  Saving model ...\n",
            "train_loss:\n",
            "1.0421355556655716\n",
            "Epoch: 315 \tTraining Loss: 1.042136 \tValidation Loss: 0.021086\n",
            "Validation loss decreased (0.021100 --> 0.021086).  Saving model ...\n",
            "train_loss:\n",
            "1.0419935629918025\n",
            "Epoch: 316 \tTraining Loss: 1.041994 \tValidation Loss: 0.021072\n",
            "Validation loss decreased (0.021086 --> 0.021072).  Saving model ...\n",
            "train_loss:\n",
            "1.0418514258913942\n",
            "Epoch: 317 \tTraining Loss: 1.041851 \tValidation Loss: 0.021058\n",
            "Validation loss decreased (0.021072 --> 0.021058).  Saving model ...\n",
            "train_loss:\n",
            "1.0417091843191084\n",
            "Epoch: 318 \tTraining Loss: 1.041709 \tValidation Loss: 0.021045\n",
            "Validation loss decreased (0.021058 --> 0.021045).  Saving model ...\n",
            "train_loss:\n",
            "1.041566828777502\n",
            "Epoch: 319 \tTraining Loss: 1.041567 \tValidation Loss: 0.021031\n",
            "Validation loss decreased (0.021045 --> 0.021031).  Saving model ...\n",
            "train_loss:\n",
            "1.0414243445291624\n",
            "Epoch: 320 \tTraining Loss: 1.041424 \tValidation Loss: 0.021017\n",
            "Validation loss decreased (0.021031 --> 0.021017).  Saving model ...\n",
            "train_loss:\n",
            "1.0412817325565842\n",
            "Epoch: 321 \tTraining Loss: 1.041282 \tValidation Loss: 0.021004\n",
            "Validation loss decreased (0.021017 --> 0.021004).  Saving model ...\n",
            "train_loss:\n",
            "1.0411390030122065\n",
            "Epoch: 322 \tTraining Loss: 1.041139 \tValidation Loss: 0.020990\n",
            "Validation loss decreased (0.021004 --> 0.020990).  Saving model ...\n",
            "train_loss:\n",
            "1.0409961467260842\n",
            "Epoch: 323 \tTraining Loss: 1.040996 \tValidation Loss: 0.020977\n",
            "Validation loss decreased (0.020990 --> 0.020977).  Saving model ...\n",
            "train_loss:\n",
            "1.0408531722131666\n",
            "Epoch: 324 \tTraining Loss: 1.040853 \tValidation Loss: 0.020963\n",
            "Validation loss decreased (0.020977 --> 0.020963).  Saving model ...\n",
            "train_loss:\n",
            "1.0407100601510688\n",
            "Epoch: 325 \tTraining Loss: 1.040710 \tValidation Loss: 0.020950\n",
            "Validation loss decreased (0.020963 --> 0.020950).  Saving model ...\n",
            "train_loss:\n",
            "1.0405668226572184\n",
            "Epoch: 326 \tTraining Loss: 1.040567 \tValidation Loss: 0.020937\n",
            "Validation loss decreased (0.020950 --> 0.020937).  Saving model ...\n",
            "train_loss:\n",
            "1.0404234803639925\n",
            "Epoch: 327 \tTraining Loss: 1.040423 \tValidation Loss: 0.020923\n",
            "Validation loss decreased (0.020937 --> 0.020923).  Saving model ...\n",
            "train_loss:\n",
            "1.0402800057615553\n",
            "Epoch: 328 \tTraining Loss: 1.040280 \tValidation Loss: 0.020910\n",
            "Validation loss decreased (0.020923 --> 0.020910).  Saving model ...\n",
            "train_loss:\n",
            "1.0401364073648558\n",
            "Epoch: 329 \tTraining Loss: 1.040136 \tValidation Loss: 0.020897\n",
            "Validation loss decreased (0.020910 --> 0.020897).  Saving model ...\n",
            "train_loss:\n",
            "1.0399926913963569\n",
            "Epoch: 330 \tTraining Loss: 1.039993 \tValidation Loss: 0.020884\n",
            "Validation loss decreased (0.020897 --> 0.020884).  Saving model ...\n",
            "train_loss:\n",
            "1.0398488477036194\n",
            "Epoch: 331 \tTraining Loss: 1.039849 \tValidation Loss: 0.020871\n",
            "Validation loss decreased (0.020884 --> 0.020871).  Saving model ...\n",
            "train_loss:\n",
            "1.0397049005214984\n",
            "Epoch: 332 \tTraining Loss: 1.039705 \tValidation Loss: 0.020858\n",
            "Validation loss decreased (0.020871 --> 0.020858).  Saving model ...\n",
            "train_loss:\n",
            "1.0395608387150608\n",
            "Epoch: 333 \tTraining Loss: 1.039561 \tValidation Loss: 0.020845\n",
            "Validation loss decreased (0.020858 --> 0.020845).  Saving model ...\n",
            "train_loss:\n",
            "1.0394166622843062\n",
            "Epoch: 334 \tTraining Loss: 1.039417 \tValidation Loss: 0.020832\n",
            "Validation loss decreased (0.020845 --> 0.020832).  Saving model ...\n",
            "train_loss:\n",
            "1.03927238039918\n",
            "Epoch: 335 \tTraining Loss: 1.039272 \tValidation Loss: 0.020820\n",
            "Validation loss decreased (0.020832 --> 0.020820).  Saving model ...\n",
            "train_loss:\n",
            "1.0391279953521686\n",
            "Epoch: 336 \tTraining Loss: 1.039128 \tValidation Loss: 0.020807\n",
            "Validation loss decreased (0.020820 --> 0.020807).  Saving model ...\n",
            "train_loss:\n",
            "1.0389834802884321\n",
            "Epoch: 337 \tTraining Loss: 1.038983 \tValidation Loss: 0.020794\n",
            "Validation loss decreased (0.020807 --> 0.020794).  Saving model ...\n",
            "train_loss:\n",
            "1.0388388836776816\n",
            "Epoch: 338 \tTraining Loss: 1.038839 \tValidation Loss: 0.020782\n",
            "Validation loss decreased (0.020794 --> 0.020782).  Saving model ...\n",
            "train_loss:\n",
            "1.038694177355085\n",
            "Epoch: 339 \tTraining Loss: 1.038694 \tValidation Loss: 0.020769\n",
            "Validation loss decreased (0.020782 --> 0.020769).  Saving model ...\n",
            "train_loss:\n",
            "1.0385493714730818\n",
            "Epoch: 340 \tTraining Loss: 1.038549 \tValidation Loss: 0.020757\n",
            "Validation loss decreased (0.020769 --> 0.020757).  Saving model ...\n",
            "train_loss:\n",
            "1.0384044768391074\n",
            "Epoch: 341 \tTraining Loss: 1.038404 \tValidation Loss: 0.020744\n",
            "Validation loss decreased (0.020757 --> 0.020744).  Saving model ...\n",
            "train_loss:\n",
            "1.038259493125664\n",
            "Epoch: 342 \tTraining Loss: 1.038259 \tValidation Loss: 0.020732\n",
            "Validation loss decreased (0.020744 --> 0.020732).  Saving model ...\n",
            "train_loss:\n",
            "1.0381144118178023\n",
            "Epoch: 343 \tTraining Loss: 1.038114 \tValidation Loss: 0.020720\n",
            "Validation loss decreased (0.020732 --> 0.020720).  Saving model ...\n",
            "train_loss:\n",
            "1.0379692545303931\n",
            "Epoch: 344 \tTraining Loss: 1.037969 \tValidation Loss: 0.020708\n",
            "Validation loss decreased (0.020720 --> 0.020708).  Saving model ...\n",
            "train_loss:\n",
            "1.0378240255209117\n",
            "Epoch: 345 \tTraining Loss: 1.037824 \tValidation Loss: 0.020695\n",
            "Validation loss decreased (0.020708 --> 0.020695).  Saving model ...\n",
            "train_loss:\n",
            "1.0376787008819999\n",
            "Epoch: 346 \tTraining Loss: 1.037679 \tValidation Loss: 0.020683\n",
            "Validation loss decreased (0.020695 --> 0.020683).  Saving model ...\n",
            "train_loss:\n",
            "1.0375333163109455\n",
            "Epoch: 347 \tTraining Loss: 1.037533 \tValidation Loss: 0.020671\n",
            "Validation loss decreased (0.020683 --> 0.020671).  Saving model ...\n",
            "train_loss:\n",
            "1.0373878400404375\n",
            "Epoch: 348 \tTraining Loss: 1.037388 \tValidation Loss: 0.020659\n",
            "Validation loss decreased (0.020671 --> 0.020659).  Saving model ...\n",
            "train_loss:\n",
            "1.0372423054752768\n",
            "Epoch: 349 \tTraining Loss: 1.037242 \tValidation Loss: 0.020647\n",
            "Validation loss decreased (0.020659 --> 0.020647).  Saving model ...\n",
            "train_loss:\n",
            "1.0370967122879657\n",
            "Epoch: 350 \tTraining Loss: 1.037097 \tValidation Loss: 0.020635\n",
            "Validation loss decreased (0.020647 --> 0.020635).  Saving model ...\n",
            "train_loss:\n",
            "1.0369510470510839\n",
            "Epoch: 351 \tTraining Loss: 1.036951 \tValidation Loss: 0.020624\n",
            "Validation loss decreased (0.020635 --> 0.020624).  Saving model ...\n",
            "train_loss:\n",
            "1.0368053346544832\n",
            "Epoch: 352 \tTraining Loss: 1.036805 \tValidation Loss: 0.020612\n",
            "Validation loss decreased (0.020624 --> 0.020612).  Saving model ...\n",
            "train_loss:\n",
            "1.0366595525007982\n",
            "Epoch: 353 \tTraining Loss: 1.036660 \tValidation Loss: 0.020600\n",
            "Validation loss decreased (0.020612 --> 0.020600).  Saving model ...\n",
            "train_loss:\n",
            "1.0365137235148922\n",
            "Epoch: 354 \tTraining Loss: 1.036514 \tValidation Loss: 0.020588\n",
            "Validation loss decreased (0.020600 --> 0.020588).  Saving model ...\n",
            "train_loss:\n",
            "1.0363678463867732\n",
            "Epoch: 355 \tTraining Loss: 1.036368 \tValidation Loss: 0.020577\n",
            "Validation loss decreased (0.020588 --> 0.020577).  Saving model ...\n",
            "train_loss:\n",
            "1.0362219279939002\n",
            "Epoch: 356 \tTraining Loss: 1.036222 \tValidation Loss: 0.020565\n",
            "Validation loss decreased (0.020577 --> 0.020565).  Saving model ...\n",
            "train_loss:\n",
            "1.0360759827461872\n",
            "Epoch: 357 \tTraining Loss: 1.036076 \tValidation Loss: 0.020554\n",
            "Validation loss decreased (0.020565 --> 0.020554).  Saving model ...\n",
            "train_loss:\n",
            "1.0359299769113353\n",
            "Epoch: 358 \tTraining Loss: 1.035930 \tValidation Loss: 0.020542\n",
            "Validation loss decreased (0.020554 --> 0.020542).  Saving model ...\n",
            "train_loss:\n",
            "1.0357839466778787\n",
            "Epoch: 359 \tTraining Loss: 1.035784 \tValidation Loss: 0.020531\n",
            "Validation loss decreased (0.020542 --> 0.020531).  Saving model ...\n",
            "train_loss:\n",
            "1.0356378832033701\n",
            "Epoch: 360 \tTraining Loss: 1.035638 \tValidation Loss: 0.020519\n",
            "Validation loss decreased (0.020531 --> 0.020519).  Saving model ...\n",
            "train_loss:\n",
            "1.035491787634053\n",
            "Epoch: 361 \tTraining Loss: 1.035492 \tValidation Loss: 0.020508\n",
            "Validation loss decreased (0.020519 --> 0.020508).  Saving model ...\n",
            "train_loss:\n",
            "1.0353456614436685\n",
            "Epoch: 362 \tTraining Loss: 1.035346 \tValidation Loss: 0.020497\n",
            "Validation loss decreased (0.020508 --> 0.020497).  Saving model ...\n",
            "train_loss:\n",
            "1.035199524445848\n",
            "Epoch: 363 \tTraining Loss: 1.035200 \tValidation Loss: 0.020486\n",
            "Validation loss decreased (0.020497 --> 0.020486).  Saving model ...\n",
            "train_loss:\n",
            "1.0350533640319175\n",
            "Epoch: 364 \tTraining Loss: 1.035053 \tValidation Loss: 0.020475\n",
            "Validation loss decreased (0.020486 --> 0.020475).  Saving model ...\n",
            "train_loss:\n",
            "1.0349071708681818\n",
            "Epoch: 365 \tTraining Loss: 1.034907 \tValidation Loss: 0.020463\n",
            "Validation loss decreased (0.020475 --> 0.020463).  Saving model ...\n",
            "train_loss:\n",
            "1.034760982944415\n",
            "Epoch: 366 \tTraining Loss: 1.034761 \tValidation Loss: 0.020452\n",
            "Validation loss decreased (0.020463 --> 0.020452).  Saving model ...\n",
            "train_loss:\n",
            "1.0346147937106562\n",
            "Epoch: 367 \tTraining Loss: 1.034615 \tValidation Loss: 0.020441\n",
            "Validation loss decreased (0.020452 --> 0.020441).  Saving model ...\n",
            "train_loss:\n",
            "1.0344685812245358\n",
            "Epoch: 368 \tTraining Loss: 1.034469 \tValidation Loss: 0.020430\n",
            "Validation loss decreased (0.020441 --> 0.020430).  Saving model ...\n",
            "train_loss:\n",
            "1.0343223677559212\n",
            "Epoch: 369 \tTraining Loss: 1.034322 \tValidation Loss: 0.020420\n",
            "Validation loss decreased (0.020430 --> 0.020420).  Saving model ...\n",
            "train_loss:\n",
            "1.0341761606735187\n",
            "Epoch: 370 \tTraining Loss: 1.034176 \tValidation Loss: 0.020409\n",
            "Validation loss decreased (0.020420 --> 0.020409).  Saving model ...\n",
            "train_loss:\n",
            "1.034029939836198\n",
            "Epoch: 371 \tTraining Loss: 1.034030 \tValidation Loss: 0.020398\n",
            "Validation loss decreased (0.020409 --> 0.020398).  Saving model ...\n",
            "train_loss:\n",
            "1.0338837227651052\n",
            "Epoch: 372 \tTraining Loss: 1.033884 \tValidation Loss: 0.020387\n",
            "Validation loss decreased (0.020398 --> 0.020387).  Saving model ...\n",
            "train_loss:\n",
            "1.0337375287826245\n",
            "Epoch: 373 \tTraining Loss: 1.033738 \tValidation Loss: 0.020376\n",
            "Validation loss decreased (0.020387 --> 0.020376).  Saving model ...\n",
            "train_loss:\n",
            "1.0335913488825599\n",
            "Epoch: 374 \tTraining Loss: 1.033591 \tValidation Loss: 0.020366\n",
            "Validation loss decreased (0.020376 --> 0.020366).  Saving model ...\n",
            "train_loss:\n",
            "1.0334451573563146\n",
            "Epoch: 375 \tTraining Loss: 1.033445 \tValidation Loss: 0.020355\n",
            "Validation loss decreased (0.020366 --> 0.020355).  Saving model ...\n",
            "train_loss:\n",
            "1.0332989923574112\n",
            "Epoch: 376 \tTraining Loss: 1.033299 \tValidation Loss: 0.020345\n",
            "Validation loss decreased (0.020355 --> 0.020345).  Saving model ...\n",
            "train_loss:\n",
            "1.0331528343997158\n",
            "Epoch: 377 \tTraining Loss: 1.033153 \tValidation Loss: 0.020334\n",
            "Validation loss decreased (0.020345 --> 0.020334).  Saving model ...\n",
            "train_loss:\n",
            "1.0330066964194016\n",
            "Epoch: 378 \tTraining Loss: 1.033007 \tValidation Loss: 0.020324\n",
            "Validation loss decreased (0.020334 --> 0.020324).  Saving model ...\n",
            "train_loss:\n",
            "1.0328605897151506\n",
            "Epoch: 379 \tTraining Loss: 1.032861 \tValidation Loss: 0.020313\n",
            "Validation loss decreased (0.020324 --> 0.020313).  Saving model ...\n",
            "train_loss:\n",
            "1.0327145054445162\n",
            "Epoch: 380 \tTraining Loss: 1.032715 \tValidation Loss: 0.020303\n",
            "Validation loss decreased (0.020313 --> 0.020303).  Saving model ...\n",
            "train_loss:\n",
            "1.032568436075043\n",
            "Epoch: 381 \tTraining Loss: 1.032568 \tValidation Loss: 0.020292\n",
            "Validation loss decreased (0.020303 --> 0.020292).  Saving model ...\n",
            "train_loss:\n",
            "1.032422384062966\n",
            "Epoch: 382 \tTraining Loss: 1.032422 \tValidation Loss: 0.020282\n",
            "Validation loss decreased (0.020292 --> 0.020282).  Saving model ...\n",
            "train_loss:\n",
            "1.0322763883805537\n",
            "Epoch: 383 \tTraining Loss: 1.032276 \tValidation Loss: 0.020272\n",
            "Validation loss decreased (0.020282 --> 0.020272).  Saving model ...\n",
            "train_loss:\n",
            "1.0321303933531374\n",
            "Epoch: 384 \tTraining Loss: 1.032130 \tValidation Loss: 0.020262\n",
            "Validation loss decreased (0.020272 --> 0.020262).  Saving model ...\n",
            "train_loss:\n",
            "1.0319844579303659\n",
            "Epoch: 385 \tTraining Loss: 1.031984 \tValidation Loss: 0.020251\n",
            "Validation loss decreased (0.020262 --> 0.020251).  Saving model ...\n",
            "train_loss:\n",
            "1.0318385362625122\n",
            "Epoch: 386 \tTraining Loss: 1.031839 \tValidation Loss: 0.020241\n",
            "Validation loss decreased (0.020251 --> 0.020241).  Saving model ...\n",
            "train_loss:\n",
            "1.031692663719366\n",
            "Epoch: 387 \tTraining Loss: 1.031693 \tValidation Loss: 0.020231\n",
            "Validation loss decreased (0.020241 --> 0.020231).  Saving model ...\n",
            "train_loss:\n",
            "1.0315468083698671\n",
            "Epoch: 388 \tTraining Loss: 1.031547 \tValidation Loss: 0.020221\n",
            "Validation loss decreased (0.020231 --> 0.020221).  Saving model ...\n",
            "train_loss:\n",
            "1.0314010101687776\n",
            "Epoch: 389 \tTraining Loss: 1.031401 \tValidation Loss: 0.020211\n",
            "Validation loss decreased (0.020221 --> 0.020211).  Saving model ...\n",
            "train_loss:\n",
            "1.031255237021289\n",
            "Epoch: 390 \tTraining Loss: 1.031255 \tValidation Loss: 0.020201\n",
            "Validation loss decreased (0.020211 --> 0.020201).  Saving model ...\n",
            "train_loss:\n",
            "1.0311095203672136\n",
            "Epoch: 391 \tTraining Loss: 1.031110 \tValidation Loss: 0.020191\n",
            "Validation loss decreased (0.020201 --> 0.020191).  Saving model ...\n",
            "train_loss:\n",
            "1.0309638413754139\n",
            "Epoch: 392 \tTraining Loss: 1.030964 \tValidation Loss: 0.020182\n",
            "Validation loss decreased (0.020191 --> 0.020182).  Saving model ...\n",
            "train_loss:\n",
            "1.030818202993372\n",
            "Epoch: 393 \tTraining Loss: 1.030818 \tValidation Loss: 0.020172\n",
            "Validation loss decreased (0.020182 --> 0.020172).  Saving model ...\n",
            "train_loss:\n",
            "1.0306726060398332\n",
            "Epoch: 394 \tTraining Loss: 1.030673 \tValidation Loss: 0.020162\n",
            "Validation loss decreased (0.020172 --> 0.020162).  Saving model ...\n",
            "train_loss:\n",
            "1.0305270659072059\n",
            "Epoch: 395 \tTraining Loss: 1.030527 \tValidation Loss: 0.020152\n",
            "Validation loss decreased (0.020162 --> 0.020152).  Saving model ...\n",
            "train_loss:\n",
            "1.0303815801392544\n",
            "Epoch: 396 \tTraining Loss: 1.030382 \tValidation Loss: 0.020143\n",
            "Validation loss decreased (0.020152 --> 0.020143).  Saving model ...\n",
            "train_loss:\n",
            "1.0302361186061586\n",
            "Epoch: 397 \tTraining Loss: 1.030236 \tValidation Loss: 0.020133\n",
            "Validation loss decreased (0.020143 --> 0.020133).  Saving model ...\n",
            "train_loss:\n",
            "1.030090718970194\n",
            "Epoch: 398 \tTraining Loss: 1.030091 \tValidation Loss: 0.020123\n",
            "Validation loss decreased (0.020133 --> 0.020123).  Saving model ...\n",
            "train_loss:\n",
            "1.0299453745176503\n",
            "Epoch: 399 \tTraining Loss: 1.029945 \tValidation Loss: 0.020114\n",
            "Validation loss decreased (0.020123 --> 0.020114).  Saving model ...\n",
            "train_loss:\n",
            "1.0298000808273042\n",
            "Epoch: 400 \tTraining Loss: 1.029800 \tValidation Loss: 0.020104\n",
            "Validation loss decreased (0.020114 --> 0.020104).  Saving model ...\n",
            "train_loss:\n",
            "1.0296548390453988\n",
            "Epoch: 401 \tTraining Loss: 1.029655 \tValidation Loss: 0.020095\n",
            "Validation loss decreased (0.020104 --> 0.020095).  Saving model ...\n",
            "train_loss:\n",
            "1.0295096232995882\n",
            "Epoch: 402 \tTraining Loss: 1.029510 \tValidation Loss: 0.020086\n",
            "Validation loss decreased (0.020095 --> 0.020086).  Saving model ...\n",
            "train_loss:\n",
            "1.029364479439599\n",
            "Epoch: 403 \tTraining Loss: 1.029364 \tValidation Loss: 0.020076\n",
            "Validation loss decreased (0.020086 --> 0.020076).  Saving model ...\n",
            "train_loss:\n",
            "1.029219395020506\n",
            "Epoch: 404 \tTraining Loss: 1.029219 \tValidation Loss: 0.020067\n",
            "Validation loss decreased (0.020076 --> 0.020067).  Saving model ...\n",
            "train_loss:\n",
            "1.0290743657848338\n",
            "Epoch: 405 \tTraining Loss: 1.029074 \tValidation Loss: 0.020058\n",
            "Validation loss decreased (0.020067 --> 0.020058).  Saving model ...\n",
            "train_loss:\n",
            "1.0289293817438923\n",
            "Epoch: 406 \tTraining Loss: 1.028929 \tValidation Loss: 0.020048\n",
            "Validation loss decreased (0.020058 --> 0.020048).  Saving model ...\n",
            "train_loss:\n",
            "1.028784455015109\n",
            "Epoch: 407 \tTraining Loss: 1.028784 \tValidation Loss: 0.020039\n",
            "Validation loss decreased (0.020048 --> 0.020039).  Saving model ...\n",
            "train_loss:\n",
            "1.0286395837972453\n",
            "Epoch: 408 \tTraining Loss: 1.028640 \tValidation Loss: 0.020030\n",
            "Validation loss decreased (0.020039 --> 0.020030).  Saving model ...\n",
            "train_loss:\n",
            "1.0284947672715554\n",
            "Epoch: 409 \tTraining Loss: 1.028495 \tValidation Loss: 0.020021\n",
            "Validation loss decreased (0.020030 --> 0.020021).  Saving model ...\n",
            "train_loss:\n",
            "1.0283500247604245\n",
            "Epoch: 410 \tTraining Loss: 1.028350 \tValidation Loss: 0.020012\n",
            "Validation loss decreased (0.020021 --> 0.020012).  Saving model ...\n",
            "train_loss:\n",
            "1.0282053274440242\n",
            "Epoch: 411 \tTraining Loss: 1.028205 \tValidation Loss: 0.020003\n",
            "Validation loss decreased (0.020012 --> 0.020003).  Saving model ...\n",
            "train_loss:\n",
            "1.028060672047374\n",
            "Epoch: 412 \tTraining Loss: 1.028061 \tValidation Loss: 0.019994\n",
            "Validation loss decreased (0.020003 --> 0.019994).  Saving model ...\n",
            "train_loss:\n",
            "1.02791609279402\n",
            "Epoch: 413 \tTraining Loss: 1.027916 \tValidation Loss: 0.019985\n",
            "Validation loss decreased (0.019994 --> 0.019985).  Saving model ...\n",
            "train_loss:\n",
            "1.0277715680690913\n",
            "Epoch: 414 \tTraining Loss: 1.027772 \tValidation Loss: 0.019976\n",
            "Validation loss decreased (0.019985 --> 0.019976).  Saving model ...\n",
            "train_loss:\n",
            "1.027627096235097\n",
            "Epoch: 415 \tTraining Loss: 1.027627 \tValidation Loss: 0.019967\n",
            "Validation loss decreased (0.019976 --> 0.019967).  Saving model ...\n",
            "train_loss:\n",
            "1.0274826815495124\n",
            "Epoch: 416 \tTraining Loss: 1.027483 \tValidation Loss: 0.019959\n",
            "Validation loss decreased (0.019967 --> 0.019959).  Saving model ...\n",
            "train_loss:\n",
            "1.027338330071051\n",
            "Epoch: 417 \tTraining Loss: 1.027338 \tValidation Loss: 0.019950\n",
            "Validation loss decreased (0.019959 --> 0.019950).  Saving model ...\n",
            "train_loss:\n",
            "1.0271940373784894\n",
            "Epoch: 418 \tTraining Loss: 1.027194 \tValidation Loss: 0.019941\n",
            "Validation loss decreased (0.019950 --> 0.019941).  Saving model ...\n",
            "train_loss:\n",
            "1.0270498123142746\n",
            "Epoch: 419 \tTraining Loss: 1.027050 \tValidation Loss: 0.019933\n",
            "Validation loss decreased (0.019941 --> 0.019933).  Saving model ...\n",
            "train_loss:\n",
            "1.0269056206548608\n",
            "Epoch: 420 \tTraining Loss: 1.026906 \tValidation Loss: 0.019924\n",
            "Validation loss decreased (0.019933 --> 0.019924).  Saving model ...\n",
            "train_loss:\n",
            "1.0267615151274336\n",
            "Epoch: 421 \tTraining Loss: 1.026762 \tValidation Loss: 0.019916\n",
            "Validation loss decreased (0.019924 --> 0.019916).  Saving model ...\n",
            "train_loss:\n",
            "1.0266174392385796\n",
            "Epoch: 422 \tTraining Loss: 1.026617 \tValidation Loss: 0.019907\n",
            "Validation loss decreased (0.019916 --> 0.019907).  Saving model ...\n",
            "train_loss:\n",
            "1.0264734426042537\n",
            "Epoch: 423 \tTraining Loss: 1.026473 \tValidation Loss: 0.019899\n",
            "Validation loss decreased (0.019907 --> 0.019899).  Saving model ...\n",
            "train_loss:\n",
            "1.0263295001708543\n",
            "Epoch: 424 \tTraining Loss: 1.026330 \tValidation Loss: 0.019890\n",
            "Validation loss decreased (0.019899 --> 0.019890).  Saving model ...\n",
            "train_loss:\n",
            "1.0261856328982573\n",
            "Epoch: 425 \tTraining Loss: 1.026186 \tValidation Loss: 0.019882\n",
            "Validation loss decreased (0.019890 --> 0.019882).  Saving model ...\n",
            "train_loss:\n",
            "1.0260418062354182\n",
            "Epoch: 426 \tTraining Loss: 1.026042 \tValidation Loss: 0.019874\n",
            "Validation loss decreased (0.019882 --> 0.019874).  Saving model ...\n",
            "train_loss:\n",
            "1.025898033446008\n",
            "Epoch: 427 \tTraining Loss: 1.025898 \tValidation Loss: 0.019865\n",
            "Validation loss decreased (0.019874 --> 0.019865).  Saving model ...\n",
            "train_loss:\n",
            "1.0257543179687563\n",
            "Epoch: 428 \tTraining Loss: 1.025754 \tValidation Loss: 0.019857\n",
            "Validation loss decreased (0.019865 --> 0.019857).  Saving model ...\n",
            "train_loss:\n",
            "1.0256106711023456\n",
            "Epoch: 429 \tTraining Loss: 1.025611 \tValidation Loss: 0.019849\n",
            "Validation loss decreased (0.019857 --> 0.019849).  Saving model ...\n",
            "train_loss:\n",
            "1.0254670880980543\n",
            "Epoch: 430 \tTraining Loss: 1.025467 \tValidation Loss: 0.019841\n",
            "Validation loss decreased (0.019849 --> 0.019841).  Saving model ...\n",
            "train_loss:\n",
            "1.0253235625696706\n",
            "Epoch: 431 \tTraining Loss: 1.025324 \tValidation Loss: 0.019833\n",
            "Validation loss decreased (0.019841 --> 0.019833).  Saving model ...\n",
            "train_loss:\n",
            "1.0251800868209902\n",
            "Epoch: 432 \tTraining Loss: 1.025180 \tValidation Loss: 0.019825\n",
            "Validation loss decreased (0.019833 --> 0.019825).  Saving model ...\n",
            "train_loss:\n",
            "1.0250366754256761\n",
            "Epoch: 433 \tTraining Loss: 1.025037 \tValidation Loss: 0.019817\n",
            "Validation loss decreased (0.019825 --> 0.019817).  Saving model ...\n",
            "train_loss:\n",
            "1.0248933260912423\n",
            "Epoch: 434 \tTraining Loss: 1.024893 \tValidation Loss: 0.019809\n",
            "Validation loss decreased (0.019817 --> 0.019809).  Saving model ...\n",
            "train_loss:\n",
            "1.0247500425839162\n",
            "Epoch: 435 \tTraining Loss: 1.024750 \tValidation Loss: 0.019801\n",
            "Validation loss decreased (0.019809 --> 0.019801).  Saving model ...\n",
            "train_loss:\n",
            "1.024606807055054\n",
            "Epoch: 436 \tTraining Loss: 1.024607 \tValidation Loss: 0.019793\n",
            "Validation loss decreased (0.019801 --> 0.019793).  Saving model ...\n",
            "train_loss:\n",
            "1.0244636347333154\n",
            "Epoch: 437 \tTraining Loss: 1.024464 \tValidation Loss: 0.019786\n",
            "Validation loss decreased (0.019793 --> 0.019786).  Saving model ...\n",
            "train_loss:\n",
            "1.0243205203787311\n",
            "Epoch: 438 \tTraining Loss: 1.024321 \tValidation Loss: 0.019778\n",
            "Validation loss decreased (0.019786 --> 0.019778).  Saving model ...\n",
            "train_loss:\n",
            "1.0241774666112857\n",
            "Epoch: 439 \tTraining Loss: 1.024177 \tValidation Loss: 0.019770\n",
            "Validation loss decreased (0.019778 --> 0.019770).  Saving model ...\n",
            "train_loss:\n",
            "1.0240344606585554\n",
            "Epoch: 440 \tTraining Loss: 1.024034 \tValidation Loss: 0.019763\n",
            "Validation loss decreased (0.019770 --> 0.019763).  Saving model ...\n",
            "train_loss:\n",
            "1.0238915333053569\n",
            "Epoch: 441 \tTraining Loss: 1.023892 \tValidation Loss: 0.019755\n",
            "Validation loss decreased (0.019763 --> 0.019755).  Saving model ...\n",
            "train_loss:\n",
            "1.0237486634280655\n",
            "Epoch: 442 \tTraining Loss: 1.023749 \tValidation Loss: 0.019748\n",
            "Validation loss decreased (0.019755 --> 0.019748).  Saving model ...\n",
            "train_loss:\n",
            "1.0236058569216466\n",
            "Epoch: 443 \tTraining Loss: 1.023606 \tValidation Loss: 0.019740\n",
            "Validation loss decreased (0.019748 --> 0.019740).  Saving model ...\n",
            "train_loss:\n",
            "1.0234631078911351\n",
            "Epoch: 444 \tTraining Loss: 1.023463 \tValidation Loss: 0.019733\n",
            "Validation loss decreased (0.019740 --> 0.019733).  Saving model ...\n",
            "train_loss:\n",
            "1.0233203927566716\n",
            "Epoch: 445 \tTraining Loss: 1.023320 \tValidation Loss: 0.019725\n",
            "Validation loss decreased (0.019733 --> 0.019725).  Saving model ...\n",
            "train_loss:\n",
            "1.0231777596604692\n",
            "Epoch: 446 \tTraining Loss: 1.023178 \tValidation Loss: 0.019718\n",
            "Validation loss decreased (0.019725 --> 0.019718).  Saving model ...\n",
            "train_loss:\n",
            "1.0230351799464488\n",
            "Epoch: 447 \tTraining Loss: 1.023035 \tValidation Loss: 0.019711\n",
            "Validation loss decreased (0.019718 --> 0.019711).  Saving model ...\n",
            "train_loss:\n",
            "1.0228926511583747\n",
            "Epoch: 448 \tTraining Loss: 1.022893 \tValidation Loss: 0.019704\n",
            "Validation loss decreased (0.019711 --> 0.019704).  Saving model ...\n",
            "train_loss:\n",
            "1.0227502019523265\n",
            "Epoch: 449 \tTraining Loss: 1.022750 \tValidation Loss: 0.019696\n",
            "Validation loss decreased (0.019704 --> 0.019696).  Saving model ...\n",
            "train_loss:\n",
            "1.0226077874610713\n",
            "Epoch: 450 \tTraining Loss: 1.022608 \tValidation Loss: 0.019689\n",
            "Validation loss decreased (0.019696 --> 0.019689).  Saving model ...\n",
            "train_loss:\n",
            "1.0224654551718262\n",
            "Epoch: 451 \tTraining Loss: 1.022465 \tValidation Loss: 0.019682\n",
            "Validation loss decreased (0.019689 --> 0.019682).  Saving model ...\n",
            "train_loss:\n",
            "1.0223231523574055\n",
            "Epoch: 452 \tTraining Loss: 1.022323 \tValidation Loss: 0.019675\n",
            "Validation loss decreased (0.019682 --> 0.019675).  Saving model ...\n",
            "train_loss:\n",
            "1.0221809286337633\n",
            "Epoch: 453 \tTraining Loss: 1.022181 \tValidation Loss: 0.019668\n",
            "Validation loss decreased (0.019675 --> 0.019668).  Saving model ...\n",
            "train_loss:\n",
            "1.0220387702459817\n",
            "Epoch: 454 \tTraining Loss: 1.022039 \tValidation Loss: 0.019661\n",
            "Validation loss decreased (0.019668 --> 0.019661).  Saving model ...\n",
            "train_loss:\n",
            "1.0218966609829074\n",
            "Epoch: 455 \tTraining Loss: 1.021897 \tValidation Loss: 0.019654\n",
            "Validation loss decreased (0.019661 --> 0.019654).  Saving model ...\n",
            "train_loss:\n",
            "1.021754603955772\n",
            "Epoch: 456 \tTraining Loss: 1.021755 \tValidation Loss: 0.019648\n",
            "Validation loss decreased (0.019654 --> 0.019648).  Saving model ...\n",
            "train_loss:\n",
            "1.021612606205783\n",
            "Epoch: 457 \tTraining Loss: 1.021613 \tValidation Loss: 0.019641\n",
            "Validation loss decreased (0.019648 --> 0.019641).  Saving model ...\n",
            "train_loss:\n",
            "1.0214706787041254\n",
            "Epoch: 458 \tTraining Loss: 1.021471 \tValidation Loss: 0.019634\n",
            "Validation loss decreased (0.019641 --> 0.019634).  Saving model ...\n",
            "train_loss:\n",
            "1.0213288072046343\n",
            "Epoch: 459 \tTraining Loss: 1.021329 \tValidation Loss: 0.019627\n",
            "Validation loss decreased (0.019634 --> 0.019627).  Saving model ...\n",
            "train_loss:\n",
            "1.021187002514745\n",
            "Epoch: 460 \tTraining Loss: 1.021187 \tValidation Loss: 0.019621\n",
            "Validation loss decreased (0.019627 --> 0.019621).  Saving model ...\n",
            "train_loss:\n",
            "1.0210452516982844\n",
            "Epoch: 461 \tTraining Loss: 1.021045 \tValidation Loss: 0.019614\n",
            "Validation loss decreased (0.019621 --> 0.019614).  Saving model ...\n",
            "train_loss:\n",
            "1.0209035675276767\n",
            "Epoch: 462 \tTraining Loss: 1.020904 \tValidation Loss: 0.019608\n",
            "Validation loss decreased (0.019614 --> 0.019608).  Saving model ...\n",
            "train_loss:\n",
            "1.0207619183993601\n",
            "Epoch: 463 \tTraining Loss: 1.020762 \tValidation Loss: 0.019601\n",
            "Validation loss decreased (0.019608 --> 0.019601).  Saving model ...\n",
            "train_loss:\n",
            "1.0206203780003957\n",
            "Epoch: 464 \tTraining Loss: 1.020620 \tValidation Loss: 0.019595\n",
            "Validation loss decreased (0.019601 --> 0.019595).  Saving model ...\n",
            "train_loss:\n",
            "1.0204788621637848\n",
            "Epoch: 465 \tTraining Loss: 1.020479 \tValidation Loss: 0.019588\n",
            "Validation loss decreased (0.019595 --> 0.019588).  Saving model ...\n",
            "train_loss:\n",
            "1.0203374069143127\n",
            "Epoch: 466 \tTraining Loss: 1.020337 \tValidation Loss: 0.019582\n",
            "Validation loss decreased (0.019588 --> 0.019582).  Saving model ...\n",
            "train_loss:\n",
            "1.0201960129069758\n",
            "Epoch: 467 \tTraining Loss: 1.020196 \tValidation Loss: 0.019576\n",
            "Validation loss decreased (0.019582 --> 0.019576).  Saving model ...\n",
            "train_loss:\n",
            "1.0200547047041275\n",
            "Epoch: 468 \tTraining Loss: 1.020055 \tValidation Loss: 0.019570\n",
            "Validation loss decreased (0.019576 --> 0.019570).  Saving model ...\n",
            "train_loss:\n",
            "1.0199134394035234\n",
            "Epoch: 469 \tTraining Loss: 1.019913 \tValidation Loss: 0.019564\n",
            "Validation loss decreased (0.019570 --> 0.019564).  Saving model ...\n",
            "train_loss:\n",
            "1.0197722436962546\n",
            "Epoch: 470 \tTraining Loss: 1.019772 \tValidation Loss: 0.019557\n",
            "Validation loss decreased (0.019564 --> 0.019557).  Saving model ...\n",
            "train_loss:\n",
            "1.0196311053011444\n",
            "Epoch: 471 \tTraining Loss: 1.019631 \tValidation Loss: 0.019551\n",
            "Validation loss decreased (0.019557 --> 0.019551).  Saving model ...\n",
            "train_loss:\n",
            "1.0194900188144747\n",
            "Epoch: 472 \tTraining Loss: 1.019490 \tValidation Loss: 0.019545\n",
            "Validation loss decreased (0.019551 --> 0.019545).  Saving model ...\n",
            "train_loss:\n",
            "1.0193490056873677\n",
            "Epoch: 473 \tTraining Loss: 1.019349 \tValidation Loss: 0.019539\n",
            "Validation loss decreased (0.019545 --> 0.019539).  Saving model ...\n",
            "train_loss:\n",
            "1.019208041848717\n",
            "Epoch: 474 \tTraining Loss: 1.019208 \tValidation Loss: 0.019533\n",
            "Validation loss decreased (0.019539 --> 0.019533).  Saving model ...\n",
            "train_loss:\n",
            "1.0190671729845004\n",
            "Epoch: 475 \tTraining Loss: 1.019067 \tValidation Loss: 0.019528\n",
            "Validation loss decreased (0.019533 --> 0.019528).  Saving model ...\n",
            "train_loss:\n",
            "1.0189263399813202\n",
            "Epoch: 476 \tTraining Loss: 1.018926 \tValidation Loss: 0.019522\n",
            "Validation loss decreased (0.019528 --> 0.019522).  Saving model ...\n",
            "train_loss:\n",
            "1.0187855862326674\n",
            "Epoch: 477 \tTraining Loss: 1.018786 \tValidation Loss: 0.019516\n",
            "Validation loss decreased (0.019522 --> 0.019516).  Saving model ...\n",
            "train_loss:\n",
            "1.0186448855386985\n",
            "Epoch: 478 \tTraining Loss: 1.018645 \tValidation Loss: 0.019510\n",
            "Validation loss decreased (0.019516 --> 0.019510).  Saving model ...\n",
            "train_loss:\n",
            "1.0185042395369037\n",
            "Epoch: 479 \tTraining Loss: 1.018504 \tValidation Loss: 0.019505\n",
            "Validation loss decreased (0.019510 --> 0.019505).  Saving model ...\n",
            "train_loss:\n",
            "1.0183636783571033\n",
            "Epoch: 480 \tTraining Loss: 1.018364 \tValidation Loss: 0.019499\n",
            "Validation loss decreased (0.019505 --> 0.019499).  Saving model ...\n",
            "train_loss:\n",
            "1.018223198396819\n",
            "Epoch: 481 \tTraining Loss: 1.018223 \tValidation Loss: 0.019494\n",
            "Validation loss decreased (0.019499 --> 0.019494).  Saving model ...\n",
            "train_loss:\n",
            "1.018082733992692\n",
            "Epoch: 482 \tTraining Loss: 1.018083 \tValidation Loss: 0.019488\n",
            "Validation loss decreased (0.019494 --> 0.019488).  Saving model ...\n",
            "train_loss:\n",
            "1.0179423591592809\n",
            "Epoch: 483 \tTraining Loss: 1.017942 \tValidation Loss: 0.019483\n",
            "Validation loss decreased (0.019488 --> 0.019483).  Saving model ...\n",
            "train_loss:\n",
            "1.0178020607966642\n",
            "Epoch: 484 \tTraining Loss: 1.017802 \tValidation Loss: 0.019477\n",
            "Validation loss decreased (0.019483 --> 0.019477).  Saving model ...\n",
            "train_loss:\n",
            "1.0176618164712257\n",
            "Epoch: 485 \tTraining Loss: 1.017662 \tValidation Loss: 0.019472\n",
            "Validation loss decreased (0.019477 --> 0.019472).  Saving model ...\n",
            "train_loss:\n",
            "1.0175216379728946\n",
            "Epoch: 486 \tTraining Loss: 1.017522 \tValidation Loss: 0.019467\n",
            "Validation loss decreased (0.019472 --> 0.019467).  Saving model ...\n",
            "train_loss:\n",
            "1.0173815218629418\n",
            "Epoch: 487 \tTraining Loss: 1.017382 \tValidation Loss: 0.019461\n",
            "Validation loss decreased (0.019467 --> 0.019461).  Saving model ...\n",
            "train_loss:\n",
            "1.0172414820600342\n",
            "Epoch: 488 \tTraining Loss: 1.017241 \tValidation Loss: 0.019456\n",
            "Validation loss decreased (0.019461 --> 0.019456).  Saving model ...\n",
            "train_loss:\n",
            "1.0171015049730028\n",
            "Epoch: 489 \tTraining Loss: 1.017102 \tValidation Loss: 0.019451\n",
            "Validation loss decreased (0.019456 --> 0.019451).  Saving model ...\n",
            "train_loss:\n",
            "1.0169616019005303\n",
            "Epoch: 490 \tTraining Loss: 1.016962 \tValidation Loss: 0.019446\n",
            "Validation loss decreased (0.019451 --> 0.019446).  Saving model ...\n",
            "train_loss:\n",
            "1.0168217759538483\n",
            "Epoch: 491 \tTraining Loss: 1.016822 \tValidation Loss: 0.019441\n",
            "Validation loss decreased (0.019446 --> 0.019441).  Saving model ...\n",
            "train_loss:\n",
            "1.0166820120680464\n",
            "Epoch: 492 \tTraining Loss: 1.016682 \tValidation Loss: 0.019436\n",
            "Validation loss decreased (0.019441 --> 0.019436).  Saving model ...\n",
            "train_loss:\n",
            "1.0165423161380893\n",
            "Epoch: 493 \tTraining Loss: 1.016542 \tValidation Loss: 0.019431\n",
            "Validation loss decreased (0.019436 --> 0.019431).  Saving model ...\n",
            "train_loss:\n",
            "1.0164027101063466\n",
            "Epoch: 494 \tTraining Loss: 1.016403 \tValidation Loss: 0.019426\n",
            "Validation loss decreased (0.019431 --> 0.019426).  Saving model ...\n",
            "train_loss:\n",
            "1.0162631576205348\n",
            "Epoch: 495 \tTraining Loss: 1.016263 \tValidation Loss: 0.019421\n",
            "Validation loss decreased (0.019426 --> 0.019421).  Saving model ...\n",
            "train_loss:\n",
            "1.0161236745643092\n",
            "Epoch: 496 \tTraining Loss: 1.016124 \tValidation Loss: 0.019417\n",
            "Validation loss decreased (0.019421 --> 0.019417).  Saving model ...\n",
            "train_loss:\n",
            "1.0159842732188467\n",
            "Epoch: 497 \tTraining Loss: 1.015984 \tValidation Loss: 0.019412\n",
            "Validation loss decreased (0.019417 --> 0.019412).  Saving model ...\n",
            "train_loss:\n",
            "1.0158449557128844\n",
            "Epoch: 498 \tTraining Loss: 1.015845 \tValidation Loss: 0.019407\n",
            "Validation loss decreased (0.019412 --> 0.019407).  Saving model ...\n",
            "train_loss:\n",
            "1.015705713040226\n",
            "Epoch: 499 \tTraining Loss: 1.015706 \tValidation Loss: 0.019403\n",
            "Validation loss decreased (0.019407 --> 0.019403).  Saving model ...\n",
            "train_loss:\n",
            "1.0155665415983934\n",
            "Epoch: 500 \tTraining Loss: 1.015567 \tValidation Loss: 0.019398\n",
            "Validation loss decreased (0.019403 --> 0.019398).  Saving model ...\n",
            "train_loss:\n",
            "1.0154274517035746\n",
            "Epoch: 501 \tTraining Loss: 1.015427 \tValidation Loss: 0.019394\n",
            "Validation loss decreased (0.019398 --> 0.019394).  Saving model ...\n",
            "train_loss:\n",
            "1.0152884523619663\n",
            "Epoch: 502 \tTraining Loss: 1.015288 \tValidation Loss: 0.019389\n",
            "Validation loss decreased (0.019394 --> 0.019389).  Saving model ...\n",
            "train_loss:\n",
            "1.0151495190112145\n",
            "Epoch: 503 \tTraining Loss: 1.015150 \tValidation Loss: 0.019385\n",
            "Validation loss decreased (0.019389 --> 0.019385).  Saving model ...\n",
            "train_loss:\n",
            "1.0150106706462063\n",
            "Epoch: 504 \tTraining Loss: 1.015011 \tValidation Loss: 0.019381\n",
            "Validation loss decreased (0.019385 --> 0.019381).  Saving model ...\n",
            "train_loss:\n",
            "1.0148718944945179\n",
            "Epoch: 505 \tTraining Loss: 1.014872 \tValidation Loss: 0.019376\n",
            "Validation loss decreased (0.019381 --> 0.019376).  Saving model ...\n",
            "train_loss:\n",
            "1.014733211516024\n",
            "Epoch: 506 \tTraining Loss: 1.014733 \tValidation Loss: 0.019372\n",
            "Validation loss decreased (0.019376 --> 0.019372).  Saving model ...\n",
            "train_loss:\n",
            "1.0145946120495324\n",
            "Epoch: 507 \tTraining Loss: 1.014595 \tValidation Loss: 0.019368\n",
            "Validation loss decreased (0.019372 --> 0.019368).  Saving model ...\n",
            "train_loss:\n",
            "1.0144560902000783\n",
            "Epoch: 508 \tTraining Loss: 1.014456 \tValidation Loss: 0.019364\n",
            "Validation loss decreased (0.019368 --> 0.019364).  Saving model ...\n",
            "train_loss:\n",
            "1.0143176472776538\n",
            "Epoch: 509 \tTraining Loss: 1.014318 \tValidation Loss: 0.019360\n",
            "Validation loss decreased (0.019364 --> 0.019360).  Saving model ...\n",
            "train_loss:\n",
            "1.0141793008034046\n",
            "Epoch: 510 \tTraining Loss: 1.014179 \tValidation Loss: 0.019356\n",
            "Validation loss decreased (0.019360 --> 0.019356).  Saving model ...\n",
            "train_loss:\n",
            "1.0140410491398402\n",
            "Epoch: 511 \tTraining Loss: 1.014041 \tValidation Loss: 0.019352\n",
            "Validation loss decreased (0.019356 --> 0.019352).  Saving model ...\n",
            "train_loss:\n",
            "1.013902871818333\n",
            "Epoch: 512 \tTraining Loss: 1.013903 \tValidation Loss: 0.019348\n",
            "Validation loss decreased (0.019352 --> 0.019348).  Saving model ...\n",
            "train_loss:\n",
            "1.0137648048636678\n",
            "Epoch: 513 \tTraining Loss: 1.013765 \tValidation Loss: 0.019344\n",
            "Validation loss decreased (0.019348 --> 0.019344).  Saving model ...\n",
            "train_loss:\n",
            "1.013626815362291\n",
            "Epoch: 514 \tTraining Loss: 1.013627 \tValidation Loss: 0.019340\n",
            "Validation loss decreased (0.019344 --> 0.019340).  Saving model ...\n",
            "train_loss:\n",
            "1.013488918379113\n",
            "Epoch: 515 \tTraining Loss: 1.013489 \tValidation Loss: 0.019337\n",
            "Validation loss decreased (0.019340 --> 0.019337).  Saving model ...\n",
            "train_loss:\n",
            "1.0133511168616158\n",
            "Epoch: 516 \tTraining Loss: 1.013351 \tValidation Loss: 0.019333\n",
            "Validation loss decreased (0.019337 --> 0.019333).  Saving model ...\n",
            "train_loss:\n",
            "1.0132134342259103\n",
            "Epoch: 517 \tTraining Loss: 1.013213 \tValidation Loss: 0.019329\n",
            "Validation loss decreased (0.019333 --> 0.019329).  Saving model ...\n",
            "train_loss:\n",
            "1.01307582789725\n",
            "Epoch: 518 \tTraining Loss: 1.013076 \tValidation Loss: 0.019326\n",
            "Validation loss decreased (0.019329 --> 0.019326).  Saving model ...\n",
            "train_loss:\n",
            "1.0129383216192434\n",
            "Epoch: 519 \tTraining Loss: 1.012938 \tValidation Loss: 0.019322\n",
            "Validation loss decreased (0.019326 --> 0.019322).  Saving model ...\n",
            "train_loss:\n",
            "1.012800926035577\n",
            "Epoch: 520 \tTraining Loss: 1.012801 \tValidation Loss: 0.019319\n",
            "Validation loss decreased (0.019322 --> 0.019319).  Saving model ...\n",
            "train_loss:\n",
            "1.0126636224788623\n",
            "Epoch: 521 \tTraining Loss: 1.012664 \tValidation Loss: 0.019315\n",
            "Validation loss decreased (0.019319 --> 0.019315).  Saving model ...\n",
            "train_loss:\n",
            "1.012526429452739\n",
            "Epoch: 522 \tTraining Loss: 1.012526 \tValidation Loss: 0.019312\n",
            "Validation loss decreased (0.019315 --> 0.019312).  Saving model ...\n",
            "train_loss:\n",
            "1.0123893484309479\n",
            "Epoch: 523 \tTraining Loss: 1.012389 \tValidation Loss: 0.019309\n",
            "Validation loss decreased (0.019312 --> 0.019309).  Saving model ...\n",
            "train_loss:\n",
            "1.012252355669881\n",
            "Epoch: 524 \tTraining Loss: 1.012252 \tValidation Loss: 0.019305\n",
            "Validation loss decreased (0.019309 --> 0.019305).  Saving model ...\n",
            "train_loss:\n",
            "1.0121154806443624\n",
            "Epoch: 525 \tTraining Loss: 1.012115 \tValidation Loss: 0.019302\n",
            "Validation loss decreased (0.019305 --> 0.019302).  Saving model ...\n",
            "train_loss:\n",
            "1.0119787132019524\n",
            "Epoch: 526 \tTraining Loss: 1.011979 \tValidation Loss: 0.019299\n",
            "Validation loss decreased (0.019302 --> 0.019299).  Saving model ...\n",
            "train_loss:\n",
            "1.0118420652963303\n",
            "Epoch: 527 \tTraining Loss: 1.011842 \tValidation Loss: 0.019296\n",
            "Validation loss decreased (0.019299 --> 0.019296).  Saving model ...\n",
            "train_loss:\n",
            "1.011705531687527\n",
            "Epoch: 528 \tTraining Loss: 1.011706 \tValidation Loss: 0.019293\n",
            "Validation loss decreased (0.019296 --> 0.019293).  Saving model ...\n",
            "train_loss:\n",
            "1.0115691125392914\n",
            "Epoch: 529 \tTraining Loss: 1.011569 \tValidation Loss: 0.019290\n",
            "Validation loss decreased (0.019293 --> 0.019290).  Saving model ...\n",
            "train_loss:\n",
            "1.0114328199690514\n",
            "Epoch: 530 \tTraining Loss: 1.011433 \tValidation Loss: 0.019287\n",
            "Validation loss decreased (0.019290 --> 0.019287).  Saving model ...\n",
            "train_loss:\n",
            "1.0112966385843989\n",
            "Epoch: 531 \tTraining Loss: 1.011297 \tValidation Loss: 0.019284\n",
            "Validation loss decreased (0.019287 --> 0.019284).  Saving model ...\n",
            "train_loss:\n",
            "1.0111605795202674\n",
            "Epoch: 532 \tTraining Loss: 1.011161 \tValidation Loss: 0.019281\n",
            "Validation loss decreased (0.019284 --> 0.019281).  Saving model ...\n",
            "train_loss:\n",
            "1.011024636554194\n",
            "Epoch: 533 \tTraining Loss: 1.011025 \tValidation Loss: 0.019279\n",
            "Validation loss decreased (0.019281 --> 0.019279).  Saving model ...\n",
            "train_loss:\n",
            "1.0108888303185557\n",
            "Epoch: 534 \tTraining Loss: 1.010889 \tValidation Loss: 0.019276\n",
            "Validation loss decreased (0.019279 --> 0.019276).  Saving model ...\n",
            "train_loss:\n",
            "1.0107531482046777\n",
            "Epoch: 535 \tTraining Loss: 1.010753 \tValidation Loss: 0.019273\n",
            "Validation loss decreased (0.019276 --> 0.019273).  Saving model ...\n",
            "train_loss:\n",
            "1.0106175980725132\n",
            "Epoch: 536 \tTraining Loss: 1.010618 \tValidation Loss: 0.019271\n",
            "Validation loss decreased (0.019273 --> 0.019271).  Saving model ...\n",
            "train_loss:\n",
            "1.0104821746820931\n",
            "Epoch: 537 \tTraining Loss: 1.010482 \tValidation Loss: 0.019268\n",
            "Validation loss decreased (0.019271 --> 0.019268).  Saving model ...\n",
            "train_loss:\n",
            "1.0103468659159902\n",
            "Epoch: 538 \tTraining Loss: 1.010347 \tValidation Loss: 0.019266\n",
            "Validation loss decreased (0.019268 --> 0.019266).  Saving model ...\n",
            "train_loss:\n",
            "1.0102117236826447\n",
            "Epoch: 539 \tTraining Loss: 1.010212 \tValidation Loss: 0.019263\n",
            "Validation loss decreased (0.019266 --> 0.019263).  Saving model ...\n",
            "train_loss:\n",
            "1.0100767000035926\n",
            "Epoch: 540 \tTraining Loss: 1.010077 \tValidation Loss: 0.019261\n",
            "Validation loss decreased (0.019263 --> 0.019261).  Saving model ...\n",
            "train_loss:\n",
            "1.0099418335236037\n",
            "Epoch: 541 \tTraining Loss: 1.009942 \tValidation Loss: 0.019259\n",
            "Validation loss decreased (0.019261 --> 0.019259).  Saving model ...\n",
            "train_loss:\n",
            "1.0098070957503475\n",
            "Epoch: 542 \tTraining Loss: 1.009807 \tValidation Loss: 0.019256\n",
            "Validation loss decreased (0.019259 --> 0.019256).  Saving model ...\n",
            "train_loss:\n",
            "1.0096725058424605\n",
            "Epoch: 543 \tTraining Loss: 1.009673 \tValidation Loss: 0.019254\n",
            "Validation loss decreased (0.019256 --> 0.019254).  Saving model ...\n",
            "train_loss:\n",
            "1.0095380646186871\n",
            "Epoch: 544 \tTraining Loss: 1.009538 \tValidation Loss: 0.019252\n",
            "Validation loss decreased (0.019254 --> 0.019252).  Saving model ...\n",
            "train_loss:\n",
            "1.0094037771552473\n",
            "Epoch: 545 \tTraining Loss: 1.009404 \tValidation Loss: 0.019250\n",
            "Validation loss decreased (0.019252 --> 0.019250).  Saving model ...\n",
            "train_loss:\n",
            "1.0092696238022585\n",
            "Epoch: 546 \tTraining Loss: 1.009270 \tValidation Loss: 0.019248\n",
            "Validation loss decreased (0.019250 --> 0.019248).  Saving model ...\n",
            "train_loss:\n",
            "1.0091356323970544\n",
            "Epoch: 547 \tTraining Loss: 1.009136 \tValidation Loss: 0.019246\n",
            "Validation loss decreased (0.019248 --> 0.019246).  Saving model ...\n",
            "train_loss:\n",
            "1.009001789020968\n",
            "Epoch: 548 \tTraining Loss: 1.009002 \tValidation Loss: 0.019244\n",
            "Validation loss decreased (0.019246 --> 0.019244).  Saving model ...\n",
            "train_loss:\n",
            "1.0088681126688863\n",
            "Epoch: 549 \tTraining Loss: 1.008868 \tValidation Loss: 0.019242\n",
            "Validation loss decreased (0.019244 --> 0.019242).  Saving model ...\n",
            "train_loss:\n",
            "1.008734594989609\n",
            "Epoch: 550 \tTraining Loss: 1.008735 \tValidation Loss: 0.019240\n",
            "Validation loss decreased (0.019242 --> 0.019240).  Saving model ...\n",
            "train_loss:\n",
            "1.0086012250119514\n",
            "Epoch: 551 \tTraining Loss: 1.008601 \tValidation Loss: 0.019238\n",
            "Validation loss decreased (0.019240 --> 0.019238).  Saving model ...\n",
            "train_loss:\n",
            "1.0084680286082592\n",
            "Epoch: 552 \tTraining Loss: 1.008468 \tValidation Loss: 0.019236\n",
            "Validation loss decreased (0.019238 --> 0.019236).  Saving model ...\n",
            "train_loss:\n",
            "1.0083350069247758\n",
            "Epoch: 553 \tTraining Loss: 1.008335 \tValidation Loss: 0.019234\n",
            "Validation loss decreased (0.019236 --> 0.019234).  Saving model ...\n",
            "train_loss:\n",
            "1.0082021421128577\n",
            "Epoch: 554 \tTraining Loss: 1.008202 \tValidation Loss: 0.019233\n",
            "Validation loss decreased (0.019234 --> 0.019233).  Saving model ...\n",
            "train_loss:\n",
            "1.0080694559511247\n",
            "Epoch: 555 \tTraining Loss: 1.008069 \tValidation Loss: 0.019231\n",
            "Validation loss decreased (0.019233 --> 0.019231).  Saving model ...\n",
            "train_loss:\n",
            "1.0079369312459296\n",
            "Epoch: 556 \tTraining Loss: 1.007937 \tValidation Loss: 0.019229\n",
            "Validation loss decreased (0.019231 --> 0.019229).  Saving model ...\n",
            "train_loss:\n",
            "1.0078045914133826\n",
            "Epoch: 557 \tTraining Loss: 1.007805 \tValidation Loss: 0.019228\n",
            "Validation loss decreased (0.019229 --> 0.019228).  Saving model ...\n",
            "train_loss:\n",
            "1.007672437272229\n",
            "Epoch: 558 \tTraining Loss: 1.007672 \tValidation Loss: 0.019226\n",
            "Validation loss decreased (0.019228 --> 0.019226).  Saving model ...\n",
            "train_loss:\n",
            "1.007540449663833\n",
            "Epoch: 559 \tTraining Loss: 1.007540 \tValidation Loss: 0.019225\n",
            "Validation loss decreased (0.019226 --> 0.019225).  Saving model ...\n",
            "train_loss:\n",
            "1.0074086567530265\n",
            "Epoch: 560 \tTraining Loss: 1.007409 \tValidation Loss: 0.019224\n",
            "Validation loss decreased (0.019225 --> 0.019224).  Saving model ...\n",
            "train_loss:\n",
            "1.0072770439661467\n",
            "Epoch: 561 \tTraining Loss: 1.007277 \tValidation Loss: 0.019222\n",
            "Validation loss decreased (0.019224 --> 0.019222).  Saving model ...\n",
            "train_loss:\n",
            "1.0071456147419227\n",
            "Epoch: 562 \tTraining Loss: 1.007146 \tValidation Loss: 0.019221\n",
            "Validation loss decreased (0.019222 --> 0.019221).  Saving model ...\n",
            "train_loss:\n",
            "1.0070143648228802\n",
            "Epoch: 563 \tTraining Loss: 1.007014 \tValidation Loss: 0.019220\n",
            "Validation loss decreased (0.019221 --> 0.019220).  Saving model ...\n",
            "train_loss:\n",
            "1.006883340549993\n",
            "Epoch: 564 \tTraining Loss: 1.006883 \tValidation Loss: 0.019218\n",
            "Validation loss decreased (0.019220 --> 0.019218).  Saving model ...\n",
            "train_loss:\n",
            "1.0067524800261298\n",
            "Epoch: 565 \tTraining Loss: 1.006752 \tValidation Loss: 0.019217\n",
            "Validation loss decreased (0.019218 --> 0.019217).  Saving model ...\n",
            "train_loss:\n",
            "1.0066218214048135\n",
            "Epoch: 566 \tTraining Loss: 1.006622 \tValidation Loss: 0.019216\n",
            "Validation loss decreased (0.019217 --> 0.019216).  Saving model ...\n",
            "train_loss:\n",
            "1.006491367142279\n",
            "Epoch: 567 \tTraining Loss: 1.006491 \tValidation Loss: 0.019215\n",
            "Validation loss decreased (0.019216 --> 0.019215).  Saving model ...\n",
            "train_loss:\n",
            "1.0063610926761732\n",
            "Epoch: 568 \tTraining Loss: 1.006361 \tValidation Loss: 0.019214\n",
            "Validation loss decreased (0.019215 --> 0.019214).  Saving model ...\n",
            "train_loss:\n",
            "1.0062310291188103\n",
            "Epoch: 569 \tTraining Loss: 1.006231 \tValidation Loss: 0.019213\n",
            "Validation loss decreased (0.019214 --> 0.019213).  Saving model ...\n",
            "train_loss:\n",
            "1.0061011957925754\n",
            "Epoch: 570 \tTraining Loss: 1.006101 \tValidation Loss: 0.019212\n",
            "Validation loss decreased (0.019213 --> 0.019212).  Saving model ...\n",
            "train_loss:\n",
            "1.0059715430815142\n",
            "Epoch: 571 \tTraining Loss: 1.005972 \tValidation Loss: 0.019211\n",
            "Validation loss decreased (0.019212 --> 0.019211).  Saving model ...\n",
            "train_loss:\n",
            "1.005842110121643\n",
            "Epoch: 572 \tTraining Loss: 1.005842 \tValidation Loss: 0.019210\n",
            "Validation loss decreased (0.019211 --> 0.019210).  Saving model ...\n",
            "train_loss:\n",
            "1.0057129013341861\n",
            "Epoch: 573 \tTraining Loss: 1.005713 \tValidation Loss: 0.019209\n",
            "Validation loss decreased (0.019210 --> 0.019209).  Saving model ...\n",
            "train_loss:\n",
            "1.0055838944492759\n",
            "Epoch: 574 \tTraining Loss: 1.005584 \tValidation Loss: 0.019208\n",
            "Validation loss decreased (0.019209 --> 0.019208).  Saving model ...\n",
            "train_loss:\n",
            "1.0054551020755873\n",
            "Epoch: 575 \tTraining Loss: 1.005455 \tValidation Loss: 0.019208\n",
            "Validation loss decreased (0.019208 --> 0.019208).  Saving model ...\n",
            "train_loss:\n",
            "1.0053265307630812\n",
            "Epoch: 576 \tTraining Loss: 1.005327 \tValidation Loss: 0.019207\n",
            "Validation loss decreased (0.019208 --> 0.019207).  Saving model ...\n",
            "train_loss:\n",
            "1.0051981847692322\n",
            "Epoch: 577 \tTraining Loss: 1.005198 \tValidation Loss: 0.019206\n",
            "Validation loss decreased (0.019207 --> 0.019206).  Saving model ...\n",
            "train_loss:\n",
            "1.0050700734277347\n",
            "Epoch: 578 \tTraining Loss: 1.005070 \tValidation Loss: 0.019206\n",
            "Validation loss decreased (0.019206 --> 0.019206).  Saving model ...\n",
            "train_loss:\n",
            "1.0049421540000936\n",
            "Epoch: 579 \tTraining Loss: 1.004942 \tValidation Loss: 0.019205\n",
            "Validation loss decreased (0.019206 --> 0.019205).  Saving model ...\n",
            "train_loss:\n",
            "1.0048144946059028\n",
            "Epoch: 580 \tTraining Loss: 1.004814 \tValidation Loss: 0.019204\n",
            "Validation loss decreased (0.019205 --> 0.019204).  Saving model ...\n",
            "train_loss:\n",
            "1.004687052506667\n",
            "Epoch: 581 \tTraining Loss: 1.004687 \tValidation Loss: 0.019204\n",
            "Validation loss decreased (0.019204 --> 0.019204).  Saving model ...\n",
            "train_loss:\n",
            "1.0045598306498684\n",
            "Epoch: 582 \tTraining Loss: 1.004560 \tValidation Loss: 0.019203\n",
            "Validation loss decreased (0.019204 --> 0.019203).  Saving model ...\n",
            "train_loss:\n",
            "1.004432852942865\n",
            "Epoch: 583 \tTraining Loss: 1.004433 \tValidation Loss: 0.019203\n",
            "Validation loss decreased (0.019203 --> 0.019203).  Saving model ...\n",
            "train_loss:\n",
            "1.0043061174206682\n",
            "Epoch: 584 \tTraining Loss: 1.004306 \tValidation Loss: 0.019202\n",
            "Validation loss decreased (0.019203 --> 0.019202).  Saving model ...\n",
            "train_loss:\n",
            "1.0041796005034185\n",
            "Epoch: 585 \tTraining Loss: 1.004180 \tValidation Loss: 0.019202\n",
            "Validation loss decreased (0.019202 --> 0.019202).  Saving model ...\n",
            "train_loss:\n",
            "1.0040533459121055\n",
            "Epoch: 586 \tTraining Loss: 1.004053 \tValidation Loss: 0.019202\n",
            "Validation loss decreased (0.019202 --> 0.019202).  Saving model ...\n",
            "train_loss:\n",
            "1.003927305668265\n",
            "Epoch: 587 \tTraining Loss: 1.003927 \tValidation Loss: 0.019201\n",
            "Validation loss decreased (0.019202 --> 0.019201).  Saving model ...\n",
            "train_loss:\n",
            "1.0038015370840554\n",
            "Epoch: 588 \tTraining Loss: 1.003802 \tValidation Loss: 0.019201\n",
            "Validation loss decreased (0.019201 --> 0.019201).  Saving model ...\n",
            "train_loss:\n",
            "1.0036760067546762\n",
            "Epoch: 589 \tTraining Loss: 1.003676 \tValidation Loss: 0.019201\n",
            "Validation loss decreased (0.019201 --> 0.019201).  Saving model ...\n",
            "train_loss:\n",
            "1.003550730072535\n",
            "Epoch: 590 \tTraining Loss: 1.003551 \tValidation Loss: 0.019201\n",
            "Validation loss decreased (0.019201 --> 0.019201).  Saving model ...\n",
            "train_loss:\n",
            "1.0034257078563773\n",
            "Epoch: 591 \tTraining Loss: 1.003426 \tValidation Loss: 0.019200\n",
            "Validation loss decreased (0.019201 --> 0.019200).  Saving model ...\n",
            "train_loss:\n",
            "1.0033009116138731\n",
            "Epoch: 592 \tTraining Loss: 1.003301 \tValidation Loss: 0.019200\n",
            "Validation loss decreased (0.019200 --> 0.019200).  Saving model ...\n",
            "train_loss:\n",
            "1.0031763708198465\n",
            "Epoch: 593 \tTraining Loss: 1.003176 \tValidation Loss: 0.019200\n",
            "Validation loss decreased (0.019200 --> 0.019200).  Saving model ...\n",
            "train_loss:\n",
            "1.0030521093816547\n",
            "Epoch: 594 \tTraining Loss: 1.003052 \tValidation Loss: 0.019200\n",
            "Validation loss decreased (0.019200 --> 0.019200).  Saving model ...\n",
            "train_loss:\n",
            "1.002928078174591\n",
            "Epoch: 595 \tTraining Loss: 1.002928 \tValidation Loss: 0.019200\n",
            "Validation loss decreased (0.019200 --> 0.019200).  Saving model ...\n",
            "train_loss:\n",
            "1.0028042993047734\n",
            "Epoch: 596 \tTraining Loss: 1.002804 \tValidation Loss: 0.019200\n",
            "Validation loss decreased (0.019200 --> 0.019200).  Saving model ...\n",
            "train_loss:\n",
            "1.0026807948783203\n",
            "Epoch: 597 \tTraining Loss: 1.002681 \tValidation Loss: 0.019200\n",
            "Validation loss decreased (0.019200 --> 0.019200).  Saving model ...\n",
            "train_loss:\n",
            "1.002557532964172\n",
            "Epoch: 598 \tTraining Loss: 1.002558 \tValidation Loss: 0.019200\n",
            "train_loss:\n",
            "1.0024345373059367\n",
            "Epoch: 599 \tTraining Loss: 1.002435 \tValidation Loss: 0.019200\n",
            "train_loss:\n",
            "1.0023118134710816\n",
            "Epoch: 600 \tTraining Loss: 1.002312 \tValidation Loss: 0.019200\n",
            "train_loss:\n",
            "1.0021893332947742\n",
            "Epoch: 601 \tTraining Loss: 1.002189 \tValidation Loss: 0.019200\n",
            "train_loss:\n",
            "1.0020671352580353\n",
            "Epoch: 602 \tTraining Loss: 1.002067 \tValidation Loss: 0.019200\n",
            "train_loss:\n",
            "1.0019452000384803\n",
            "Epoch: 603 \tTraining Loss: 1.001945 \tValidation Loss: 0.019200\n",
            "train_loss:\n",
            "1.0018235235423831\n",
            "Epoch: 604 \tTraining Loss: 1.001824 \tValidation Loss: 0.019200\n",
            "train_loss:\n",
            "1.0017021206709056\n",
            "Epoch: 605 \tTraining Loss: 1.001702 \tValidation Loss: 0.019201\n",
            "train_loss:\n",
            "1.0015809881490665\n",
            "Epoch: 606 \tTraining Loss: 1.001581 \tValidation Loss: 0.019201\n",
            "train_loss:\n",
            "1.001460115496929\n",
            "Epoch: 607 \tTraining Loss: 1.001460 \tValidation Loss: 0.019201\n",
            "train_loss:\n",
            "1.0013395109019436\n",
            "Epoch: 608 \tTraining Loss: 1.001340 \tValidation Loss: 0.019201\n",
            "train_loss:\n",
            "1.0012191869727858\n",
            "Epoch: 609 \tTraining Loss: 1.001219 \tValidation Loss: 0.019201\n",
            "train_loss:\n",
            "1.0010991315920275\n",
            "Epoch: 610 \tTraining Loss: 1.001099 \tValidation Loss: 0.019202\n",
            "train_loss:\n",
            "1.000979328057268\n",
            "Epoch: 611 \tTraining Loss: 1.000979 \tValidation Loss: 0.019202\n",
            "train_loss:\n",
            "1.000859831551929\n",
            "Epoch: 612 \tTraining Loss: 1.000860 \tValidation Loss: 0.019202\n",
            "train_loss:\n",
            "1.0007405839451067\n",
            "Epoch: 613 \tTraining Loss: 1.000741 \tValidation Loss: 0.019203\n",
            "train_loss:\n",
            "1.0006216350165043\n",
            "Epoch: 614 \tTraining Loss: 1.000622 \tValidation Loss: 0.019203\n",
            "train_loss:\n",
            "1.0005029372789047\n",
            "Epoch: 615 \tTraining Loss: 1.000503 \tValidation Loss: 0.019203\n",
            "train_loss:\n",
            "1.0003845213533757\n",
            "Epoch: 616 \tTraining Loss: 1.000385 \tValidation Loss: 0.019204\n",
            "train_loss:\n",
            "1.0002663772512268\n",
            "Epoch: 617 \tTraining Loss: 1.000266 \tValidation Loss: 0.019204\n",
            "train_loss:\n",
            "1.0001485244585917\n",
            "Epoch: 618 \tTraining Loss: 1.000149 \tValidation Loss: 0.019205\n",
            "train_loss:\n",
            "1.0000309303894148\n",
            "Epoch: 619 \tTraining Loss: 1.000031 \tValidation Loss: 0.019205\n",
            "train_loss:\n",
            "0.9999136282847478\n",
            "Epoch: 620 \tTraining Loss: 0.999914 \tValidation Loss: 0.019205\n",
            "train_loss:\n",
            "0.9997965847397898\n",
            "Epoch: 621 \tTraining Loss: 0.999797 \tValidation Loss: 0.019206\n",
            "train_loss:\n",
            "0.9996798117082197\n",
            "Epoch: 622 \tTraining Loss: 0.999680 \tValidation Loss: 0.019206\n",
            "train_loss:\n",
            "0.9995633183599828\n",
            "Epoch: 623 \tTraining Loss: 0.999563 \tValidation Loss: 0.019207\n",
            "train_loss:\n",
            "0.999447124508711\n",
            "Epoch: 624 \tTraining Loss: 0.999447 \tValidation Loss: 0.019207\n",
            "train_loss:\n",
            "0.9993311721872498\n",
            "Epoch: 625 \tTraining Loss: 0.999331 \tValidation Loss: 0.019208\n",
            "train_loss:\n",
            "0.9992155173977653\n",
            "Epoch: 626 \tTraining Loss: 0.999216 \tValidation Loss: 0.019208\n",
            "train_loss:\n",
            "0.9991001324666725\n",
            "Epoch: 627 \tTraining Loss: 0.999100 \tValidation Loss: 0.019209\n",
            "train_loss:\n",
            "0.998985007077783\n",
            "Epoch: 628 \tTraining Loss: 0.998985 \tValidation Loss: 0.019209\n",
            "train_loss:\n",
            "0.9988701446698263\n",
            "Epoch: 629 \tTraining Loss: 0.998870 \tValidation Loss: 0.019209\n",
            "train_loss:\n",
            "0.9987555906012818\n",
            "Epoch: 630 \tTraining Loss: 0.998756 \tValidation Loss: 0.019210\n",
            "train_loss:\n",
            "0.9986412888699836\n",
            "Epoch: 631 \tTraining Loss: 0.998641 \tValidation Loss: 0.019210\n",
            "train_loss:\n",
            "0.9985272686232577\n",
            "Epoch: 632 \tTraining Loss: 0.998527 \tValidation Loss: 0.019211\n",
            "train_loss:\n",
            "0.9984135262586258\n",
            "Epoch: 633 \tTraining Loss: 0.998414 \tValidation Loss: 0.019211\n",
            "train_loss:\n",
            "0.9983000404887147\n",
            "Epoch: 634 \tTraining Loss: 0.998300 \tValidation Loss: 0.019212\n",
            "train_loss:\n",
            "0.9981868463558156\n",
            "Epoch: 635 \tTraining Loss: 0.998187 \tValidation Loss: 0.019212\n",
            "train_loss:\n",
            "0.9980739210988139\n",
            "Epoch: 636 \tTraining Loss: 0.998074 \tValidation Loss: 0.019213\n",
            "train_loss:\n",
            "0.9979612426115916\n",
            "Epoch: 637 \tTraining Loss: 0.997961 \tValidation Loss: 0.019213\n",
            "train_loss:\n",
            "0.9978488235028236\n",
            "Epoch: 638 \tTraining Loss: 0.997849 \tValidation Loss: 0.019214\n",
            "train_loss:\n",
            "0.997736699797295\n",
            "Epoch: 639 \tTraining Loss: 0.997737 \tValidation Loss: 0.019214\n",
            "train_loss:\n",
            "0.9976248344877264\n",
            "Epoch: 640 \tTraining Loss: 0.997625 \tValidation Loss: 0.019215\n",
            "train_loss:\n",
            "0.9975132305216003\n",
            "Epoch: 641 \tTraining Loss: 0.997513 \tValidation Loss: 0.019215\n",
            "train_loss:\n",
            "0.9974019160637488\n",
            "Epoch: 642 \tTraining Loss: 0.997402 \tValidation Loss: 0.019216\n",
            "train_loss:\n",
            "0.9972908673705635\n",
            "Epoch: 643 \tTraining Loss: 0.997291 \tValidation Loss: 0.019216\n",
            "train_loss:\n",
            "0.9971800665934007\n",
            "Epoch: 644 \tTraining Loss: 0.997180 \tValidation Loss: 0.019217\n",
            "train_loss:\n",
            "0.9970695343646374\n",
            "Epoch: 645 \tTraining Loss: 0.997070 \tValidation Loss: 0.019217\n",
            "train_loss:\n",
            "0.9969592901704075\n",
            "Epoch: 646 \tTraining Loss: 0.996959 \tValidation Loss: 0.019218\n",
            "train_loss:\n",
            "0.996849272768576\n",
            "Epoch: 647 \tTraining Loss: 0.996849 \tValidation Loss: 0.019218\n",
            "train_loss:\n",
            "0.9967395376700622\n",
            "Epoch: 648 \tTraining Loss: 0.996740 \tValidation Loss: 0.019218\n",
            "train_loss:\n",
            "0.9966300504875707\n",
            "Epoch: 649 \tTraining Loss: 0.996630 \tValidation Loss: 0.019219\n",
            "train_loss:\n",
            "0.9965208207185452\n",
            "Epoch: 650 \tTraining Loss: 0.996521 \tValidation Loss: 0.019219\n",
            "train_loss:\n",
            "0.9964118634278958\n",
            "Epoch: 651 \tTraining Loss: 0.996412 \tValidation Loss: 0.019220\n",
            "train_loss:\n",
            "0.996303161585724\n",
            "Epoch: 652 \tTraining Loss: 0.996303 \tValidation Loss: 0.019220\n",
            "train_loss:\n",
            "0.9961947120807984\n",
            "Epoch: 653 \tTraining Loss: 0.996195 \tValidation Loss: 0.019221\n",
            "train_loss:\n",
            "0.9960865178606012\n",
            "Epoch: 654 \tTraining Loss: 0.996087 \tValidation Loss: 0.019221\n",
            "train_loss:\n",
            "0.9959785859663408\n",
            "Epoch: 655 \tTraining Loss: 0.995979 \tValidation Loss: 0.019221\n",
            "train_loss:\n",
            "0.9958709147605267\n",
            "Epoch: 656 \tTraining Loss: 0.995871 \tValidation Loss: 0.019222\n",
            "train_loss:\n",
            "0.9957634626509069\n",
            "Epoch: 657 \tTraining Loss: 0.995763 \tValidation Loss: 0.019222\n",
            "train_loss:\n",
            "0.995656267791004\n",
            "Epoch: 658 \tTraining Loss: 0.995656 \tValidation Loss: 0.019222\n",
            "train_loss:\n",
            "0.9955493557256657\n",
            "Epoch: 659 \tTraining Loss: 0.995549 \tValidation Loss: 0.019223\n",
            "train_loss:\n",
            "0.9954426853538869\n",
            "Epoch: 660 \tTraining Loss: 0.995443 \tValidation Loss: 0.019223\n",
            "train_loss:\n",
            "0.9953362743605624\n",
            "Epoch: 661 \tTraining Loss: 0.995336 \tValidation Loss: 0.019223\n",
            "train_loss:\n",
            "0.9952300822996831\n",
            "Epoch: 662 \tTraining Loss: 0.995230 \tValidation Loss: 0.019223\n",
            "train_loss:\n",
            "0.9951241582959562\n",
            "Epoch: 663 \tTraining Loss: 0.995124 \tValidation Loss: 0.019224\n",
            "train_loss:\n",
            "0.9950184609208789\n",
            "Epoch: 664 \tTraining Loss: 0.995018 \tValidation Loss: 0.019224\n",
            "train_loss:\n",
            "0.9949130365154245\n",
            "Epoch: 665 \tTraining Loss: 0.994913 \tValidation Loss: 0.019224\n",
            "train_loss:\n",
            "0.994807848072314\n",
            "Epoch: 666 \tTraining Loss: 0.994808 \tValidation Loss: 0.019224\n",
            "train_loss:\n",
            "0.9947028906790765\n",
            "Epoch: 667 \tTraining Loss: 0.994703 \tValidation Loss: 0.019224\n",
            "train_loss:\n",
            "0.9945981974130148\n",
            "Epoch: 668 \tTraining Loss: 0.994598 \tValidation Loss: 0.019225\n",
            "train_loss:\n",
            "0.9944937302843555\n",
            "Epoch: 669 \tTraining Loss: 0.994494 \tValidation Loss: 0.019225\n",
            "train_loss:\n",
            "0.9943895037030126\n",
            "Epoch: 670 \tTraining Loss: 0.994390 \tValidation Loss: 0.019225\n",
            "train_loss:\n",
            "0.9942855178327351\n",
            "Epoch: 671 \tTraining Loss: 0.994286 \tValidation Loss: 0.019225\n",
            "train_loss:\n",
            "0.9941817667785582\n",
            "Epoch: 672 \tTraining Loss: 0.994182 \tValidation Loss: 0.019225\n",
            "train_loss:\n",
            "0.9940782731378471\n",
            "Epoch: 673 \tTraining Loss: 0.994078 \tValidation Loss: 0.019225\n",
            "train_loss:\n",
            "0.9939749948271028\n",
            "Epoch: 674 \tTraining Loss: 0.993975 \tValidation Loss: 0.019225\n",
            "train_loss:\n",
            "0.9938719785147971\n",
            "Epoch: 675 \tTraining Loss: 0.993872 \tValidation Loss: 0.019225\n",
            "train_loss:\n",
            "0.9937691898136348\n",
            "Epoch: 676 \tTraining Loss: 0.993769 \tValidation Loss: 0.019225\n",
            "train_loss:\n",
            "0.9936666315073496\n",
            "Epoch: 677 \tTraining Loss: 0.993667 \tValidation Loss: 0.019225\n",
            "train_loss:\n",
            "0.9935642898410231\n",
            "Epoch: 678 \tTraining Loss: 0.993564 \tValidation Loss: 0.019225\n",
            "train_loss:\n",
            "0.9934621903595033\n",
            "Epoch: 679 \tTraining Loss: 0.993462 \tValidation Loss: 0.019225\n",
            "train_loss:\n",
            "0.9933603217641076\n",
            "Epoch: 680 \tTraining Loss: 0.993360 \tValidation Loss: 0.019225\n",
            "train_loss:\n",
            "0.9932586878210634\n",
            "Epoch: 681 \tTraining Loss: 0.993259 \tValidation Loss: 0.019225\n",
            "train_loss:\n",
            "0.9931573076890066\n",
            "Epoch: 682 \tTraining Loss: 0.993157 \tValidation Loss: 0.019225\n",
            "train_loss:\n",
            "0.9930561443606576\n",
            "Epoch: 683 \tTraining Loss: 0.993056 \tValidation Loss: 0.019224\n",
            "train_loss:\n",
            "0.9929551853910907\n",
            "Epoch: 684 \tTraining Loss: 0.992955 \tValidation Loss: 0.019224\n",
            "train_loss:\n",
            "0.9928544638576088\n",
            "Epoch: 685 \tTraining Loss: 0.992854 \tValidation Loss: 0.019224\n",
            "train_loss:\n",
            "0.9927539779589727\n",
            "Epoch: 686 \tTraining Loss: 0.992754 \tValidation Loss: 0.019224\n",
            "train_loss:\n",
            "0.992653713121519\n",
            "Epoch: 687 \tTraining Loss: 0.992654 \tValidation Loss: 0.019224\n",
            "train_loss:\n",
            "0.9925536652515222\n",
            "Epoch: 688 \tTraining Loss: 0.992554 \tValidation Loss: 0.019223\n",
            "train_loss:\n",
            "0.9924538598938303\n",
            "Epoch: 689 \tTraining Loss: 0.992454 \tValidation Loss: 0.019223\n",
            "train_loss:\n",
            "0.9923542569299321\n",
            "Epoch: 690 \tTraining Loss: 0.992354 \tValidation Loss: 0.019223\n",
            "train_loss:\n",
            "0.9922548552135845\n",
            "Epoch: 691 \tTraining Loss: 0.992255 \tValidation Loss: 0.019222\n",
            "train_loss:\n",
            "0.9921556999395181\n",
            "Epoch: 692 \tTraining Loss: 0.992156 \tValidation Loss: 0.019222\n",
            "train_loss:\n",
            "0.9920567783353093\n",
            "Epoch: 693 \tTraining Loss: 0.992057 \tValidation Loss: 0.019222\n",
            "train_loss:\n",
            "0.9919580424224937\n",
            "Epoch: 694 \tTraining Loss: 0.991958 \tValidation Loss: 0.019221\n",
            "train_loss:\n",
            "0.9918595550806968\n",
            "Epoch: 695 \tTraining Loss: 0.991860 \tValidation Loss: 0.019221\n",
            "train_loss:\n",
            "0.9917612653839719\n",
            "Epoch: 696 \tTraining Loss: 0.991761 \tValidation Loss: 0.019220\n",
            "train_loss:\n",
            "0.9916632050996298\n",
            "Epoch: 697 \tTraining Loss: 0.991663 \tValidation Loss: 0.019220\n",
            "train_loss:\n",
            "0.9915653727539293\n",
            "Epoch: 698 \tTraining Loss: 0.991565 \tValidation Loss: 0.019219\n",
            "train_loss:\n",
            "0.991467718567167\n",
            "Epoch: 699 \tTraining Loss: 0.991468 \tValidation Loss: 0.019219\n",
            "train_loss:\n",
            "0.9913703059102152\n",
            "Epoch: 700 \tTraining Loss: 0.991370 \tValidation Loss: 0.019218\n",
            "train_loss:\n",
            "0.9912731141506971\n",
            "Epoch: 701 \tTraining Loss: 0.991273 \tValidation Loss: 0.019217\n",
            "train_loss:\n",
            "0.9911761049713407\n",
            "Epoch: 702 \tTraining Loss: 0.991176 \tValidation Loss: 0.019217\n",
            "train_loss:\n",
            "0.9910793410880225\n",
            "Epoch: 703 \tTraining Loss: 0.991079 \tValidation Loss: 0.019216\n",
            "train_loss:\n",
            "0.9909827620773525\n",
            "Epoch: 704 \tTraining Loss: 0.990983 \tValidation Loss: 0.019215\n",
            "train_loss:\n",
            "0.9908864124790653\n",
            "Epoch: 705 \tTraining Loss: 0.990886 \tValidation Loss: 0.019215\n",
            "train_loss:\n",
            "0.9907902674033091\n",
            "Epoch: 706 \tTraining Loss: 0.990790 \tValidation Loss: 0.019214\n",
            "train_loss:\n",
            "0.9906943108026798\n",
            "Epoch: 707 \tTraining Loss: 0.990694 \tValidation Loss: 0.019213\n",
            "train_loss:\n",
            "0.9905985863981667\n",
            "Epoch: 708 \tTraining Loss: 0.990599 \tValidation Loss: 0.019212\n",
            "train_loss:\n",
            "0.9905030350763719\n",
            "Epoch: 709 \tTraining Loss: 0.990503 \tValidation Loss: 0.019212\n",
            "train_loss:\n",
            "0.9904077090732344\n",
            "Epoch: 710 \tTraining Loss: 0.990408 \tValidation Loss: 0.019211\n",
            "train_loss:\n",
            "0.9903125961075773\n",
            "Epoch: 711 \tTraining Loss: 0.990313 \tValidation Loss: 0.019210\n",
            "train_loss:\n",
            "0.990217673582035\n",
            "Epoch: 712 \tTraining Loss: 0.990218 \tValidation Loss: 0.019209\n",
            "train_loss:\n",
            "0.9901229668777067\n",
            "Epoch: 713 \tTraining Loss: 0.990123 \tValidation Loss: 0.019208\n",
            "train_loss:\n",
            "0.9900284642046624\n",
            "Epoch: 714 \tTraining Loss: 0.990028 \tValidation Loss: 0.019207\n",
            "train_loss:\n",
            "0.9899341545917175\n",
            "Epoch: 715 \tTraining Loss: 0.989934 \tValidation Loss: 0.019206\n",
            "train_loss:\n",
            "0.9898400553962686\n",
            "Epoch: 716 \tTraining Loss: 0.989840 \tValidation Loss: 0.019205\n",
            "train_loss:\n",
            "0.989746148605923\n",
            "Epoch: 717 \tTraining Loss: 0.989746 \tValidation Loss: 0.019204\n",
            "train_loss:\n",
            "0.9896524515780774\n",
            "Epoch: 718 \tTraining Loss: 0.989652 \tValidation Loss: 0.019203\n",
            "train_loss:\n",
            "0.9895589512128097\n",
            "Epoch: 719 \tTraining Loss: 0.989559 \tValidation Loss: 0.019202\n",
            "train_loss:\n",
            "0.9894656512763474\n",
            "Epoch: 720 \tTraining Loss: 0.989466 \tValidation Loss: 0.019201\n",
            "train_loss:\n",
            "0.9893725416162512\n",
            "Epoch: 721 \tTraining Loss: 0.989373 \tValidation Loss: 0.019200\n",
            "Validation loss decreased (0.019200 --> 0.019200).  Saving model ...\n",
            "train_loss:\n",
            "0.989279629437478\n",
            "Epoch: 722 \tTraining Loss: 0.989280 \tValidation Loss: 0.019198\n",
            "Validation loss decreased (0.019200 --> 0.019198).  Saving model ...\n",
            "train_loss:\n",
            "0.989186921781236\n",
            "Epoch: 723 \tTraining Loss: 0.989187 \tValidation Loss: 0.019197\n",
            "Validation loss decreased (0.019198 --> 0.019197).  Saving model ...\n",
            "train_loss:\n",
            "0.9890943913014381\n",
            "Epoch: 724 \tTraining Loss: 0.989094 \tValidation Loss: 0.019196\n",
            "Validation loss decreased (0.019197 --> 0.019196).  Saving model ...\n",
            "train_loss:\n",
            "0.989002077461599\n",
            "Epoch: 725 \tTraining Loss: 0.989002 \tValidation Loss: 0.019195\n",
            "Validation loss decreased (0.019196 --> 0.019195).  Saving model ...\n",
            "train_loss:\n",
            "0.9889099484944082\n",
            "Epoch: 726 \tTraining Loss: 0.988910 \tValidation Loss: 0.019193\n",
            "Validation loss decreased (0.019195 --> 0.019193).  Saving model ...\n",
            "train_loss:\n",
            "0.9888180155347992\n",
            "Epoch: 727 \tTraining Loss: 0.988818 \tValidation Loss: 0.019192\n",
            "Validation loss decreased (0.019193 --> 0.019192).  Saving model ...\n",
            "train_loss:\n",
            "0.988726278091525\n",
            "Epoch: 728 \tTraining Loss: 0.988726 \tValidation Loss: 0.019191\n",
            "Validation loss decreased (0.019192 --> 0.019191).  Saving model ...\n",
            "train_loss:\n",
            "0.988634739275817\n",
            "Epoch: 729 \tTraining Loss: 0.988635 \tValidation Loss: 0.019189\n",
            "Validation loss decreased (0.019191 --> 0.019189).  Saving model ...\n",
            "train_loss:\n",
            "0.9885433756715649\n",
            "Epoch: 730 \tTraining Loss: 0.988543 \tValidation Loss: 0.019188\n",
            "Validation loss decreased (0.019189 --> 0.019188).  Saving model ...\n",
            "train_loss:\n",
            "0.9884522215023146\n",
            "Epoch: 731 \tTraining Loss: 0.988452 \tValidation Loss: 0.019186\n",
            "Validation loss decreased (0.019188 --> 0.019186).  Saving model ...\n",
            "train_loss:\n",
            "0.988361235503312\n",
            "Epoch: 732 \tTraining Loss: 0.988361 \tValidation Loss: 0.019185\n",
            "Validation loss decreased (0.019186 --> 0.019185).  Saving model ...\n",
            "train_loss:\n",
            "0.9882704386344323\n",
            "Epoch: 733 \tTraining Loss: 0.988270 \tValidation Loss: 0.019184\n",
            "Validation loss decreased (0.019185 --> 0.019184).  Saving model ...\n",
            "train_loss:\n",
            "0.9881798376093854\n",
            "Epoch: 734 \tTraining Loss: 0.988180 \tValidation Loss: 0.019182\n",
            "Validation loss decreased (0.019184 --> 0.019182).  Saving model ...\n",
            "train_loss:\n",
            "0.9880894324281714\n",
            "Epoch: 735 \tTraining Loss: 0.988089 \tValidation Loss: 0.019181\n",
            "Validation loss decreased (0.019182 --> 0.019181).  Saving model ...\n",
            "train_loss:\n",
            "0.9879992022946641\n",
            "Epoch: 736 \tTraining Loss: 0.987999 \tValidation Loss: 0.019179\n",
            "Validation loss decreased (0.019181 --> 0.019179).  Saving model ...\n",
            "train_loss:\n",
            "0.9879091745549506\n",
            "Epoch: 737 \tTraining Loss: 0.987909 \tValidation Loss: 0.019177\n",
            "Validation loss decreased (0.019179 --> 0.019177).  Saving model ...\n",
            "train_loss:\n",
            "0.987819315476732\n",
            "Epoch: 738 \tTraining Loss: 0.987819 \tValidation Loss: 0.019176\n",
            "Validation loss decreased (0.019177 --> 0.019176).  Saving model ...\n",
            "train_loss:\n",
            "0.9877296483123695\n",
            "Epoch: 739 \tTraining Loss: 0.987730 \tValidation Loss: 0.019174\n",
            "Validation loss decreased (0.019176 --> 0.019174).  Saving model ...\n",
            "train_loss:\n",
            "0.987640170114381\n",
            "Epoch: 740 \tTraining Loss: 0.987640 \tValidation Loss: 0.019172\n",
            "Validation loss decreased (0.019174 --> 0.019172).  Saving model ...\n",
            "train_loss:\n",
            "0.9875508787540289\n",
            "Epoch: 741 \tTraining Loss: 0.987551 \tValidation Loss: 0.019171\n",
            "Validation loss decreased (0.019172 --> 0.019171).  Saving model ...\n",
            "train_loss:\n",
            "0.9874617537626853\n",
            "Epoch: 742 \tTraining Loss: 0.987462 \tValidation Loss: 0.019169\n",
            "Validation loss decreased (0.019171 --> 0.019169).  Saving model ...\n",
            "train_loss:\n",
            "0.9873728175739666\n",
            "Epoch: 743 \tTraining Loss: 0.987373 \tValidation Loss: 0.019167\n",
            "Validation loss decreased (0.019169 --> 0.019167).  Saving model ...\n",
            "train_loss:\n",
            "0.9872840706791196\n",
            "Epoch: 744 \tTraining Loss: 0.987284 \tValidation Loss: 0.019166\n",
            "Validation loss decreased (0.019167 --> 0.019166).  Saving model ...\n",
            "train_loss:\n",
            "0.9871954844220654\n",
            "Epoch: 745 \tTraining Loss: 0.987195 \tValidation Loss: 0.019164\n",
            "Validation loss decreased (0.019166 --> 0.019164).  Saving model ...\n",
            "train_loss:\n",
            "0.9871071241386644\n",
            "Epoch: 746 \tTraining Loss: 0.987107 \tValidation Loss: 0.019162\n",
            "Validation loss decreased (0.019164 --> 0.019162).  Saving model ...\n",
            "train_loss:\n",
            "0.987018883064553\n",
            "Epoch: 747 \tTraining Loss: 0.987019 \tValidation Loss: 0.019160\n",
            "Validation loss decreased (0.019162 --> 0.019160).  Saving model ...\n",
            "train_loss:\n",
            "0.986930862232879\n",
            "Epoch: 748 \tTraining Loss: 0.986931 \tValidation Loss: 0.019158\n",
            "Validation loss decreased (0.019160 --> 0.019158).  Saving model ...\n",
            "train_loss:\n",
            "0.9868430148114214\n",
            "Epoch: 749 \tTraining Loss: 0.986843 \tValidation Loss: 0.019156\n",
            "Validation loss decreased (0.019158 --> 0.019156).  Saving model ...\n",
            "train_loss:\n",
            "0.9867553404726825\n",
            "Epoch: 750 \tTraining Loss: 0.986755 \tValidation Loss: 0.019154\n",
            "Validation loss decreased (0.019156 --> 0.019154).  Saving model ...\n",
            "train_loss:\n",
            "0.9866678405266541\n",
            "Epoch: 751 \tTraining Loss: 0.986668 \tValidation Loss: 0.019152\n",
            "Validation loss decreased (0.019154 --> 0.019152).  Saving model ...\n",
            "train_loss:\n",
            "0.9865805310207408\n",
            "Epoch: 752 \tTraining Loss: 0.986581 \tValidation Loss: 0.019151\n",
            "Validation loss decreased (0.019152 --> 0.019151).  Saving model ...\n",
            "train_loss:\n",
            "0.9864934013112561\n",
            "Epoch: 753 \tTraining Loss: 0.986493 \tValidation Loss: 0.019149\n",
            "Validation loss decreased (0.019151 --> 0.019149).  Saving model ...\n",
            "train_loss:\n",
            "0.9864064382982778\n",
            "Epoch: 754 \tTraining Loss: 0.986406 \tValidation Loss: 0.019147\n",
            "Validation loss decreased (0.019149 --> 0.019147).  Saving model ...\n",
            "train_loss:\n",
            "0.9863196369055863\n",
            "Epoch: 755 \tTraining Loss: 0.986320 \tValidation Loss: 0.019145\n",
            "Validation loss decreased (0.019147 --> 0.019145).  Saving model ...\n",
            "train_loss:\n",
            "0.9862330259530099\n",
            "Epoch: 756 \tTraining Loss: 0.986233 \tValidation Loss: 0.019142\n",
            "Validation loss decreased (0.019145 --> 0.019142).  Saving model ...\n",
            "train_loss:\n",
            "0.9861465913581324\n",
            "Epoch: 757 \tTraining Loss: 0.986147 \tValidation Loss: 0.019140\n",
            "Validation loss decreased (0.019142 --> 0.019140).  Saving model ...\n",
            "train_loss:\n",
            "0.9860603188747888\n",
            "Epoch: 758 \tTraining Loss: 0.986060 \tValidation Loss: 0.019138\n",
            "Validation loss decreased (0.019140 --> 0.019138).  Saving model ...\n",
            "train_loss:\n",
            "0.9859742304453483\n",
            "Epoch: 759 \tTraining Loss: 0.985974 \tValidation Loss: 0.019136\n",
            "Validation loss decreased (0.019138 --> 0.019136).  Saving model ...\n",
            "train_loss:\n",
            "0.9858883042911907\n",
            "Epoch: 760 \tTraining Loss: 0.985888 \tValidation Loss: 0.019134\n",
            "Validation loss decreased (0.019136 --> 0.019134).  Saving model ...\n",
            "train_loss:\n",
            "0.9858025469622769\n",
            "Epoch: 761 \tTraining Loss: 0.985803 \tValidation Loss: 0.019132\n",
            "Validation loss decreased (0.019134 --> 0.019132).  Saving model ...\n",
            "train_loss:\n",
            "0.9857169596048502\n",
            "Epoch: 762 \tTraining Loss: 0.985717 \tValidation Loss: 0.019130\n",
            "Validation loss decreased (0.019132 --> 0.019130).  Saving model ...\n",
            "train_loss:\n",
            "0.9856315641612797\n",
            "Epoch: 763 \tTraining Loss: 0.985632 \tValidation Loss: 0.019127\n",
            "Validation loss decreased (0.019130 --> 0.019127).  Saving model ...\n",
            "train_loss:\n",
            "0.9855463211680506\n",
            "Epoch: 764 \tTraining Loss: 0.985546 \tValidation Loss: 0.019125\n",
            "Validation loss decreased (0.019127 --> 0.019125).  Saving model ...\n",
            "train_loss:\n",
            "0.9854612535500265\n",
            "Epoch: 765 \tTraining Loss: 0.985461 \tValidation Loss: 0.019123\n",
            "Validation loss decreased (0.019125 --> 0.019123).  Saving model ...\n",
            "train_loss:\n",
            "0.9853763504997715\n",
            "Epoch: 766 \tTraining Loss: 0.985376 \tValidation Loss: 0.019121\n",
            "Validation loss decreased (0.019123 --> 0.019121).  Saving model ...\n",
            "train_loss:\n",
            "0.9852916369071374\n",
            "Epoch: 767 \tTraining Loss: 0.985292 \tValidation Loss: 0.019118\n",
            "Validation loss decreased (0.019121 --> 0.019118).  Saving model ...\n",
            "train_loss:\n",
            "0.9852070634836679\n",
            "Epoch: 768 \tTraining Loss: 0.985207 \tValidation Loss: 0.019116\n",
            "Validation loss decreased (0.019118 --> 0.019116).  Saving model ...\n",
            "train_loss:\n",
            "0.9851226533179754\n",
            "Epoch: 769 \tTraining Loss: 0.985123 \tValidation Loss: 0.019114\n",
            "Validation loss decreased (0.019116 --> 0.019114).  Saving model ...\n",
            "train_loss:\n",
            "0.9850384293349235\n",
            "Epoch: 770 \tTraining Loss: 0.985038 \tValidation Loss: 0.019111\n",
            "Validation loss decreased (0.019114 --> 0.019111).  Saving model ...\n",
            "train_loss:\n",
            "0.9849543579659619\n",
            "Epoch: 771 \tTraining Loss: 0.984954 \tValidation Loss: 0.019109\n",
            "Validation loss decreased (0.019111 --> 0.019109).  Saving model ...\n",
            "train_loss:\n",
            "0.9848704706509035\n",
            "Epoch: 772 \tTraining Loss: 0.984870 \tValidation Loss: 0.019107\n",
            "Validation loss decreased (0.019109 --> 0.019107).  Saving model ...\n",
            "train_loss:\n",
            "0.9847867213762723\n",
            "Epoch: 773 \tTraining Loss: 0.984787 \tValidation Loss: 0.019104\n",
            "Validation loss decreased (0.019107 --> 0.019104).  Saving model ...\n",
            "train_loss:\n",
            "0.9847031509155756\n",
            "Epoch: 774 \tTraining Loss: 0.984703 \tValidation Loss: 0.019102\n",
            "Validation loss decreased (0.019104 --> 0.019102).  Saving model ...\n",
            "train_loss:\n",
            "0.9846197499351187\n",
            "Epoch: 775 \tTraining Loss: 0.984620 \tValidation Loss: 0.019099\n",
            "Validation loss decreased (0.019102 --> 0.019099).  Saving model ...\n",
            "train_loss:\n",
            "0.9845365071362191\n",
            "Epoch: 776 \tTraining Loss: 0.984537 \tValidation Loss: 0.019097\n",
            "Validation loss decreased (0.019099 --> 0.019097).  Saving model ...\n",
            "train_loss:\n",
            "0.984453423501371\n",
            "Epoch: 777 \tTraining Loss: 0.984453 \tValidation Loss: 0.019094\n",
            "Validation loss decreased (0.019097 --> 0.019094).  Saving model ...\n",
            "train_loss:\n",
            "0.984370510329257\n",
            "Epoch: 778 \tTraining Loss: 0.984371 \tValidation Loss: 0.019092\n",
            "Validation loss decreased (0.019094 --> 0.019092).  Saving model ...\n",
            "train_loss:\n",
            "0.9842877609061671\n",
            "Epoch: 779 \tTraining Loss: 0.984288 \tValidation Loss: 0.019089\n",
            "Validation loss decreased (0.019092 --> 0.019089).  Saving model ...\n",
            "train_loss:\n",
            "0.9842051945544861\n",
            "Epoch: 780 \tTraining Loss: 0.984205 \tValidation Loss: 0.019087\n",
            "Validation loss decreased (0.019089 --> 0.019087).  Saving model ...\n",
            "train_loss:\n",
            "0.9841227397158906\n",
            "Epoch: 781 \tTraining Loss: 0.984123 \tValidation Loss: 0.019084\n",
            "Validation loss decreased (0.019087 --> 0.019084).  Saving model ...\n",
            "train_loss:\n",
            "0.9840404658199666\n",
            "Epoch: 782 \tTraining Loss: 0.984040 \tValidation Loss: 0.019082\n",
            "Validation loss decreased (0.019084 --> 0.019082).  Saving model ...\n",
            "train_loss:\n",
            "0.9839583538718276\n",
            "Epoch: 783 \tTraining Loss: 0.983958 \tValidation Loss: 0.019079\n",
            "Validation loss decreased (0.019082 --> 0.019079).  Saving model ...\n",
            "train_loss:\n",
            "0.9838763951927751\n",
            "Epoch: 784 \tTraining Loss: 0.983876 \tValidation Loss: 0.019076\n",
            "Validation loss decreased (0.019079 --> 0.019076).  Saving model ...\n",
            "train_loss:\n",
            "0.9837946064852097\n",
            "Epoch: 785 \tTraining Loss: 0.983795 \tValidation Loss: 0.019074\n",
            "Validation loss decreased (0.019076 --> 0.019074).  Saving model ...\n",
            "train_loss:\n",
            "0.9837129780879388\n",
            "Epoch: 786 \tTraining Loss: 0.983713 \tValidation Loss: 0.019071\n",
            "Validation loss decreased (0.019074 --> 0.019071).  Saving model ...\n",
            "train_loss:\n",
            "0.983631486584852\n",
            "Epoch: 787 \tTraining Loss: 0.983631 \tValidation Loss: 0.019069\n",
            "Validation loss decreased (0.019071 --> 0.019069).  Saving model ...\n",
            "train_loss:\n",
            "0.9835501722582094\n",
            "Epoch: 788 \tTraining Loss: 0.983550 \tValidation Loss: 0.019066\n",
            "Validation loss decreased (0.019069 --> 0.019066).  Saving model ...\n",
            "train_loss:\n",
            "0.983469022171838\n",
            "Epoch: 789 \tTraining Loss: 0.983469 \tValidation Loss: 0.019063\n",
            "Validation loss decreased (0.019066 --> 0.019063).  Saving model ...\n",
            "train_loss:\n",
            "0.9833880111083879\n",
            "Epoch: 790 \tTraining Loss: 0.983388 \tValidation Loss: 0.019061\n",
            "Validation loss decreased (0.019063 --> 0.019061).  Saving model ...\n",
            "train_loss:\n",
            "0.9833071827888489\n",
            "Epoch: 791 \tTraining Loss: 0.983307 \tValidation Loss: 0.019058\n",
            "Validation loss decreased (0.019061 --> 0.019058).  Saving model ...\n",
            "train_loss:\n",
            "0.9832264630349128\n",
            "Epoch: 792 \tTraining Loss: 0.983226 \tValidation Loss: 0.019055\n",
            "Validation loss decreased (0.019058 --> 0.019055).  Saving model ...\n",
            "train_loss:\n",
            "0.98314593568608\n",
            "Epoch: 793 \tTraining Loss: 0.983146 \tValidation Loss: 0.019052\n",
            "Validation loss decreased (0.019055 --> 0.019052).  Saving model ...\n",
            "train_loss:\n",
            "0.9830655453951804\n",
            "Epoch: 794 \tTraining Loss: 0.983066 \tValidation Loss: 0.019050\n",
            "Validation loss decreased (0.019052 --> 0.019050).  Saving model ...\n",
            "train_loss:\n",
            "0.9829853448893998\n",
            "Epoch: 795 \tTraining Loss: 0.982985 \tValidation Loss: 0.019047\n",
            "Validation loss decreased (0.019050 --> 0.019047).  Saving model ...\n",
            "train_loss:\n",
            "0.9829052513117319\n",
            "Epoch: 796 \tTraining Loss: 0.982905 \tValidation Loss: 0.019044\n",
            "Validation loss decreased (0.019047 --> 0.019044).  Saving model ...\n",
            "train_loss:\n",
            "0.9828253440804534\n",
            "Epoch: 797 \tTraining Loss: 0.982825 \tValidation Loss: 0.019041\n",
            "Validation loss decreased (0.019044 --> 0.019041).  Saving model ...\n",
            "train_loss:\n",
            "0.9827455647371628\n",
            "Epoch: 798 \tTraining Loss: 0.982746 \tValidation Loss: 0.019039\n",
            "Validation loss decreased (0.019041 --> 0.019039).  Saving model ...\n",
            "train_loss:\n",
            "0.982665954874112\n",
            "Epoch: 799 \tTraining Loss: 0.982666 \tValidation Loss: 0.019036\n",
            "Validation loss decreased (0.019039 --> 0.019036).  Saving model ...\n",
            "train_loss:\n",
            "0.9825864796127591\n",
            "Epoch: 800 \tTraining Loss: 0.982586 \tValidation Loss: 0.019033\n",
            "Validation loss decreased (0.019036 --> 0.019033).  Saving model ...\n",
            "train_loss:\n",
            "0.9825071751416384\n",
            "Epoch: 801 \tTraining Loss: 0.982507 \tValidation Loss: 0.019030\n",
            "Validation loss decreased (0.019033 --> 0.019030).  Saving model ...\n",
            "train_loss:\n",
            "0.9824280028159802\n",
            "Epoch: 802 \tTraining Loss: 0.982428 \tValidation Loss: 0.019027\n",
            "Validation loss decreased (0.019030 --> 0.019027).  Saving model ...\n",
            "train_loss:\n",
            "0.9823490032455423\n",
            "Epoch: 803 \tTraining Loss: 0.982349 \tValidation Loss: 0.019024\n",
            "Validation loss decreased (0.019027 --> 0.019024).  Saving model ...\n",
            "train_loss:\n",
            "0.982270129598104\n",
            "Epoch: 804 \tTraining Loss: 0.982270 \tValidation Loss: 0.019021\n",
            "Validation loss decreased (0.019024 --> 0.019021).  Saving model ...\n",
            "train_loss:\n",
            "0.9821914270683959\n",
            "Epoch: 805 \tTraining Loss: 0.982191 \tValidation Loss: 0.019019\n",
            "Validation loss decreased (0.019021 --> 0.019019).  Saving model ...\n",
            "train_loss:\n",
            "0.9821128797727626\n",
            "Epoch: 806 \tTraining Loss: 0.982113 \tValidation Loss: 0.019016\n",
            "Validation loss decreased (0.019019 --> 0.019016).  Saving model ...\n",
            "train_loss:\n",
            "0.9820344695350626\n",
            "Epoch: 807 \tTraining Loss: 0.982034 \tValidation Loss: 0.019013\n",
            "Validation loss decreased (0.019016 --> 0.019013).  Saving model ...\n",
            "train_loss:\n",
            "0.9819561952090525\n",
            "Epoch: 808 \tTraining Loss: 0.981956 \tValidation Loss: 0.019010\n",
            "Validation loss decreased (0.019013 --> 0.019010).  Saving model ...\n",
            "train_loss:\n",
            "0.9818780883982942\n",
            "Epoch: 809 \tTraining Loss: 0.981878 \tValidation Loss: 0.019007\n",
            "Validation loss decreased (0.019010 --> 0.019007).  Saving model ...\n",
            "train_loss:\n",
            "0.9818001201192101\n",
            "Epoch: 810 \tTraining Loss: 0.981800 \tValidation Loss: 0.019004\n",
            "Validation loss decreased (0.019007 --> 0.019004).  Saving model ...\n",
            "train_loss:\n",
            "0.9817222882430632\n",
            "Epoch: 811 \tTraining Loss: 0.981722 \tValidation Loss: 0.019001\n",
            "Validation loss decreased (0.019004 --> 0.019001).  Saving model ...\n",
            "train_loss:\n",
            "0.9816446068522694\n",
            "Epoch: 812 \tTraining Loss: 0.981645 \tValidation Loss: 0.018998\n",
            "Validation loss decreased (0.019001 --> 0.018998).  Saving model ...\n",
            "train_loss:\n",
            "0.9815670915029862\n",
            "Epoch: 813 \tTraining Loss: 0.981567 \tValidation Loss: 0.018995\n",
            "Validation loss decreased (0.018998 --> 0.018995).  Saving model ...\n",
            "train_loss:\n",
            "0.9814897050241848\n",
            "Epoch: 814 \tTraining Loss: 0.981490 \tValidation Loss: 0.018992\n",
            "Validation loss decreased (0.018995 --> 0.018992).  Saving model ...\n",
            "train_loss:\n",
            "0.9814124606795364\n",
            "Epoch: 815 \tTraining Loss: 0.981412 \tValidation Loss: 0.018989\n",
            "Validation loss decreased (0.018992 --> 0.018989).  Saving model ...\n",
            "train_loss:\n",
            "0.9813353846688847\n",
            "Epoch: 816 \tTraining Loss: 0.981335 \tValidation Loss: 0.018986\n",
            "Validation loss decreased (0.018989 --> 0.018986).  Saving model ...\n",
            "train_loss:\n",
            "0.9812584455524173\n",
            "Epoch: 817 \tTraining Loss: 0.981258 \tValidation Loss: 0.018983\n",
            "Validation loss decreased (0.018986 --> 0.018983).  Saving model ...\n",
            "train_loss:\n",
            "0.9811816374351691\n",
            "Epoch: 818 \tTraining Loss: 0.981182 \tValidation Loss: 0.018980\n",
            "Validation loss decreased (0.018983 --> 0.018980).  Saving model ...\n",
            "train_loss:\n",
            "0.981104986516984\n",
            "Epoch: 819 \tTraining Loss: 0.981105 \tValidation Loss: 0.018977\n",
            "Validation loss decreased (0.018980 --> 0.018977).  Saving model ...\n",
            "train_loss:\n",
            "0.9810284415444175\n",
            "Epoch: 820 \tTraining Loss: 0.981028 \tValidation Loss: 0.018974\n",
            "Validation loss decreased (0.018977 --> 0.018974).  Saving model ...\n",
            "train_loss:\n",
            "0.9809521045331117\n",
            "Epoch: 821 \tTraining Loss: 0.980952 \tValidation Loss: 0.018971\n",
            "Validation loss decreased (0.018974 --> 0.018971).  Saving model ...\n",
            "train_loss:\n",
            "0.9808758597125063\n",
            "Epoch: 822 \tTraining Loss: 0.980876 \tValidation Loss: 0.018968\n",
            "Validation loss decreased (0.018971 --> 0.018968).  Saving model ...\n",
            "train_loss:\n",
            "0.9807997755296938\n",
            "Epoch: 823 \tTraining Loss: 0.980800 \tValidation Loss: 0.018965\n",
            "Validation loss decreased (0.018968 --> 0.018965).  Saving model ...\n",
            "train_loss:\n",
            "0.9807238269310731\n",
            "Epoch: 824 \tTraining Loss: 0.980724 \tValidation Loss: 0.018962\n",
            "Validation loss decreased (0.018965 --> 0.018962).  Saving model ...\n",
            "train_loss:\n",
            "0.9806480456839551\n",
            "Epoch: 825 \tTraining Loss: 0.980648 \tValidation Loss: 0.018959\n",
            "Validation loss decreased (0.018962 --> 0.018959).  Saving model ...\n",
            "train_loss:\n",
            "0.9805723741486833\n",
            "Epoch: 826 \tTraining Loss: 0.980572 \tValidation Loss: 0.018956\n",
            "Validation loss decreased (0.018959 --> 0.018956).  Saving model ...\n",
            "train_loss:\n",
            "0.9804968377063562\n",
            "Epoch: 827 \tTraining Loss: 0.980497 \tValidation Loss: 0.018953\n",
            "Validation loss decreased (0.018956 --> 0.018953).  Saving model ...\n",
            "train_loss:\n",
            "0.9804214712355163\n",
            "Epoch: 828 \tTraining Loss: 0.980421 \tValidation Loss: 0.018950\n",
            "Validation loss decreased (0.018953 --> 0.018950).  Saving model ...\n",
            "train_loss:\n",
            "0.9803462429688528\n",
            "Epoch: 829 \tTraining Loss: 0.980346 \tValidation Loss: 0.018947\n",
            "Validation loss decreased (0.018950 --> 0.018947).  Saving model ...\n",
            "train_loss:\n",
            "0.9802711427539259\n",
            "Epoch: 830 \tTraining Loss: 0.980271 \tValidation Loss: 0.018944\n",
            "Validation loss decreased (0.018947 --> 0.018944).  Saving model ...\n",
            "train_loss:\n",
            "0.98019618336316\n",
            "Epoch: 831 \tTraining Loss: 0.980196 \tValidation Loss: 0.018941\n",
            "Validation loss decreased (0.018944 --> 0.018941).  Saving model ...\n",
            "train_loss:\n",
            "0.980121340070452\n",
            "Epoch: 832 \tTraining Loss: 0.980121 \tValidation Loss: 0.018938\n",
            "Validation loss decreased (0.018941 --> 0.018938).  Saving model ...\n",
            "train_loss:\n",
            "0.9800466711704547\n",
            "Epoch: 833 \tTraining Loss: 0.980047 \tValidation Loss: 0.018935\n",
            "Validation loss decreased (0.018938 --> 0.018935).  Saving model ...\n",
            "train_loss:\n",
            "0.9799721183685156\n",
            "Epoch: 834 \tTraining Loss: 0.979972 \tValidation Loss: 0.018932\n",
            "Validation loss decreased (0.018935 --> 0.018932).  Saving model ...\n",
            "train_loss:\n",
            "0.9798977157244315\n",
            "Epoch: 835 \tTraining Loss: 0.979898 \tValidation Loss: 0.018929\n",
            "Validation loss decreased (0.018932 --> 0.018929).  Saving model ...\n",
            "train_loss:\n",
            "0.9798234376933549\n",
            "Epoch: 836 \tTraining Loss: 0.979823 \tValidation Loss: 0.018926\n",
            "Validation loss decreased (0.018929 --> 0.018926).  Saving model ...\n",
            "train_loss:\n",
            "0.9797492970477094\n",
            "Epoch: 837 \tTraining Loss: 0.979749 \tValidation Loss: 0.018923\n",
            "Validation loss decreased (0.018926 --> 0.018923).  Saving model ...\n",
            "train_loss:\n",
            "0.9796752890387734\n",
            "Epoch: 838 \tTraining Loss: 0.979675 \tValidation Loss: 0.018920\n",
            "Validation loss decreased (0.018923 --> 0.018920).  Saving model ...\n",
            "train_loss:\n",
            "0.9796014288952063\n",
            "Epoch: 839 \tTraining Loss: 0.979601 \tValidation Loss: 0.018917\n",
            "Validation loss decreased (0.018920 --> 0.018917).  Saving model ...\n",
            "train_loss:\n",
            "0.9795276981133682\n",
            "Epoch: 840 \tTraining Loss: 0.979528 \tValidation Loss: 0.018914\n",
            "Validation loss decreased (0.018917 --> 0.018914).  Saving model ...\n",
            "train_loss:\n",
            "0.9794541068456986\n",
            "Epoch: 841 \tTraining Loss: 0.979454 \tValidation Loss: 0.018910\n",
            "Validation loss decreased (0.018914 --> 0.018910).  Saving model ...\n",
            "train_loss:\n",
            "0.9793806356060636\n",
            "Epoch: 842 \tTraining Loss: 0.979381 \tValidation Loss: 0.018907\n",
            "Validation loss decreased (0.018910 --> 0.018907).  Saving model ...\n",
            "train_loss:\n",
            "0.9793073176355153\n",
            "Epoch: 843 \tTraining Loss: 0.979307 \tValidation Loss: 0.018904\n",
            "Validation loss decreased (0.018907 --> 0.018904).  Saving model ...\n",
            "train_loss:\n",
            "0.9792341165817701\n",
            "Epoch: 844 \tTraining Loss: 0.979234 \tValidation Loss: 0.018901\n",
            "Validation loss decreased (0.018904 --> 0.018901).  Saving model ...\n",
            "train_loss:\n",
            "0.9791610570071818\n",
            "Epoch: 845 \tTraining Loss: 0.979161 \tValidation Loss: 0.018898\n",
            "Validation loss decreased (0.018901 --> 0.018898).  Saving model ...\n",
            "train_loss:\n",
            "0.9790881336717815\n",
            "Epoch: 846 \tTraining Loss: 0.979088 \tValidation Loss: 0.018895\n",
            "Validation loss decreased (0.018898 --> 0.018895).  Saving model ...\n",
            "train_loss:\n",
            "0.9790153342943925\n",
            "Epoch: 847 \tTraining Loss: 0.979015 \tValidation Loss: 0.018892\n",
            "Validation loss decreased (0.018895 --> 0.018892).  Saving model ...\n",
            "train_loss:\n",
            "0.9789426898235803\n",
            "Epoch: 848 \tTraining Loss: 0.978943 \tValidation Loss: 0.018889\n",
            "Validation loss decreased (0.018892 --> 0.018889).  Saving model ...\n",
            "train_loss:\n",
            "0.9788701454034219\n",
            "Epoch: 849 \tTraining Loss: 0.978870 \tValidation Loss: 0.018886\n",
            "Validation loss decreased (0.018889 --> 0.018886).  Saving model ...\n",
            "train_loss:\n",
            "0.9787977555623422\n",
            "Epoch: 850 \tTraining Loss: 0.978798 \tValidation Loss: 0.018883\n",
            "Validation loss decreased (0.018886 --> 0.018883).  Saving model ...\n",
            "train_loss:\n",
            "0.9787254836205598\n",
            "Epoch: 851 \tTraining Loss: 0.978725 \tValidation Loss: 0.018880\n",
            "Validation loss decreased (0.018883 --> 0.018880).  Saving model ...\n",
            "train_loss:\n",
            "0.9786533452979811\n",
            "Epoch: 852 \tTraining Loss: 0.978653 \tValidation Loss: 0.018876\n",
            "Validation loss decreased (0.018880 --> 0.018876).  Saving model ...\n",
            "train_loss:\n",
            "0.9785813522207868\n",
            "Epoch: 853 \tTraining Loss: 0.978581 \tValidation Loss: 0.018873\n",
            "Validation loss decreased (0.018876 --> 0.018873).  Saving model ...\n",
            "train_loss:\n",
            "0.9785094768791408\n",
            "Epoch: 854 \tTraining Loss: 0.978509 \tValidation Loss: 0.018870\n",
            "Validation loss decreased (0.018873 --> 0.018870).  Saving model ...\n",
            "train_loss:\n",
            "0.9784377145243215\n",
            "Epoch: 855 \tTraining Loss: 0.978438 \tValidation Loss: 0.018867\n",
            "Validation loss decreased (0.018870 --> 0.018867).  Saving model ...\n",
            "train_loss:\n",
            "0.9783660875899451\n",
            "Epoch: 856 \tTraining Loss: 0.978366 \tValidation Loss: 0.018864\n",
            "Validation loss decreased (0.018867 --> 0.018864).  Saving model ...\n",
            "train_loss:\n",
            "0.9782945991872432\n",
            "Epoch: 857 \tTraining Loss: 0.978295 \tValidation Loss: 0.018861\n",
            "Validation loss decreased (0.018864 --> 0.018861).  Saving model ...\n",
            "train_loss:\n",
            "0.9782232337600583\n",
            "Epoch: 858 \tTraining Loss: 0.978223 \tValidation Loss: 0.018858\n",
            "Validation loss decreased (0.018861 --> 0.018858).  Saving model ...\n",
            "train_loss:\n",
            "0.9781520042445634\n",
            "Epoch: 859 \tTraining Loss: 0.978152 \tValidation Loss: 0.018855\n",
            "Validation loss decreased (0.018858 --> 0.018855).  Saving model ...\n",
            "train_loss:\n",
            "0.9780808973770875\n",
            "Epoch: 860 \tTraining Loss: 0.978081 \tValidation Loss: 0.018852\n",
            "Validation loss decreased (0.018855 --> 0.018852).  Saving model ...\n",
            "train_loss:\n",
            "0.9780099034964383\n",
            "Epoch: 861 \tTraining Loss: 0.978010 \tValidation Loss: 0.018849\n",
            "Validation loss decreased (0.018852 --> 0.018849).  Saving model ...\n",
            "train_loss:\n",
            "0.9779390763122957\n",
            "Epoch: 862 \tTraining Loss: 0.977939 \tValidation Loss: 0.018846\n",
            "Validation loss decreased (0.018849 --> 0.018846).  Saving model ...\n",
            "train_loss:\n",
            "0.9778683434475909\n",
            "Epoch: 863 \tTraining Loss: 0.977868 \tValidation Loss: 0.018842\n",
            "Validation loss decreased (0.018846 --> 0.018842).  Saving model ...\n",
            "train_loss:\n",
            "0.9777977276634384\n",
            "Epoch: 864 \tTraining Loss: 0.977798 \tValidation Loss: 0.018839\n",
            "Validation loss decreased (0.018842 --> 0.018839).  Saving model ...\n",
            "train_loss:\n",
            "0.9777272515572034\n",
            "Epoch: 865 \tTraining Loss: 0.977727 \tValidation Loss: 0.018836\n",
            "Validation loss decreased (0.018839 --> 0.018836).  Saving model ...\n",
            "train_loss:\n",
            "0.9776569054676936\n",
            "Epoch: 866 \tTraining Loss: 0.977657 \tValidation Loss: 0.018833\n",
            "Validation loss decreased (0.018836 --> 0.018833).  Saving model ...\n",
            "train_loss:\n",
            "0.9775866816987048\n",
            "Epoch: 867 \tTraining Loss: 0.977587 \tValidation Loss: 0.018830\n",
            "Validation loss decreased (0.018833 --> 0.018830).  Saving model ...\n",
            "train_loss:\n",
            "0.9775165712440407\n",
            "Epoch: 868 \tTraining Loss: 0.977517 \tValidation Loss: 0.018827\n",
            "Validation loss decreased (0.018830 --> 0.018827).  Saving model ...\n",
            "train_loss:\n",
            "0.9774466048885178\n",
            "Epoch: 869 \tTraining Loss: 0.977447 \tValidation Loss: 0.018824\n",
            "Validation loss decreased (0.018827 --> 0.018824).  Saving model ...\n",
            "train_loss:\n",
            "0.9773767400573898\n",
            "Epoch: 870 \tTraining Loss: 0.977377 \tValidation Loss: 0.018821\n",
            "Validation loss decreased (0.018824 --> 0.018821).  Saving model ...\n",
            "train_loss:\n",
            "0.9773070111379518\n",
            "Epoch: 871 \tTraining Loss: 0.977307 \tValidation Loss: 0.018818\n",
            "Validation loss decreased (0.018821 --> 0.018818).  Saving model ...\n",
            "train_loss:\n",
            "0.9772373988078191\n",
            "Epoch: 872 \tTraining Loss: 0.977237 \tValidation Loss: 0.018815\n",
            "Validation loss decreased (0.018818 --> 0.018815).  Saving model ...\n",
            "train_loss:\n",
            "0.9771679155119173\n",
            "Epoch: 873 \tTraining Loss: 0.977168 \tValidation Loss: 0.018812\n",
            "Validation loss decreased (0.018815 --> 0.018812).  Saving model ...\n",
            "train_loss:\n",
            "0.9770985535540424\n",
            "Epoch: 874 \tTraining Loss: 0.977099 \tValidation Loss: 0.018809\n",
            "Validation loss decreased (0.018812 --> 0.018809).  Saving model ...\n",
            "train_loss:\n",
            "0.9770293224316376\n",
            "Epoch: 875 \tTraining Loss: 0.977029 \tValidation Loss: 0.018806\n",
            "Validation loss decreased (0.018809 --> 0.018806).  Saving model ...\n",
            "train_loss:\n",
            "0.9769601975823496\n",
            "Epoch: 876 \tTraining Loss: 0.976960 \tValidation Loss: 0.018803\n",
            "Validation loss decreased (0.018806 --> 0.018803).  Saving model ...\n",
            "train_loss:\n",
            "0.9768911875211276\n",
            "Epoch: 877 \tTraining Loss: 0.976891 \tValidation Loss: 0.018799\n",
            "Validation loss decreased (0.018803 --> 0.018799).  Saving model ...\n",
            "train_loss:\n",
            "0.9768223025641598\n",
            "Epoch: 878 \tTraining Loss: 0.976822 \tValidation Loss: 0.018796\n",
            "Validation loss decreased (0.018799 --> 0.018796).  Saving model ...\n",
            "train_loss:\n",
            "0.9767535584313529\n",
            "Epoch: 879 \tTraining Loss: 0.976754 \tValidation Loss: 0.018793\n",
            "Validation loss decreased (0.018796 --> 0.018793).  Saving model ...\n",
            "train_loss:\n",
            "0.9766849356365728\n",
            "Epoch: 880 \tTraining Loss: 0.976685 \tValidation Loss: 0.018790\n",
            "Validation loss decreased (0.018793 --> 0.018790).  Saving model ...\n",
            "train_loss:\n",
            "0.9766164063424855\n",
            "Epoch: 881 \tTraining Loss: 0.976616 \tValidation Loss: 0.018787\n",
            "Validation loss decreased (0.018790 --> 0.018787).  Saving model ...\n",
            "train_loss:\n",
            "0.9765480005151623\n",
            "Epoch: 882 \tTraining Loss: 0.976548 \tValidation Loss: 0.018784\n",
            "Validation loss decreased (0.018787 --> 0.018784).  Saving model ...\n",
            "train_loss:\n",
            "0.9764797278157957\n",
            "Epoch: 883 \tTraining Loss: 0.976480 \tValidation Loss: 0.018781\n",
            "Validation loss decreased (0.018784 --> 0.018781).  Saving model ...\n",
            "train_loss:\n",
            "0.9764115676120088\n",
            "Epoch: 884 \tTraining Loss: 0.976412 \tValidation Loss: 0.018778\n",
            "Validation loss decreased (0.018781 --> 0.018778).  Saving model ...\n",
            "train_loss:\n",
            "0.9763435151550796\n",
            "Epoch: 885 \tTraining Loss: 0.976344 \tValidation Loss: 0.018775\n",
            "Validation loss decreased (0.018778 --> 0.018775).  Saving model ...\n",
            "train_loss:\n",
            "0.9762755933698717\n",
            "Epoch: 886 \tTraining Loss: 0.976276 \tValidation Loss: 0.018772\n",
            "Validation loss decreased (0.018775 --> 0.018772).  Saving model ...\n",
            "train_loss:\n",
            "0.9762077860452316\n",
            "Epoch: 887 \tTraining Loss: 0.976208 \tValidation Loss: 0.018769\n",
            "Validation loss decreased (0.018772 --> 0.018769).  Saving model ...\n",
            "train_loss:\n",
            "0.9761401005498656\n",
            "Epoch: 888 \tTraining Loss: 0.976140 \tValidation Loss: 0.018766\n",
            "Validation loss decreased (0.018769 --> 0.018766).  Saving model ...\n",
            "train_loss:\n",
            "0.9760725288600712\n",
            "Epoch: 889 \tTraining Loss: 0.976073 \tValidation Loss: 0.018763\n",
            "Validation loss decreased (0.018766 --> 0.018763).  Saving model ...\n",
            "train_loss:\n",
            "0.9760050645896367\n",
            "Epoch: 890 \tTraining Loss: 0.976005 \tValidation Loss: 0.018760\n",
            "Validation loss decreased (0.018763 --> 0.018760).  Saving model ...\n",
            "train_loss:\n",
            "0.9759377159260132\n",
            "Epoch: 891 \tTraining Loss: 0.975938 \tValidation Loss: 0.018757\n",
            "Validation loss decreased (0.018760 --> 0.018757).  Saving model ...\n",
            "train_loss:\n",
            "0.9758704940041343\n",
            "Epoch: 892 \tTraining Loss: 0.975870 \tValidation Loss: 0.018754\n",
            "Validation loss decreased (0.018757 --> 0.018754).  Saving model ...\n",
            "train_loss:\n",
            "0.9758033734429014\n",
            "Epoch: 893 \tTraining Loss: 0.975803 \tValidation Loss: 0.018751\n",
            "Validation loss decreased (0.018754 --> 0.018751).  Saving model ...\n",
            "train_loss:\n",
            "0.9757363735646992\n",
            "Epoch: 894 \tTraining Loss: 0.975736 \tValidation Loss: 0.018748\n",
            "Validation loss decreased (0.018751 --> 0.018748).  Saving model ...\n",
            "train_loss:\n",
            "0.9756694950245239\n",
            "Epoch: 895 \tTraining Loss: 0.975669 \tValidation Loss: 0.018745\n",
            "Validation loss decreased (0.018748 --> 0.018745).  Saving model ...\n",
            "train_loss:\n",
            "0.975602727997434\n",
            "Epoch: 896 \tTraining Loss: 0.975603 \tValidation Loss: 0.018742\n",
            "Validation loss decreased (0.018745 --> 0.018742).  Saving model ...\n",
            "train_loss:\n",
            "0.9755360541435388\n",
            "Epoch: 897 \tTraining Loss: 0.975536 \tValidation Loss: 0.018739\n",
            "Validation loss decreased (0.018742 --> 0.018739).  Saving model ...\n",
            "train_loss:\n",
            "0.9754695225875456\n",
            "Epoch: 898 \tTraining Loss: 0.975470 \tValidation Loss: 0.018736\n",
            "Validation loss decreased (0.018739 --> 0.018736).  Saving model ...\n",
            "train_loss:\n",
            "0.975403083549751\n",
            "Epoch: 899 \tTraining Loss: 0.975403 \tValidation Loss: 0.018733\n",
            "Validation loss decreased (0.018736 --> 0.018733).  Saving model ...\n",
            "train_loss:\n",
            "0.9753367871373564\n",
            "Epoch: 900 \tTraining Loss: 0.975337 \tValidation Loss: 0.018730\n",
            "Validation loss decreased (0.018733 --> 0.018730).  Saving model ...\n",
            "train_loss:\n",
            "0.9752705850443997\n",
            "Epoch: 901 \tTraining Loss: 0.975271 \tValidation Loss: 0.018727\n",
            "Validation loss decreased (0.018730 --> 0.018727).  Saving model ...\n",
            "train_loss:\n",
            "0.9752044820196026\n",
            "Epoch: 902 \tTraining Loss: 0.975204 \tValidation Loss: 0.018724\n",
            "Validation loss decreased (0.018727 --> 0.018724).  Saving model ...\n",
            "train_loss:\n",
            "0.9751384965666048\n",
            "Epoch: 903 \tTraining Loss: 0.975138 \tValidation Loss: 0.018722\n",
            "Validation loss decreased (0.018724 --> 0.018722).  Saving model ...\n",
            "train_loss:\n",
            "0.9750726421128263\n",
            "Epoch: 904 \tTraining Loss: 0.975073 \tValidation Loss: 0.018719\n",
            "Validation loss decreased (0.018722 --> 0.018719).  Saving model ...\n",
            "train_loss:\n",
            "0.9750068821422346\n",
            "Epoch: 905 \tTraining Loss: 0.975007 \tValidation Loss: 0.018716\n",
            "Validation loss decreased (0.018719 --> 0.018716).  Saving model ...\n",
            "train_loss:\n",
            "0.9749412179648221\n",
            "Epoch: 906 \tTraining Loss: 0.974941 \tValidation Loss: 0.018713\n",
            "Validation loss decreased (0.018716 --> 0.018713).  Saving model ...\n",
            "train_loss:\n",
            "0.9748756949390683\n",
            "Epoch: 907 \tTraining Loss: 0.974876 \tValidation Loss: 0.018710\n",
            "Validation loss decreased (0.018713 --> 0.018710).  Saving model ...\n",
            "train_loss:\n",
            "0.9748102703264782\n",
            "Epoch: 908 \tTraining Loss: 0.974810 \tValidation Loss: 0.018707\n",
            "Validation loss decreased (0.018710 --> 0.018707).  Saving model ...\n",
            "train_loss:\n",
            "0.974744941670816\n",
            "Epoch: 909 \tTraining Loss: 0.974745 \tValidation Loss: 0.018704\n",
            "Validation loss decreased (0.018707 --> 0.018704).  Saving model ...\n",
            "train_loss:\n",
            "0.97467974286813\n",
            "Epoch: 910 \tTraining Loss: 0.974680 \tValidation Loss: 0.018701\n",
            "Validation loss decreased (0.018704 --> 0.018701).  Saving model ...\n",
            "train_loss:\n",
            "0.974614639203627\n",
            "Epoch: 911 \tTraining Loss: 0.974615 \tValidation Loss: 0.018698\n",
            "Validation loss decreased (0.018701 --> 0.018698).  Saving model ...\n",
            "train_loss:\n",
            "0.97454964361348\n",
            "Epoch: 912 \tTraining Loss: 0.974550 \tValidation Loss: 0.018695\n",
            "Validation loss decreased (0.018698 --> 0.018695).  Saving model ...\n",
            "train_loss:\n",
            "0.974484771162599\n",
            "Epoch: 913 \tTraining Loss: 0.974485 \tValidation Loss: 0.018692\n",
            "Validation loss decreased (0.018695 --> 0.018692).  Saving model ...\n",
            "train_loss:\n",
            "0.9744199897561755\n",
            "Epoch: 914 \tTraining Loss: 0.974420 \tValidation Loss: 0.018690\n",
            "Validation loss decreased (0.018692 --> 0.018690).  Saving model ...\n",
            "train_loss:\n",
            "0.9743553285415356\n",
            "Epoch: 915 \tTraining Loss: 0.974355 \tValidation Loss: 0.018687\n",
            "Validation loss decreased (0.018690 --> 0.018687).  Saving model ...\n",
            "train_loss:\n",
            "0.9742907758924987\n",
            "Epoch: 916 \tTraining Loss: 0.974291 \tValidation Loss: 0.018684\n",
            "Validation loss decreased (0.018687 --> 0.018684).  Saving model ...\n",
            "train_loss:\n",
            "0.9742263095391975\n",
            "Epoch: 917 \tTraining Loss: 0.974226 \tValidation Loss: 0.018681\n",
            "Validation loss decreased (0.018684 --> 0.018681).  Saving model ...\n",
            "train_loss:\n",
            "0.9741619807350766\n",
            "Epoch: 918 \tTraining Loss: 0.974162 \tValidation Loss: 0.018678\n",
            "Validation loss decreased (0.018681 --> 0.018678).  Saving model ...\n",
            "train_loss:\n",
            "0.9740977362617032\n",
            "Epoch: 919 \tTraining Loss: 0.974098 \tValidation Loss: 0.018675\n",
            "Validation loss decreased (0.018678 --> 0.018675).  Saving model ...\n",
            "train_loss:\n",
            "0.9740336134538545\n",
            "Epoch: 920 \tTraining Loss: 0.974034 \tValidation Loss: 0.018672\n",
            "Validation loss decreased (0.018675 --> 0.018672).  Saving model ...\n",
            "train_loss:\n",
            "0.973969585784189\n",
            "Epoch: 921 \tTraining Loss: 0.973970 \tValidation Loss: 0.018670\n",
            "Validation loss decreased (0.018672 --> 0.018670).  Saving model ...\n",
            "train_loss:\n",
            "0.9739056611126595\n",
            "Epoch: 922 \tTraining Loss: 0.973906 \tValidation Loss: 0.018667\n",
            "Validation loss decreased (0.018670 --> 0.018667).  Saving model ...\n",
            "train_loss:\n",
            "0.973841857779157\n",
            "Epoch: 923 \tTraining Loss: 0.973842 \tValidation Loss: 0.018664\n",
            "Validation loss decreased (0.018667 --> 0.018664).  Saving model ...\n",
            "train_loss:\n",
            "0.9737781413963863\n",
            "Epoch: 924 \tTraining Loss: 0.973778 \tValidation Loss: 0.018661\n",
            "Validation loss decreased (0.018664 --> 0.018661).  Saving model ...\n",
            "train_loss:\n",
            "0.9737145306317361\n",
            "Epoch: 925 \tTraining Loss: 0.973715 \tValidation Loss: 0.018658\n",
            "Validation loss decreased (0.018661 --> 0.018658).  Saving model ...\n",
            "train_loss:\n",
            "0.9736510374388851\n",
            "Epoch: 926 \tTraining Loss: 0.973651 \tValidation Loss: 0.018655\n",
            "Validation loss decreased (0.018658 --> 0.018655).  Saving model ...\n",
            "train_loss:\n",
            "0.9735876382379741\n",
            "Epoch: 927 \tTraining Loss: 0.973588 \tValidation Loss: 0.018653\n",
            "Validation loss decreased (0.018655 --> 0.018653).  Saving model ...\n",
            "train_loss:\n",
            "0.9735243430176934\n",
            "Epoch: 928 \tTraining Loss: 0.973524 \tValidation Loss: 0.018650\n",
            "Validation loss decreased (0.018653 --> 0.018650).  Saving model ...\n",
            "train_loss:\n",
            "0.9734611576730079\n",
            "Epoch: 929 \tTraining Loss: 0.973461 \tValidation Loss: 0.018647\n",
            "Validation loss decreased (0.018650 --> 0.018647).  Saving model ...\n",
            "train_loss:\n",
            "0.9733980697589916\n",
            "Epoch: 930 \tTraining Loss: 0.973398 \tValidation Loss: 0.018644\n",
            "Validation loss decreased (0.018647 --> 0.018644).  Saving model ...\n",
            "train_loss:\n",
            "0.9733350818956291\n",
            "Epoch: 931 \tTraining Loss: 0.973335 \tValidation Loss: 0.018642\n",
            "Validation loss decreased (0.018644 --> 0.018642).  Saving model ...\n",
            "train_loss:\n",
            "0.973272212095313\n",
            "Epoch: 932 \tTraining Loss: 0.973272 \tValidation Loss: 0.018639\n",
            "Validation loss decreased (0.018642 --> 0.018639).  Saving model ...\n",
            "train_loss:\n",
            "0.9732094230232659\n",
            "Epoch: 933 \tTraining Loss: 0.973209 \tValidation Loss: 0.018636\n",
            "Validation loss decreased (0.018639 --> 0.018636).  Saving model ...\n",
            "train_loss:\n",
            "0.9731467448093079\n",
            "Epoch: 934 \tTraining Loss: 0.973147 \tValidation Loss: 0.018633\n",
            "Validation loss decreased (0.018636 --> 0.018633).  Saving model ...\n",
            "train_loss:\n",
            "0.9730841782721844\n",
            "Epoch: 935 \tTraining Loss: 0.973084 \tValidation Loss: 0.018631\n",
            "Validation loss decreased (0.018633 --> 0.018631).  Saving model ...\n",
            "train_loss:\n",
            "0.9730217145694481\n",
            "Epoch: 936 \tTraining Loss: 0.973022 \tValidation Loss: 0.018628\n",
            "Validation loss decreased (0.018631 --> 0.018628).  Saving model ...\n",
            "train_loss:\n",
            "0.9729593510811145\n",
            "Epoch: 937 \tTraining Loss: 0.972959 \tValidation Loss: 0.018625\n",
            "Validation loss decreased (0.018628 --> 0.018625).  Saving model ...\n",
            "train_loss:\n",
            "0.9728970750347599\n",
            "Epoch: 938 \tTraining Loss: 0.972897 \tValidation Loss: 0.018622\n",
            "Validation loss decreased (0.018625 --> 0.018622).  Saving model ...\n",
            "train_loss:\n",
            "0.9728349080452552\n",
            "Epoch: 939 \tTraining Loss: 0.972835 \tValidation Loss: 0.018620\n",
            "Validation loss decreased (0.018622 --> 0.018620).  Saving model ...\n",
            "train_loss:\n",
            "0.9727728319364589\n",
            "Epoch: 940 \tTraining Loss: 0.972773 \tValidation Loss: 0.018617\n",
            "Validation loss decreased (0.018620 --> 0.018617).  Saving model ...\n",
            "train_loss:\n",
            "0.9727108642295167\n",
            "Epoch: 941 \tTraining Loss: 0.972711 \tValidation Loss: 0.018614\n",
            "Validation loss decreased (0.018617 --> 0.018614).  Saving model ...\n",
            "train_loss:\n",
            "0.9726489761046001\n",
            "Epoch: 942 \tTraining Loss: 0.972649 \tValidation Loss: 0.018612\n",
            "Validation loss decreased (0.018614 --> 0.018612).  Saving model ...\n",
            "train_loss:\n",
            "0.972587206697726\n",
            "Epoch: 943 \tTraining Loss: 0.972587 \tValidation Loss: 0.018609\n",
            "Validation loss decreased (0.018612 --> 0.018609).  Saving model ...\n",
            "train_loss:\n",
            "0.9725255402889881\n",
            "Epoch: 944 \tTraining Loss: 0.972526 \tValidation Loss: 0.018606\n",
            "Validation loss decreased (0.018609 --> 0.018606).  Saving model ...\n",
            "train_loss:\n",
            "0.9724639719659156\n",
            "Epoch: 945 \tTraining Loss: 0.972464 \tValidation Loss: 0.018604\n",
            "Validation loss decreased (0.018606 --> 0.018604).  Saving model ...\n",
            "train_loss:\n",
            "0.972402511880948\n",
            "Epoch: 946 \tTraining Loss: 0.972403 \tValidation Loss: 0.018601\n",
            "Validation loss decreased (0.018604 --> 0.018601).  Saving model ...\n",
            "train_loss:\n",
            "0.9723411195880765\n",
            "Epoch: 947 \tTraining Loss: 0.972341 \tValidation Loss: 0.018598\n",
            "Validation loss decreased (0.018601 --> 0.018598).  Saving model ...\n",
            "train_loss:\n",
            "0.9722798476507376\n",
            "Epoch: 948 \tTraining Loss: 0.972280 \tValidation Loss: 0.018596\n",
            "Validation loss decreased (0.018598 --> 0.018596).  Saving model ...\n",
            "train_loss:\n",
            "0.9722186675766012\n",
            "Epoch: 949 \tTraining Loss: 0.972219 \tValidation Loss: 0.018593\n",
            "Validation loss decreased (0.018596 --> 0.018593).  Saving model ...\n",
            "train_loss:\n",
            "0.9721575842781381\n",
            "Epoch: 950 \tTraining Loss: 0.972158 \tValidation Loss: 0.018590\n",
            "Validation loss decreased (0.018593 --> 0.018590).  Saving model ...\n",
            "train_loss:\n",
            "0.9720966102002742\n",
            "Epoch: 951 \tTraining Loss: 0.972097 \tValidation Loss: 0.018588\n",
            "Validation loss decreased (0.018590 --> 0.018588).  Saving model ...\n",
            "train_loss:\n",
            "0.9720357204531576\n",
            "Epoch: 952 \tTraining Loss: 0.972036 \tValidation Loss: 0.018585\n",
            "Validation loss decreased (0.018588 --> 0.018585).  Saving model ...\n",
            "train_loss:\n",
            "0.97197493321293\n",
            "Epoch: 953 \tTraining Loss: 0.971975 \tValidation Loss: 0.018582\n",
            "Validation loss decreased (0.018585 --> 0.018582).  Saving model ...\n",
            "train_loss:\n",
            "0.9719142609245175\n",
            "Epoch: 954 \tTraining Loss: 0.971914 \tValidation Loss: 0.018580\n",
            "Validation loss decreased (0.018582 --> 0.018580).  Saving model ...\n",
            "train_loss:\n",
            "0.9718536462757613\n",
            "Epoch: 955 \tTraining Loss: 0.971854 \tValidation Loss: 0.018577\n",
            "Validation loss decreased (0.018580 --> 0.018577).  Saving model ...\n",
            "train_loss:\n",
            "0.9717931630355972\n",
            "Epoch: 956 \tTraining Loss: 0.971793 \tValidation Loss: 0.018575\n",
            "Validation loss decreased (0.018577 --> 0.018575).  Saving model ...\n",
            "train_loss:\n",
            "0.9717327683017805\n",
            "Epoch: 957 \tTraining Loss: 0.971733 \tValidation Loss: 0.018572\n",
            "Validation loss decreased (0.018575 --> 0.018572).  Saving model ...\n",
            "train_loss:\n",
            "0.971672461091817\n",
            "Epoch: 958 \tTraining Loss: 0.971672 \tValidation Loss: 0.018570\n",
            "Validation loss decreased (0.018572 --> 0.018570).  Saving model ...\n",
            "train_loss:\n",
            "0.9716122527862643\n",
            "Epoch: 959 \tTraining Loss: 0.971612 \tValidation Loss: 0.018567\n",
            "Validation loss decreased (0.018570 --> 0.018567).  Saving model ...\n",
            "train_loss:\n",
            "0.9715521279108393\n",
            "Epoch: 960 \tTraining Loss: 0.971552 \tValidation Loss: 0.018564\n",
            "Validation loss decreased (0.018567 --> 0.018564).  Saving model ...\n",
            "train_loss:\n",
            "0.9714921133203821\n",
            "Epoch: 961 \tTraining Loss: 0.971492 \tValidation Loss: 0.018562\n",
            "Validation loss decreased (0.018564 --> 0.018562).  Saving model ...\n",
            "train_loss:\n",
            "0.9714321621007972\n",
            "Epoch: 962 \tTraining Loss: 0.971432 \tValidation Loss: 0.018559\n",
            "Validation loss decreased (0.018562 --> 0.018559).  Saving model ...\n",
            "train_loss:\n",
            "0.97137234130731\n",
            "Epoch: 963 \tTraining Loss: 0.971372 \tValidation Loss: 0.018557\n",
            "Validation loss decreased (0.018559 --> 0.018557).  Saving model ...\n",
            "train_loss:\n",
            "0.9713126202369784\n",
            "Epoch: 964 \tTraining Loss: 0.971313 \tValidation Loss: 0.018554\n",
            "Validation loss decreased (0.018557 --> 0.018554).  Saving model ...\n",
            "train_loss:\n",
            "0.9712529803042883\n",
            "Epoch: 965 \tTraining Loss: 0.971253 \tValidation Loss: 0.018552\n",
            "Validation loss decreased (0.018554 --> 0.018552).  Saving model ...\n",
            "train_loss:\n",
            "0.9711934220823613\n",
            "Epoch: 966 \tTraining Loss: 0.971193 \tValidation Loss: 0.018549\n",
            "Validation loss decreased (0.018552 --> 0.018549).  Saving model ...\n",
            "train_loss:\n",
            "0.9711339892103121\n",
            "Epoch: 967 \tTraining Loss: 0.971134 \tValidation Loss: 0.018547\n",
            "Validation loss decreased (0.018549 --> 0.018547).  Saving model ...\n",
            "train_loss:\n",
            "0.9710746219197472\n",
            "Epoch: 968 \tTraining Loss: 0.971075 \tValidation Loss: 0.018544\n",
            "Validation loss decreased (0.018547 --> 0.018544).  Saving model ...\n",
            "train_loss:\n",
            "0.9710153592648086\n",
            "Epoch: 969 \tTraining Loss: 0.971015 \tValidation Loss: 0.018542\n",
            "Validation loss decreased (0.018544 --> 0.018542).  Saving model ...\n",
            "train_loss:\n",
            "0.970956164074468\n",
            "Epoch: 970 \tTraining Loss: 0.970956 \tValidation Loss: 0.018539\n",
            "Validation loss decreased (0.018542 --> 0.018539).  Saving model ...\n",
            "train_loss:\n",
            "0.9708970883390405\n",
            "Epoch: 971 \tTraining Loss: 0.970897 \tValidation Loss: 0.018537\n",
            "Validation loss decreased (0.018539 --> 0.018537).  Saving model ...\n",
            "train_loss:\n",
            "0.9708381133911373\n",
            "Epoch: 972 \tTraining Loss: 0.970838 \tValidation Loss: 0.018535\n",
            "Validation loss decreased (0.018537 --> 0.018535).  Saving model ...\n",
            "train_loss:\n",
            "0.9707792140952833\n",
            "Epoch: 973 \tTraining Loss: 0.970779 \tValidation Loss: 0.018532\n",
            "Validation loss decreased (0.018535 --> 0.018532).  Saving model ...\n",
            "train_loss:\n",
            "0.9707204279500049\n",
            "Epoch: 974 \tTraining Loss: 0.970720 \tValidation Loss: 0.018530\n",
            "Validation loss decreased (0.018532 --> 0.018530).  Saving model ...\n",
            "train_loss:\n",
            "0.9706617350597958\n",
            "Epoch: 975 \tTraining Loss: 0.970662 \tValidation Loss: 0.018527\n",
            "Validation loss decreased (0.018530 --> 0.018527).  Saving model ...\n",
            "train_loss:\n",
            "0.9706030890836821\n",
            "Epoch: 976 \tTraining Loss: 0.970603 \tValidation Loss: 0.018525\n",
            "Validation loss decreased (0.018527 --> 0.018525).  Saving model ...\n",
            "train_loss:\n",
            "0.9705445727967954\n",
            "Epoch: 977 \tTraining Loss: 0.970545 \tValidation Loss: 0.018522\n",
            "Validation loss decreased (0.018525 --> 0.018522).  Saving model ...\n",
            "train_loss:\n",
            "0.9704861582799272\n",
            "Epoch: 978 \tTraining Loss: 0.970486 \tValidation Loss: 0.018520\n",
            "Validation loss decreased (0.018522 --> 0.018520).  Saving model ...\n",
            "train_loss:\n",
            "0.9704277883027936\n",
            "Epoch: 979 \tTraining Loss: 0.970428 \tValidation Loss: 0.018518\n",
            "Validation loss decreased (0.018520 --> 0.018518).  Saving model ...\n",
            "train_loss:\n",
            "0.9703695715128721\n",
            "Epoch: 980 \tTraining Loss: 0.970370 \tValidation Loss: 0.018515\n",
            "Validation loss decreased (0.018518 --> 0.018515).  Saving model ...\n",
            "train_loss:\n",
            "0.9703113943502143\n",
            "Epoch: 981 \tTraining Loss: 0.970311 \tValidation Loss: 0.018513\n",
            "Validation loss decreased (0.018515 --> 0.018513).  Saving model ...\n",
            "train_loss:\n",
            "0.9702533425374346\n",
            "Epoch: 982 \tTraining Loss: 0.970253 \tValidation Loss: 0.018511\n",
            "Validation loss decreased (0.018513 --> 0.018511).  Saving model ...\n",
            "train_loss:\n",
            "0.9701953597448685\n",
            "Epoch: 983 \tTraining Loss: 0.970195 \tValidation Loss: 0.018508\n",
            "Validation loss decreased (0.018511 --> 0.018508).  Saving model ...\n",
            "train_loss:\n",
            "0.9701374722542343\n",
            "Epoch: 984 \tTraining Loss: 0.970137 \tValidation Loss: 0.018506\n",
            "Validation loss decreased (0.018508 --> 0.018506).  Saving model ...\n",
            "train_loss:\n",
            "0.9700796861242462\n",
            "Epoch: 985 \tTraining Loss: 0.970080 \tValidation Loss: 0.018504\n",
            "Validation loss decreased (0.018506 --> 0.018504).  Saving model ...\n",
            "train_loss:\n",
            "0.9700219897287232\n",
            "Epoch: 986 \tTraining Loss: 0.970022 \tValidation Loss: 0.018501\n",
            "Validation loss decreased (0.018504 --> 0.018501).  Saving model ...\n",
            "train_loss:\n",
            "0.9699643738977202\n",
            "Epoch: 987 \tTraining Loss: 0.969964 \tValidation Loss: 0.018499\n",
            "Validation loss decreased (0.018501 --> 0.018499).  Saving model ...\n",
            "train_loss:\n",
            "0.9699068534505236\n",
            "Epoch: 988 \tTraining Loss: 0.969907 \tValidation Loss: 0.018497\n",
            "Validation loss decreased (0.018499 --> 0.018497).  Saving model ...\n",
            "train_loss:\n",
            "0.9698494235565375\n",
            "Epoch: 989 \tTraining Loss: 0.969849 \tValidation Loss: 0.018494\n",
            "Validation loss decreased (0.018497 --> 0.018494).  Saving model ...\n",
            "train_loss:\n",
            "0.9697920828238948\n",
            "Epoch: 990 \tTraining Loss: 0.969792 \tValidation Loss: 0.018492\n",
            "Validation loss decreased (0.018494 --> 0.018492).  Saving model ...\n",
            "train_loss:\n",
            "0.969734824702635\n",
            "Epoch: 991 \tTraining Loss: 0.969735 \tValidation Loss: 0.018490\n",
            "Validation loss decreased (0.018492 --> 0.018490).  Saving model ...\n",
            "train_loss:\n",
            "0.9696776821063116\n",
            "Epoch: 992 \tTraining Loss: 0.969678 \tValidation Loss: 0.018488\n",
            "Validation loss decreased (0.018490 --> 0.018488).  Saving model ...\n",
            "train_loss:\n",
            "0.9696206041089781\n",
            "Epoch: 993 \tTraining Loss: 0.969621 \tValidation Loss: 0.018485\n",
            "Validation loss decreased (0.018488 --> 0.018485).  Saving model ...\n",
            "train_loss:\n",
            "0.9695636174836002\n",
            "Epoch: 994 \tTraining Loss: 0.969564 \tValidation Loss: 0.018483\n",
            "Validation loss decreased (0.018485 --> 0.018483).  Saving model ...\n",
            "train_loss:\n",
            "0.9695067177270795\n",
            "Epoch: 995 \tTraining Loss: 0.969507 \tValidation Loss: 0.018481\n",
            "Validation loss decreased (0.018483 --> 0.018481).  Saving model ...\n",
            "train_loss:\n",
            "0.9694499311211345\n",
            "Epoch: 996 \tTraining Loss: 0.969450 \tValidation Loss: 0.018479\n",
            "Validation loss decreased (0.018481 --> 0.018479).  Saving model ...\n",
            "train_loss:\n",
            "0.9693932024004696\n",
            "Epoch: 997 \tTraining Loss: 0.969393 \tValidation Loss: 0.018476\n",
            "Validation loss decreased (0.018479 --> 0.018476).  Saving model ...\n",
            "train_loss:\n",
            "0.9693365785610545\n",
            "Epoch: 998 \tTraining Loss: 0.969337 \tValidation Loss: 0.018474\n",
            "Validation loss decreased (0.018476 --> 0.018474).  Saving model ...\n",
            "train_loss:\n",
            "0.969280043555485\n",
            "Epoch: 999 \tTraining Loss: 0.969280 \tValidation Loss: 0.018472\n",
            "Validation loss decreased (0.018474 --> 0.018472).  Saving model ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_sec=MyDataset(df,[20, 21, 22])"
      ],
      "metadata": {
        "id": "N7T9hW5KJTtU"
      },
      "id": "N7T9hW5KJTtU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_total_predictions=400\n",
        "num_predicciones_correctas=0\n",
        "for i in range(num_total_predictions):\n",
        "  i = random.randrange(1, 905)\n",
        "  tensor_prediccion = data_sec[i][1]\n",
        "  tensor_columnas= data_sec[i][0]\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    tensor_modelo = model(tensor_columnas)\n",
        "  if np.argmax(tensor_prediccion)== np.argmax(tensor_modelo):\n",
        "    num_predicciones_correctas+=1\n",
        "\n",
        "print(\"Las predicciones que estan bien son:\", num_predicciones_correctas)\n",
        "print(\"Total predicciones:\", num_total_predictions)\n",
        "print(\"Accuracy:\", num_predicciones_correctas/num_total_predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ldDNoRh3mKq0",
        "outputId": "9d889aa8-7d88-450e-ccbf-b91d88815830"
      },
      "id": "ldDNoRh3mKq0",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Las predicciones que estan bien son: 187\n",
            "Total predicciones: 400\n",
            "Accuracy: 0.4675\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como se ve, aumentó el score de accuracy, por lo que este es un modelo más preciso para clasificar los datos."
      ],
      "metadata": {
        "id": "bUOeE-nmm-VF"
      },
      "id": "bUOeE-nmm-VF"
    },
    {
      "cell_type": "code",
      "source": [
        "registro = random.randrange(1,905)\n",
        "tensor_prediccion = data_sec[registro][1]\n",
        "tensor_columnas= data_sec[registro][0]\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    tensor_modelo = model(tensor_columnas)\n",
        "\n",
        "print(tensor_prediccion)\n",
        "print(tensor_modelo)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2OiGEUKVocv6",
        "outputId": "fec4030a-4609-42f5-8f94-e61418fb321c"
      },
      "id": "2OiGEUKVocv6",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0., 0., 1.])\n",
            "tensor([-0.0545, -0.3442,  0.0089])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Y con una muestra aleatoria, si predijo bien la emoción, sad"
      ],
      "metadata": {
        "id": "1naiwl8Koe51"
      },
      "id": "1naiwl8Koe51"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusiones"
      ],
      "metadata": {
        "id": "yoSNk3tKk-Zy"
      },
      "id": "yoSNk3tKk-Zy"
    },
    {
      "cell_type": "markdown",
      "source": [
        "En cuanto al desempeño del modelo se obtuvo que al experimentar con distintos valores de los parámetros como el learning rate y número de epochs  el modelo se obtienen distintos resultados y la precisión del modelo varía dependiendo de estos valores. Con los diferentes modelos que hicimos, aunque teniamos un test loss no tan alto, no tenemos un acuraccy muy bueno, y en general se puede afirmar que los modelos no son tan buenos para este dataset, y esto se puede deber a los hiperparametros que les pusimos, batch_size, learning rate, las proporciones del train, test y validation, numero de epochs, que pueden no ser los optimos para que no se sobreajuste pero al mismo tiempo prediga bien los datos. Y tambien puede ser por los pocos datos que se tienen, que tengan un patron especifico que la red no pudo entender.\n",
        "A medida que usabamos mas epoch, el tiempo se aumentaba, por eso no lo hicimos con tantas, pero puede que con mayor numero se obtenga un mejor accuracy, pero al hacer esto se corre el riesgo de sobreajustar el modelo a los datos de entrenamiento y validacion. Es importante aclarar que el test loss y el accuracy son medidas de desempeño diferente, y aunque con muchas epoch se tiene un menor test loss, esto no implica un mayor accuracy, asi que podria evaluarse con mayor numero y obtener un mejor modelo. Con un learning rate alto el modelo aprende patrones especificos de los datos de entrenamiento, por lo que al evaluarlo con el test loss, se obtiene un mayor puntaje.\n",
        "Se podrian considerar otras medidas de desempeño, como el F1 score, para evaluar los modelos en futuros proyectos."
      ],
      "metadata": {
        "id": "fGQtlAZ6pusl"
      },
      "id": "fGQtlAZ6pusl"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}